[
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html",
    "href": "technical-details/unsupervised-learning/instructions.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the dataset\ndf = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\n\n# Specify the feature columns\nfeatures = [\n    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',\n    'Subscriber Count', 'Definition', 'Mean Sentiment Score',\n    'Duration_seconds', 'genre_label', 'singer_followers', 'singer_popularity'\n]\n\n# Ensure the target column 'popularity' exists in DataFrame\nif 'Popularity' not in df.columns:\n    raise ValueError(\"The 'popularity' column is missing from the DataFrame.\")\n\n# Split into input (X) and target (y)\nX = df[features]  # Inputs\ny = df['Popularity']  # Target\n\n# Applying PCA\npca = PCA(n_components=3)  # Reduce to 3 dimensions for visualization\nX_pca = pca.fit_transform(X)\n# How much variance was retained?\nprint(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\nprint(\"Total Variance Explained:\", sum(pca.explained_variance_ratio_))\n\nExplained Variance Ratio: [0.35411601 0.1606999  0.12194563]\nTotal Variance Explained: 0.6367615347014879"
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#pca",
    "href": "technical-details/unsupervised-learning/instructions.html#pca",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the dataset\ndf = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\n\n# Specify the feature columns\nfeatures = [\n    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',\n    'Subscriber Count', 'Definition', 'Mean Sentiment Score',\n    'Duration_seconds', 'genre_label', 'singer_followers', 'singer_popularity'\n]\n\n# Ensure the target column 'popularity' exists in DataFrame\nif 'Popularity' not in df.columns:\n    raise ValueError(\"The 'popularity' column is missing from the DataFrame.\")\n\n# Split into input (X) and target (y)\nX = df[features]  # Inputs\ny = df['Popularity']  # Target\n\n# Applying PCA\npca = PCA(n_components=3)  # Reduce to 3 dimensions for visualization\nX_pca = pca.fit_transform(X)\n# How much variance was retained?\nprint(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\nprint(\"Total Variance Explained:\", sum(pca.explained_variance_ratio_))\n\nExplained Variance Ratio: [0.35411601 0.1606999  0.12194563]\nTotal Variance Explained: 0.6367615347014879"
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#t-sne",
    "href": "technical-details/unsupervised-learning/instructions.html#t-sne",
    "title": "Dimensionality Reduction",
    "section": "t-SNE",
    "text": "t-SNE\n\nperplexities = [5, 30, 50, 100]\nfig, ax = plt.subplots(2, 2, figsize=(15, 10))\nfor i, perplexity in enumerate(perplexities):\n    # Applying t-SNE\n    tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=3000, random_state=42)\n    X_tsne = tsne.fit_transform(X)\n\n    # Plotting\n    ax[i//2, i%2].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', marker='o', alpha=0.5)\n    ax[i//2, i%2].set_title(f't-SNE with Perplexity={perplexity}')\n    ax[i//2, i%2].set_xlabel('t-SNE Feature 1')\n    ax[i//2, i%2].set_ylabel('t-SNE Feature 2')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFrom the plot, we can observe perplexities=50 has a better performance so we chose it as the parameter for dimension reduction."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#evaluation-and-comparison-pca-vs.-t-sne",
    "href": "technical-details/unsupervised-learning/instructions.html#evaluation-and-comparison-pca-vs.-t-sne",
    "title": "Dimensionality Reduction",
    "section": "Evaluation and Comparison: PCA vs. t-SNE",
    "text": "Evaluation and Comparison: PCA vs. t-SNE\n\nEffectiveness in Preserving Data Structure\n\nPCA: The PCA visualization shows a somewhat linear distribution of the data points, focusing on the major directions of data variance. It effectively captures the global structure of the data, emphasizing the spread along the directions of highest variance.\nt-SNE: In contrast, the t-SNE visualization exhibits a more scattered and differentiated structure with distinct clusters. This indicates t-SNE’s superior capability in capturing local structures within the data, potentially revealing intrinsic patterns that PCA might overlook.\n\n\nVisualization Capabilities\n\nPCA: Provides a simplified overview, projecting the data into lower dimensions while trying to preserve the variance. It’s effective for quick explorations of the data to identify gross underlying patterns but might miss finer details.\nt-SNE: Produces more visually distinct clusters, making it easier to identify groups and patterns within the data. This makes t-SNE a better tool for tasks requiring detailed pattern recognition and cluster analysis.\n\n\n\nTrade-offs and Scenarios\n\nPCA:\n\nPros: Less computationally intensive, suitable for larger datasets, provides a quick overview.\nCons: Might miss non-linear relationships between features.\nBest for: Preliminary data analysis, reducing dimensionality for linear data, or when computational resources are limited.\n\nt-SNE:\n\nPros: Captures complex non-linear relationships, excellent for identifying clusters and local patterns.\nCons: Computationally expensive, sensitive to parameter settings (like perplexity), not suitable for very large datasets.\nBest for: Detailed exploratory data analysis, when clusters or local patterns within the data are of interest, and computational resources are adequate.\n\n\nFor the data given, t-SNE has the better performance for dimentionality reduction and for future classification task."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#k-means-clustering",
    "href": "technical-details/unsupervised-learning/instructions.html#k-means-clustering",
    "title": "Dimensionality Reduction",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\nK-Means clustering was applied to both PCA and t-SNE processed data. The algorithm partitions the data into K mutually exclusive clusters by assigning each data point to the cluster with the nearest mean. This method is effective in producing spherical clusters where the centroid represents the mean of the cluster’s points. The number of clusters, K, was set to 3 based on domain knowledge and preliminary analysis."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#dbscan",
    "href": "technical-details/unsupervised-learning/instructions.html#dbscan",
    "title": "Dimensionality Reduction",
    "section": "DBSCAN",
    "text": "DBSCAN\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) was utilized to identify arbitrarily shaped clusters based on density. It categorizes data points into clusters when they are closely packed together, while points in low-density areas are labeled as noise. This method is particularly effective for data with noise and outliers."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#hierarchical-clustering",
    "href": "technical-details/unsupervised-learning/instructions.html#hierarchical-clustering",
    "title": "Dimensionality Reduction",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nHierarchical clustering was performed using the Ward method, which minimizes the total within-cluster variance. At each step, the pair of clusters with the minimum between-cluster distance are merged. This method is well-suited for identifying hierarchical relationships between clusters."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#results-section",
    "href": "technical-details/unsupervised-learning/instructions.html#results-section",
    "title": "Dimensionality Reduction",
    "section": "Results Section",
    "text": "Results Section\n\nVisualizing Cluster Results\nThe clustering results from K-Means, DBSCAN, and Hierarchical clustering were visualized using scatter plots, clearly labeled with clusters identified from PCA and t-SNE processed data. Each visualization helps in understanding the cluster distribution and separation.\n\nimport numpy as np\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.cluster import AgglomerativeClustering\nimport matplotlib.pyplot as plt\n\n# K-Means Clustering\nkmeans = KMeans(n_clusters=3, random_state=42)\nkmeans_tsne = kmeans.fit(X_tsne)\nkmeans_tsne_labels = kmeans_tsne.labels_\n\n# DBSCAN\ndbscan = DBSCAN(eps=1, min_samples=5)\ndbscan_tsne = dbscan.fit(X_tsne)\ndbscan_tsne_labels = dbscan_tsne.labels_\n\n# Hierarchical Clustering\nhierarchical = AgglomerativeClustering(n_clusters=3)\nhierarchical_tsne = hierarchical.fit(X_tsne)\nhierarchical_tsne_labels = hierarchical_tsne.labels_\n\n# Plotting the results\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# K-Means Plot\naxes[0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=kmeans_tsne_labels, cmap='viridis', marker='o')\naxes[0].set_title('K-Means Clustering on t-SNE Data')\naxes[0].set_xlabel('t-SNE Axis 1')\naxes[0].set_ylabel('t-SNE Axis 2')\n\n# DBSCAN Plot\naxes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=dbscan_tsne_labels, cmap='viridis', marker='o')\naxes[1].set_title('DBSCAN Clustering on t-SNE Data')\naxes[1].set_xlabel('t-SNE Axis 1')\naxes[1].set_ylabel('t-SNE Axis 2')\n\n# Hierarchical Plot\naxes[2].scatter(X_tsne[:, 0], X_tsne[:, 1], c=hierarchical_tsne_labels, cmap='viridis', marker='o')\naxes[2].set_title('Hierarchical Clustering on t-SNE Data')\naxes[2].set_xlabel('t-SNE Axis 1')\naxes[2].set_ylabel('t-SNE Axis 2')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nResults Insights\n\nK-Means Clustering:\n\nK-Means with n_clusters=3 segmented the data into three distinct groups. The clusters are relatively well-separated with slight overlap, indicating a clear grouping pattern in the dataset.\n\nDBSCAN Clustering:\n\nDBSCAN with eps=1 and min_samples=5 identified varying densities within the dataset. This method has successfully differentiated the dense core groups from sparser outliers, which are not included in any cluster and are labeled as noise. But there is overlap between different groups.\n\nHierarchical Clustering:\n\nHierarchical clustering revealed three main clusters, with results similar to K-Means but differing slightly in the cluster boundaries.\n\n\n\n\nPerformance Comparison\n\nSilhouette Scores:\n\nK-Means achieved the highest Silhouette Score of 0.463882, suggesting a good level of separation and cohesion within clusters.\nHierarchical Clustering followed closely with a Silhouette Score of 0.456777.\nDBSCAN had a lower Silhouette Score of 0.3538208, indicating more overlap or less distinct clustering compared to the other methods.\n\n\nSo K-Means has the best performance on the t-sne processed dataset.\n\n\nConclusion\nFrom the visualization, we can see there is chance for the dataset to be classified into groups, indicating there may be differences beween different songs for their popularity according to our features given. And in the future this can be used for popularity forecast."
  },
  {
    "objectID": "technical-details/progress-log.html",
    "href": "technical-details/progress-log.html",
    "title": "Progress log",
    "section": "",
    "text": "Confirm the project topic\nConfirm the data sources\nCollect data using YouTube API\nCollect data using Spotify API\nClean the data\nNormalize the data\nPerform Exploratory Data Analysis (EDA)\nImplement regression analysis\nImplement binary classification\nImplement multi-class classification\nWork on unsupervised learning (e.g., clustering, dimensionality reduction)\nUpdate progress log regularly\nWork on the LLM (Language Model) part of the project\nWrite the final report"
  },
  {
    "objectID": "technical-details/progress-log.html#to-do",
    "href": "technical-details/progress-log.html#to-do",
    "title": "Progress log",
    "section": "",
    "text": "Confirm the project topic\nConfirm the data sources\nCollect data using YouTube API\nCollect data using Spotify API\nClean the data\nNormalize the data\nPerform Exploratory Data Analysis (EDA)\nImplement regression analysis\nImplement binary classification\nImplement multi-class classification\nWork on unsupervised learning (e.g., clustering, dimensionality reduction)\nUpdate progress log regularly\nWork on the LLM (Language Model) part of the project\nWrite the final report"
  },
  {
    "objectID": "technical-details/progress-log.html#member-1-yiqin-zhou",
    "href": "technical-details/progress-log.html#member-1-yiqin-zhou",
    "title": "Progress log",
    "section": "Member-1: Yiqin Zhou",
    "text": "Member-1: Yiqin Zhou\nClick here to open\nWeekly project contribution log: T: 12-15-2024\n\nCompleted final report section\n\nCompleted progess- log section\n\nCompleted unsupervised learning multi-class classification section\n\nF: 12-14-2024\n\nCompleted EDA section\n\nTh: 12-13-2024\n\nCompleted data collection\n\nW: 12-12-2024\n\nConfirmed data source in group meeting and began data collection\n\nT: 12-11-2024\n\nHeld group meeting to decide on project topic due to changes in Spotify API permissions\n\nW: 11-21-2024\n\nParticipated in group discussion to decide on project topic and data source"
  },
  {
    "objectID": "technical-details/progress-log.html#member-2-xinzhou-li",
    "href": "technical-details/progress-log.html#member-2-xinzhou-li",
    "title": "Progress log",
    "section": "Member-2: Xinzhou Li",
    "text": "Member-2: Xinzhou Li\nClick here to open\nWeekly project contribution log:\nT: 12-15-2024\n\nCompleted final report section\n\nCompleted supervised classification modeling\nCompleted unsupervised learning classification section\n\nF: 12-14-2024\n\nStarted supervised classification modeling\nCompleted regression modeling\n\nTh: 12-13-2024\n\nStarted data cleaning on Youtube data and did regression modeling\n\nW: 12-12-2024\n\nConfirmed data source in group meeting and began data collection on Youtube\n\nT: 12-11-2024\n\nHeld group meeting to decide on project topic due to changes in Spotify API permissions\n\nT: 12-09-2024\n\nDid the first version of Youtube data collection\n\nW: 11-21-2024\n\nParticipated in group discussion to decide on project topic and data source"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Landing page",
    "section": "",
    "text": "Yiqin Zhou\nXinzhou Li"
  },
  {
    "objectID": "index.html#project-summary",
    "href": "index.html#project-summary",
    "title": "Landing page",
    "section": "Project Summary",
    "text": "Project Summary\nThe aim of this project is to analyze the popularity of YouTube’s music video with Youtube’s mv features, combined with the corresponding Spotify’s artist popularity and the corresponding track popularity, also explore these features and YouTube music videos’ popularity through regression analysis, binary and multiclassification analyses, unsupervised learning, interaction rate, etc."
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "Landing page",
    "section": "Research Questions",
    "text": "Research Questions\n\nIs there a relationship between video features (such as mean sentiment score，comment count, subscriber count, and publishing time) and YouTube video views?\nHow can we predict Spotify’s track popularity based on singer popularity and YouTube video views or other related features?\nCan sentiment analysis (sentiment score range: -0.5 to 0.2) help evaluate the interaction level between the video and its audience?\nHow can we predict the Like/View ratio (engagement rate) based on video features such as comment count, duration_seconds, etc.?\nHow do video features like mean sentiment score impact the likelihood of a video receiving high views?"
  },
  {
    "objectID": "index.html#literature-review",
    "href": "index.html#literature-review",
    "title": "Landing page",
    "section": "Literature Review",
    "text": "Literature Review\nIn recent years, the rapid expansion of video-sharing platforms like Youtube has triggered a wide range of research aimed at understanding the factors that influence video popularity, viewer engagement, and content analysis. Lots of studies have explored different aspects of YouTube dynamics, such as the discovery of persistent tags and trending videos, to better understand user behavior.\nDokuz (2024) proposed a method for identifying popular and persistent tags from a dataset of trending videos on YouTube, revealing trends in the user-video relationship and how video tags influence YouTube comments for engagement and visibility analysis, which is also a prominent area of research as it provides valuable insights into viewers’ reactions and opinions about video content.Khin Nyunt and Naw Thiri Khin (2024) investigated how the combination of sentiment analysis and trend analysis can guide aspiring youtube users to choose a successful career path.Similaet al. (2024) focused on sentiment analysis techniques applied to YouTube comments to provide valuable insights into users’ opinions on videos. Their study highlights how sentiment analysis can be used to predict video performance, as positive sentiment is often associated with higher engagement.\nAdditionally, artificial intelligence and deep learning are becoming more prevalent in analyzing YouTube comments.Meghana (2024) provides a comprehensive overview of sentiment analysis techniques, evaluating how advances in Natural Language Processing (NLP) can improve the accuracy of sentiment categorization and address challenges such as multi-lingual comments and context sensitivity. These studies help to understand how content features and user responses interact to shape the success of YouTube videos."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Landing page",
    "section": "References",
    "text": "References\n\nDokuz, Y. (2024). Discovering popular and persistent tags from YouTube trending video big dataset. Multimedia Tools and Applications, 83, 10779–10797.\nKhin Nyunt, N. T., & Khin, T. (2024). YouTube Career Analysis with the Combination of Trending Analysis and Sentiments Analysis. ESS Open Archive.\nMeghana, K. (2024). Artificial Intelligence and Sentiment Analysis in YouTube Comments: A Comprehensive Overview. 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT), 1565-1572.\nGiri, R., Sirsath, M., & Kanakia, H. T. (2024). YouTube Comments Sentiment Analysis. IEEE 9th International Conference for Convergence in Technology (I2CT), Pune, India, 1-4.\nDasovich-Wilson, J. N., Thompson, M., & Saarikallio, S. (2024). The characteristics of music video experiences and their relationship to future listening outcomes. Psychology of Music, 0(0).\nEfe, I. E., Tesch, C., & Subedi, P. (2024). YouTube as a source of patient information on awake craniotomy: Analysis of content quality and user engagement. World Neurosurgery: X, 21, 100249."
  },
  {
    "objectID": "technical-details/eda/instructions.html",
    "href": "technical-details/eda/instructions.html",
    "title": "Introduction and Motivation",
    "section": "",
    "text": "Introduction and Motivation\nThis eda section focuses primarily on examining the relationship between track popularity, mv’s view count and youtube related factors. By analyzing the trends and correlations between these variables, we want to find which YouTube factors have a significant impact on Spotify track popularity,view count? This analysis can help us better understand the cross-platform interaction mechanism of the music market and provide data support for music promotion strategies.\n\n\nOverview of Methods\nFor Univariate Analysis, we use summary statistics to provide an overview of central tendencies and spread. Distributions were visualized using histograms with KDE (Kernel Density Estimation) overlays for key metrics such as the logarithm of View Count, like_ratio, comment_ratio and singer_popularity. For categorical variables, the distribution of music genres is examined using a bar plot, highlighting the frequency of each genre.\nFor Bivariate and Multivariate Analysis, we employ correlation analysis, visualization, and statistical summarization to explore relationships in the dataset. It calculates a correlation matrix to identify linear relationships between numerical variables and uses a heatmap for visualization. Scatterplots analyze pairwise relationships like logarithmic popularity and like count, days since published and subscriber count, and engagement rate versus duration, with genre as a categorical hue. A boxplot examines the distribution of singer popularity across genres. Cross-tabulation categorizes singer popularity into three levels (low, medium, high) and calculates genre-wise percentages, while groupby summarization provides mean values of key metrics by genre.\nFor Data Distribution and Normalization, We use skewness and kurtosis to assess the distribution of numeric variables. For heavily skewed data, log transformation was applied to reduce skewness. Normalization techniques were implemented, including Min-Max scaling to scale values between 0 and 1 and Z-score normalization to standardize data with a mean of 0 and a standard deviation of 1. The impact of these transformations was visualized using histograms for original, Min-Max scaled, and Z-score normalized distributions, highlighting changes in data distribution.\nFor Statistical Insights, Our method includes ANOVA and T-tests. One-way ANOVA was applied to compare means of ‘View Count’ across different genres, ‘Popularity’ across various durations, and ‘View Count’ across like ratio bins, providing F-statistics and p-values to assess significant differences. Additionally, a T-test was conducted to evaluate’View Count’ based on the median of ‘Mean Sentiment Score.’\nFor Visual Summary, An engagement rate was calculated as the combined percentage of likes and comments relative to views. To explore relationships, scatter plots with regression lines were used to assess the correlation between popularity and likes, while a box plot examined engagement rates across genres. A violin plot visualized the distribution of sentiment scores by genre, and a scatter plot highlighted the relationship between video duration and views, categorized by genre.\n\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/data-collection/methods.html",
    "href": "technical-details/data-collection/methods.html",
    "title": "Methods",
    "section": "",
    "text": "Methods\nIn this section, provide an overview summary of the methods used on this page. This should include a brief description of the key techniques, algorithms, tools, or processes employed in your work. Make sure to outline the approach taken for data collection, processing, analysis, or any specific technical steps relevant to the project.\nIf you are developing a package, include a reference to the relevant documentation and provide a link here for easy access. Ensure that the package details are properly documented in its dedicated section, but mentioned and connected here for a complete understanding of the methods used in this project.\nmy method: This project aims to explore the key factors affecting the view count of official mv on Youtube. We employed the dual- platform data collection method, combing the API interfaces of Youtube and Spotify to get the overall horizon. As for the music genre selection, we chose pop, hip-pop, rock, electronic music, jazz, and selected 20 representative artists for each genre, as well as the top five official music videos, to ensure the diversity and representativeness of the sample.\nThe data collection method is divided into two main phases. Firstly, details of mv such as view count, like comment, top 10 comment, definition were obtained through Youtube API. Then, by using Spotify Web API, specialized traits on metrics on artists and tracks were collected.\nFinally, the data integration process uses artists name as the main matching key to ensure the data quality through a strict cleaning process."
  },
  {
    "objectID": "technical-details/data-collection/closing.html",
    "href": "technical-details/data-collection/closing.html",
    "title": "Summary",
    "section": "",
    "text": "This section should be written for a technical audience, focusing on detailed analysis, factual reporting, and clear presentation of data. The following serves as a guide, but feel free to adjust as needed.\n\n\nI encountered several technical challenges during the data collection process. First, when selecting artists, some singers may have turned off the music video comment section on YouTube, which caused me to have to re-search for other artists. In addition, Spotify’s search engine is not always accurate, and even if I provide the song title and artist name, I still may not be able to find the target song. This is an issue for which I haven’t found a perfect solution yet, considering that I may need to further clean up the song titles to make them more accurate in order to improve the accuracy of my searches. Although the original plan was to acquire 500 pieces of data, in the end only 376 pieces of data were successfully acquired. So we may have to make real-time adjustments when it comes to data collection.\nAnother technical difficulty is that the YouTube API sometimes returns irrelevant data, so it is important to carefully review and filter out the results that meet the requirements. In addition, the use of matching key to find relevant data in the YouTube and Spotify APIs is very important to improve the matching accuracy and reliability of the data.\n\n\n\nThe challenge of the project was to optimize the use of the API and reduce the interference of irrelevant data. While most projects can collect large amounts of data through APIs, accurately finding the target data is still a common difficulty. Therefore, how to improve the quality of data through data cleansing and precise matching algorithms remains a direction for further research and improvement in the future.\n\n\n\nWe can conclude that firstly, during the data collection process, despite the convenience provided by the use of APIs, the issues of exact matching and removal of irrelevant data still need to be addressed. Second, Spotify’s search engine may need further optimization or more detailed preprocessing of the input data to improve the accuracy of the matches.\nFuture steps could focus on optimizing the process of data collection and cleansing. For example, for the Spotify search problem, attempts could be made to improve the method of standardization of song titles or validation in combination with other data sources. On the other hand, further research can be done to improve the efficiency of data filtering in the YouTube API to ensure that the data collected is more in line with expectations and less interfered by irrelevant information. These improvements will help enhance the efficiency and quality of data collection and lay a solid foundation for subsequent analysis."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#challenges",
    "href": "technical-details/data-collection/closing.html#challenges",
    "title": "Summary",
    "section": "",
    "text": "I encountered several technical challenges during the data collection process. First, when selecting artists, some singers may have turned off the music video comment section on YouTube, which caused me to have to re-search for other artists. In addition, Spotify’s search engine is not always accurate, and even if I provide the song title and artist name, I still may not be able to find the target song. This is an issue for which I haven’t found a perfect solution yet, considering that I may need to further clean up the song titles to make them more accurate in order to improve the accuracy of my searches. Although the original plan was to acquire 500 pieces of data, in the end only 376 pieces of data were successfully acquired. So we may have to make real-time adjustments when it comes to data collection.\nAnother technical difficulty is that the YouTube API sometimes returns irrelevant data, so it is important to carefully review and filter out the results that meet the requirements. In addition, the use of matching key to find relevant data in the YouTube and Spotify APIs is very important to improve the matching accuracy and reliability of the data."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#benchmarks",
    "href": "technical-details/data-collection/closing.html#benchmarks",
    "title": "Summary",
    "section": "",
    "text": "The challenge of the project was to optimize the use of the API and reduce the interference of irrelevant data. While most projects can collect large amounts of data through APIs, accurately finding the target data is still a common difficulty. Therefore, how to improve the quality of data through data cleansing and precise matching algorithms remains a direction for further research and improvement in the future."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "title": "Summary",
    "section": "",
    "text": "We can conclude that firstly, during the data collection process, despite the convenience provided by the use of APIs, the issues of exact matching and removal of irrelevant data still need to be addressed. Second, Spotify’s search engine may need further optimization or more detailed preprocessing of the input data to improve the accuracy of the matches.\nFuture steps could focus on optimizing the process of data collection and cleansing. For example, for the Spotify search problem, attempts could be made to improve the method of standardization of song titles or validation in combination with other data sources. On the other hand, further research can be done to improve the efficiency of data filtering in the YouTube API to ensure that the data collected is more in line with expectations and less interfered by irrelevant information. These improvements will help enhance the efficiency and quality of data collection and lay a solid foundation for subsequent analysis."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html",
    "href": "technical-details/unsupervised-learning/main.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the dataset\ndf = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\n\n# Specify the feature columns\nfeatures = [\n    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',\n    'Subscriber Count', 'Definition', 'Mean Sentiment Score',\n    'Duration_seconds', 'genre_label', 'singer_followers', 'singer_popularity'\n]\n\n# Ensure the target column 'popularity' exists in DataFrame\nif 'Popularity' not in df.columns:\n    raise ValueError(\"The 'popularity' column is missing from the DataFrame.\")\n\n# Split into input (X) and target (y)\nX = df[features]  # Inputs\ny = df['Popularity']  # Target\n\n# Applying PCA\npca = PCA(n_components=3)  # Reduce to 3 dimensions for visualization\nX_pca = pca.fit_transform(X)\n# How much variance was retained?\nprint(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\nprint(\"Total Variance Explained:\", sum(pca.explained_variance_ratio_))\n\n\n\nperplexities = [5, 30, 50, 100]\nfig, ax = plt.subplots(2, 2, figsize=(15, 10))\nfor i, perplexity in enumerate(perplexities):\n    # Applying t-SNE\n    tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=3000, random_state=42)\n    X_tsne = tsne.fit_transform(X)\n\n    # Plotting\n    ax[i//2, i%2].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', marker='o', alpha=0.5)\n    ax[i//2, i%2].set_title(f't-SNE with Perplexity={perplexity}')\n    ax[i//2, i%2].set_xlabel('t-SNE Feature 1')\n    ax[i//2, i%2].set_ylabel('t-SNE Feature 2')\n\nplt.tight_layout()\nplt.show()\nFrom the plot, we can observe perplexities=50 has a better performance so we chose it as the parameter for dimension reduction.\n\n\n\n\n\n\nPCA: The PCA visualization shows a somewhat linear distribution of the data points, focusing on the major directions of data variance. It effectively captures the global structure of the data, emphasizing the spread along the directions of highest variance.\nt-SNE: In contrast, the t-SNE visualization exhibits a more scattered and differentiated structure with distinct clusters. This indicates t-SNE’s superior capability in capturing local structures within the data, potentially revealing intrinsic patterns that PCA might overlook.\n\n\n\n\nPCA: Provides a simplified overview, projecting the data into lower dimensions while trying to preserve the variance. It’s effective for quick explorations of the data to identify gross underlying patterns but might miss finer details.\nt-SNE: Produces more visually distinct clusters, making it easier to identify groups and patterns within the data. This makes t-SNE a better tool for tasks requiring detailed pattern recognition and cluster analysis.\n\n\n\n\n\nPCA:\n\nPros: Less computationally intensive, suitable for larger datasets, provides a quick overview.\nCons: Might miss non-linear relationships between features.\nBest for: Preliminary data analysis, reducing dimensionality for linear data, or when computational resources are limited.\n\nt-SNE:\n\nPros: Captures complex non-linear relationships, excellent for identifying clusters and local patterns.\nCons: Computationally expensive, sensitive to parameter settings (like perplexity), not suitable for very large datasets.\nBest for: Detailed exploratory data analysis, when clusters or local patterns within the data are of interest, and computational resources are adequate.\n\n\nFor the data given, t-SNE has the better performance for dimentionality reduction and for future classification task."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#pca",
    "href": "technical-details/unsupervised-learning/main.html#pca",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the dataset\ndf = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\n\n# Specify the feature columns\nfeatures = [\n    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',\n    'Subscriber Count', 'Definition', 'Mean Sentiment Score',\n    'Duration_seconds', 'genre_label', 'singer_followers', 'singer_popularity'\n]\n\n# Ensure the target column 'popularity' exists in DataFrame\nif 'Popularity' not in df.columns:\n    raise ValueError(\"The 'popularity' column is missing from the DataFrame.\")\n\n# Split into input (X) and target (y)\nX = df[features]  # Inputs\ny = df['Popularity']  # Target\n\n# Applying PCA\npca = PCA(n_components=3)  # Reduce to 3 dimensions for visualization\nX_pca = pca.fit_transform(X)\n# How much variance was retained?\nprint(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\nprint(\"Total Variance Explained:\", sum(pca.explained_variance_ratio_))"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#t-sne",
    "href": "technical-details/unsupervised-learning/main.html#t-sne",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "perplexities = [5, 30, 50, 100]\nfig, ax = plt.subplots(2, 2, figsize=(15, 10))\nfor i, perplexity in enumerate(perplexities):\n    # Applying t-SNE\n    tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=3000, random_state=42)\n    X_tsne = tsne.fit_transform(X)\n\n    # Plotting\n    ax[i//2, i%2].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', marker='o', alpha=0.5)\n    ax[i//2, i%2].set_title(f't-SNE with Perplexity={perplexity}')\n    ax[i//2, i%2].set_xlabel('t-SNE Feature 1')\n    ax[i//2, i%2].set_ylabel('t-SNE Feature 2')\n\nplt.tight_layout()\nplt.show()\nFrom the plot, we can observe perplexities=50 has a better performance so we chose it as the parameter for dimension reduction."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#evaluation-and-comparison-pca-vs.-t-sne",
    "href": "technical-details/unsupervised-learning/main.html#evaluation-and-comparison-pca-vs.-t-sne",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "PCA: The PCA visualization shows a somewhat linear distribution of the data points, focusing on the major directions of data variance. It effectively captures the global structure of the data, emphasizing the spread along the directions of highest variance.\nt-SNE: In contrast, the t-SNE visualization exhibits a more scattered and differentiated structure with distinct clusters. This indicates t-SNE’s superior capability in capturing local structures within the data, potentially revealing intrinsic patterns that PCA might overlook.\n\n\n\n\nPCA: Provides a simplified overview, projecting the data into lower dimensions while trying to preserve the variance. It’s effective for quick explorations of the data to identify gross underlying patterns but might miss finer details.\nt-SNE: Produces more visually distinct clusters, making it easier to identify groups and patterns within the data. This makes t-SNE a better tool for tasks requiring detailed pattern recognition and cluster analysis.\n\n\n\n\n\nPCA:\n\nPros: Less computationally intensive, suitable for larger datasets, provides a quick overview.\nCons: Might miss non-linear relationships between features.\nBest for: Preliminary data analysis, reducing dimensionality for linear data, or when computational resources are limited.\n\nt-SNE:\n\nPros: Captures complex non-linear relationships, excellent for identifying clusters and local patterns.\nCons: Computationally expensive, sensitive to parameter settings (like perplexity), not suitable for very large datasets.\nBest for: Detailed exploratory data analysis, when clusters or local patterns within the data are of interest, and computational resources are adequate.\n\n\nFor the data given, t-SNE has the better performance for dimentionality reduction and for future classification task."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#k-means-clustering",
    "href": "technical-details/unsupervised-learning/main.html#k-means-clustering",
    "title": "Unsupervised Learning",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\nK-Means clustering was applied to both PCA and t-SNE processed data. The algorithm partitions the data into K mutually exclusive clusters by assigning each data point to the cluster with the nearest mean. This method is effective in producing spherical clusters where the centroid represents the mean of the cluster’s points. The number of clusters, K, was set to 3 based on domain knowledge and preliminary analysis."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#dbscan",
    "href": "technical-details/unsupervised-learning/main.html#dbscan",
    "title": "Unsupervised Learning",
    "section": "DBSCAN",
    "text": "DBSCAN\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) was utilized to identify arbitrarily shaped clusters based on density. It categorizes data points into clusters when they are closely packed together, while points in low-density areas are labeled as noise. This method is particularly effective for data with noise and outliers."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#hierarchical-clustering",
    "href": "technical-details/unsupervised-learning/main.html#hierarchical-clustering",
    "title": "Unsupervised Learning",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nHierarchical clustering was performed using the Ward method, which minimizes the total within-cluster variance. At each step, the pair of clusters with the minimum between-cluster distance are merged. This method is well-suited for identifying hierarchical relationships between clusters."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#results-section",
    "href": "technical-details/unsupervised-learning/main.html#results-section",
    "title": "Unsupervised Learning",
    "section": "Results Section",
    "text": "Results Section\n\nVisualizing Cluster Results\nThe clustering results from K-Means, DBSCAN, and Hierarchical clustering were visualized using scatter plots, clearly labeled with clusters identified from PCA and t-SNE processed data. Each visualization helps in understanding the cluster distribution and separation.\nimport numpy as np\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.cluster import AgglomerativeClustering\nimport matplotlib.pyplot as plt\n\n# K-Means Clustering\nkmeans = KMeans(n_clusters=3, random_state=42)\nkmeans_tsne = kmeans.fit(X_tsne)\nkmeans_tsne_labels = kmeans_tsne.labels_\n\n# DBSCAN\ndbscan = DBSCAN(eps=1, min_samples=5)\ndbscan_tsne = dbscan.fit(X_tsne)\ndbscan_tsne_labels = dbscan_tsne.labels_\n\n# Hierarchical Clustering\nhierarchical = AgglomerativeClustering(n_clusters=3)\nhierarchical_tsne = hierarchical.fit(X_tsne)\nhierarchical_tsne_labels = hierarchical_tsne.labels_\n\n# Plotting the results\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# K-Means Plot\naxes[0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=kmeans_tsne_labels, cmap='viridis', marker='o')\naxes[0].set_title('K-Means Clustering on t-SNE Data')\naxes[0].set_xlabel('t-SNE Axis 1')\naxes[0].set_ylabel('t-SNE Axis 2')\n\n# DBSCAN Plot\naxes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=dbscan_tsne_labels, cmap='viridis', marker='o')\naxes[1].set_title('DBSCAN Clustering on t-SNE Data')\naxes[1].set_xlabel('t-SNE Axis 1')\naxes[1].set_ylabel('t-SNE Axis 2')\n\n# Hierarchical Plot\naxes[2].scatter(X_tsne[:, 0], X_tsne[:, 1], c=hierarchical_tsne_labels, cmap='viridis', marker='o')\naxes[2].set_title('Hierarchical Clustering on t-SNE Data')\naxes[2].set_xlabel('t-SNE Axis 1')\naxes[2].set_ylabel('t-SNE Axis 2')\n\nplt.tight_layout()\nplt.show()\n\n\nResults Insights\n\nK-Means Clustering:\n\nK-Means with n_clusters=3 segmented the data into three distinct groups. The clusters are relatively well-separated with slight overlap, indicating a clear grouping pattern in the dataset.\n\nDBSCAN Clustering:\n\nDBSCAN with eps=1 and min_samples=5 identified varying densities within the dataset. This method has successfully differentiated the dense core groups from sparser outliers, which are not included in any cluster and are labeled as noise. But there is overlap between different groups.\n\nHierarchical Clustering:\n\nHierarchical clustering revealed three main clusters, with results similar to K-Means but differing slightly in the cluster boundaries.\n\n\n\n\nPerformance Comparison\n\nSilhouette Scores:\n\nK-Means achieved the highest Silhouette Score of 0.463882, suggesting a good level of separation and cohesion within clusters.\nHierarchical Clustering followed closely with a Silhouette Score of 0.456777.\nDBSCAN had a lower Silhouette Score of 0.3538208, indicating more overlap or less distinct clustering compared to the other methods.\n\n\nSo K-Means has the best performance on the t-sne processed dataset.\n\n\nConclusion\nFrom the visualization, we can see there is chance for the dataset to be classified into groups, indicating there may be differences beween different songs for their popularity according to our features given. And in the future this can be used for popularity forecast."
  },
  {
    "objectID": "technical-details/eda/main.html",
    "href": "technical-details/eda/main.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Introduction and Motivation\nThis eda section focuses primarily on examining the relationship between track popularity, mv’s view count and youtube related factors. By analyzing the trends and correlations between these variables, we want to find which YouTube factors have a significant impact on Spotify track popularity,view count? This analysis can help us better understand the cross-platform interaction mechanism of the music market and provide data support for music promotion strategies.\n\n\nOverview of Methods\nFor Univariate Analysis, we use summary statistics to provide an overview of central tendencies and spread. Distributions were visualized using histograms with KDE (Kernel Density Estimation) overlays for key metrics such as the logarithm of View Count, like_ratio, comment_ratio and singer_popularity. For categorical variables, the distribution of music genres is examined using a bar plot, highlighting the frequency of each genre.\nFor Bivariate and Multivariate Analysis, we employ correlation analysis, visualization, and statistical summarization to explore relationships in the dataset. It calculates a correlation matrix to identify linear relationships between numerical variables and uses a heatmap for visualization. Scatterplots analyze pairwise relationships like logarithmic popularity and like count, days since published and subscriber count, and engagement rate versus duration, with genre as a categorical hue. A boxplot examines the distribution of singer popularity across genres. Cross-tabulation categorizes singer popularity into three levels (low, medium, high) and calculates genre-wise percentages, while groupby summarization provides mean values of key metrics by genre.\nFor Data Distribution and Normalization, We use skewness and kurtosis to assess the distribution of numeric variables. For heavily skewed data, log transformation was applied to reduce skewness. Normalization techniques were implemented, including Min-Max scaling to scale values between 0 and 1 and Z-score normalization to standardize data with a mean of 0 and a standard deviation of 1. The impact of these transformations was visualized using histograms for original, Min-Max scaled, and Z-score normalized distributions, highlighting changes in data distribution.\nFor Statistical Insights, Our method includes ANOVA and T-tests. One-way ANOVA was applied to compare means of ‘View Count’ across different genres, ‘Popularity’ across various durations, and ‘View Count’ across like ratio bins, providing F-statistics and p-values to assess significant differences. Additionally, a T-test was conducted to evaluate’View Count’ based on the median of ‘Mean Sentiment Score.’\nFor Visual Summary, An engagement rate was calculated as the combined percentage of likes and comments relative to views. To explore relationships, scatter plots with regression lines were used to assess the correlation between popularity and likes, while a box plot examined engagement rates across genres. A violin plot visualized the distribution of sentiment scores by genre, and a scatter plot highlighted the relationship between video duration and views, categorized by genre.\n\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\nCode\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nfile_path = \"../../data/processed-data/Updated_Data_with_Sentiments.csv\"\ndf = pd.read_csv(file_path)\n\n\n# include interaction metrics (like count/view count %, comment count/ view count%) as factors\ndf['like_ratio'] = df['Like Count'] / df['View Count'] *100\ndf['comment_ratio'] = df['Comment Count'] / df['View Count'] *100\n\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\n\n\nnumeric_cols = ['singer_popularity', 'Days Since Published', 'Subscriber Count', \n                  'View Count', 'Like Count', 'Comment Count', 'like_ratio', \n                  'comment_ratio', 'Duration_seconds', 'Mean Sentiment Score']\n                \nprint(\"\\nNumerical Variables Summary Statistics:\")\nprint(df[numeric_cols].describe())\n\n\nNumerical Variables Summary Statistics:\n       singer_popularity  Days Since Published  Subscriber Count  \\\ncount         375.000000            375.000000      3.750000e+02   \nmean           80.120000           3212.032000      8.392395e+06   \nstd            11.509773           2002.848485      1.220033e+07   \nmin            39.000000              0.000000      9.000000e+00   \n25%            77.000000           1473.500000      7.550000e+05   \n50%            81.000000           3391.000000      3.760000e+06   \n75%            87.000000           5374.500000      1.060000e+07   \nmax           100.000000           6794.000000      6.040000e+07   \n\n         View Count    Like Count  Comment Count  like_ratio  comment_ratio  \\\ncount  3.750000e+02  3.750000e+02   3.750000e+02  375.000000     375.000000   \nmean   3.702933e+08  2.498181e+06   9.775152e+04    1.187740       0.053684   \nstd    7.043627e+08  4.194654e+06   1.783573e+05    1.201180       0.083883   \nmin    2.760000e+02  0.000000e+00   0.000000e+00    0.000000       0.000000   \n25%    1.374476e+07  1.327810e+05   4.136500e+03    0.557778       0.016598   \n50%    9.101476e+07  7.850650e+05   2.807200e+04    0.787268       0.026350   \n75%    3.711109e+08  2.878034e+06   1.032220e+05    1.402276       0.059631   \nmax    6.379786e+09  3.378610e+07   1.184861e+06   12.401685       0.963266   \n\n       Duration_seconds  Mean Sentiment Score  \ncount        375.000000            375.000000  \nmean         292.744000              0.325594  \nstd          397.971143              0.225798  \nmin           21.000000             -0.336767  \n25%          200.500000              0.169655  \n50%          239.000000              0.319200  \n75%          284.500000              0.484267  \nmax         4835.000000              0.817367  \n\n\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 6))\n\n# subplot1: Distribution of Log10(View Count)\nsns.histplot(np.log10(df['View Count']), kde=True, ax=axes[0, 0])\naxes[0, 0].set_title('Distribution of Log10(View Count)')\naxes[0, 0].set_xlabel('Log10(View Count)')\n\n# subplot2: Like Ratio distribution\nsns.histplot(df['like_ratio'], kde=True, ax=axes[0, 1])\naxes[0, 1].set_title('Distribution of Like Ratio (%)')\naxes[0, 1].set_xlabel('Like Ratio (%)')\naxes[0, 1].set_xlim(-1, 7)\n\n# subplot3: Comment Ratio distribution\nsns.histplot(df['comment_ratio'], kde=True, ax=axes[1, 0])\naxes[1, 0].set_title('Distribution of Comment Ratio (%)')\naxes[1, 0].set_xlabel('Comment Ratio (%)')\n\n# subplot4: Distribution of Singer Popularity\nsns.histplot(df['singer_popularity'], kde=True, ax=axes[1, 1])\naxes[1, 1].set_title('Distribution of Singer Popularity')\naxes[1, 1].set_xlabel('Singer Popularity')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFor plot Distribution of Log10(View Count), it shows normal distribution characteristics. We can conclude that most of the videos have view counts between 107-109.\nFor plot Like Ratio Distribution, it shows right skewed distribution. The Like Ratio of most videos is concentrated between 0-2%.\nFor plot Comment Ratio Distribution, it shows a strong right-skewed distribution. Most videos have a comment ratio of less than 0.2%\nFor plot Singer Popularity Distribution, it ranges from 40-100 scores. There is a clear peak at around 80.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\n\n\n# Genre distribution\nplt.figure(figsize=(8, 5))\ngenre_counts = df['genre'].value_counts()\nsns.barplot(x=genre_counts.index, y=genre_counts.values)\nplt.title('Distribution of Music Genres')\nplt.xticks(rotation=45)\n\n([0, 1, 2, 3, 4],\n [Text(0, 0, 'rock'),\n  Text(1, 0, 'pop'),\n  Text(2, 0, 'jazz'),\n  Text(3, 0, 'hip-pop'),\n  Text(4, 0, 'electronic')])\n\n\n\n\n\n\n\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\n\n\nnumeric_cols = ['singer_popularity', 'Days Since Published', 'Subscriber Count', \n                  'View Count', 'like_ratio','comment_ratio', 'Duration_seconds', 'Mean Sentiment Score','Popularity']\ncorrelation_matrix = df[numeric_cols].corr()\n\n# Create correlation heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, \n            annot=True, \n            cmap='coolwarm', \n            center=0,  \n            fmt='.2f') \nplt.title('Correlation Matrix of Numerical Variables')\nplt.tight_layout()\n\n\n\n\n\n\n\n\nFrom this plot, we observe three key positive correlations: A strong relationship between the rate of likes and comments (0.72) suggests that sticky subscriber behaviors such as likes and comments are closely related. And correlation (0.72) between Singer_popularity and Popularity: reflects the consistency of the scoring system, suggesting that the two metrics may be measuring similar popularity dimensions A moderate correlation between the number of subscribers and the number of views (0.45) indicates that channels with a higher number of subscribers tend to get more views. Another moderate correlation (0.41) shows that more popular artists tend to attract more channel subscribers.\nFor negative correlations: A strong negative correlation (-0.56) between days of posting and the likes ratio demonstrates that videos posted for a longer period of time tend to have a lower likes ratio; A weak negative correlation (-0.30) between Singer_popularity and the average sentiment score interestingly suggests that more popular singers may receive lower sentiment scores in comments, which might be due to an increase in negative feedback as their popularity grows.\nIn addition, Singer_popularity showed significant correlations with several features, making these features valuable predictors for assessing singer popularity.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\n\n# Feature Pairing Analysis\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\n\n# 1. Log10(Popularity) vs Log10(Like Count) by Genre\nsns.scatterplot(data=df, \n                x=np.log10(df['Popularity']), \n                y=np.log10(df['Like Count']),\n                hue='genre',\n                ax=axes[0, 0])\naxes[0, 0].set_title('Log10(Popularity) vs Log10(Like Count) by Genre')\naxes[0, 0].set_xlabel('Log10(Popularity)')\naxes[0, 0].set_ylabel('Log10(Like Count)')\n\n# 2. Days Since Published vs Subscriber Count\nsns.scatterplot(data=df,\n                x='Days Since Published',\n                y=np.log10(df['Subscriber Count'] + 1),  # Log transform for scaling\n                hue='genre',\n                ax=axes[0, 1])\naxes[0, 1].set_title('Days Since Published vs Log10(Subscriber Count) by Genre')\naxes[0, 1].set_xlabel('Days Since Published')\naxes[0, 1].set_ylabel('Log10(Subscriber Count)')\n\n# 3. Duration vs Engagement Rate by Genre\ndf['Engagement_Rate'] = (df['Like Count'] + df['Comment Count']) / df['View Count'] * 100\nsns.scatterplot(data=df,\n                x='Duration_seconds',\n                y='Engagement_Rate',\n                hue='genre',\n                ax=axes[1, 0])\naxes[1, 0].set_title('Duration vs Engagement Rate by Genre')\naxes[1, 0].set_xlabel('Duration (seconds)')\naxes[1, 0].set_ylabel('Engagement Rate (%)')\n\n# 4. Singer Popularity Distribution by Genre\nsns.boxplot(data=df,\n            x='genre',\n            y='singer_popularity',\n            ax=axes[1, 1])\naxes[1, 1].set_title('Singer Popularity Distribution by Genre')\naxes[1, 1].set_xlabel('Genre')\naxes[1, 1].set_ylabel('Singer Popularity')\naxes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log10\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/var/folders/qx/qwpm5zm52fzd9w3sfybpybzh0000gn/T/ipykernel_35679/2832034967.py:43: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  axes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=45)\n\n\n\n\n\n\n\n\n\nFrom plot Log10(Popularity) and Log10(Like Count) show a strong positive correlation between genres.Jazz has a lower popularity and like count, concentrated in the lower left corner of the plot. In contrast, Pop, Hip-hop and Electronic music occupy the upper right region, with higher popularity and more likes, while Rock is more evenly distributed. Overall, the relationship is close to linear on a logarithmic scale, indicating that the more popular a song is, the more the number of likes increases exponentially, with some fluctuations.\nFrom plot days to release vs. number of subscribers (Log10), we observe that, similar to the chart of Log10 (number of views) vs. number of subscribers (number of likes), jazz have significantly fewer subscribers than other genres. In contrast, pop, hip-hop, rock, and electronic music typically have higher subscriber counts. The relationship between the time span of publication and the number of subscribers is not very clear and the distribution is spread out. Notably, from the dispersed distribution, the relatively low number of subscribers in the pop, hip-hop and electronic music genres compared to the other genres suggests that publication time has little effect on the number of subscribers.\nFrom plot Duration vs Engagement Rate, it can be observed that the engagement rate of most music videos is concentrated in the range of 0-4%, and the video duration is mainly concentrated in less than 1,000 seconds, which is shorter video may be more likely to get a higher engagement rate.\nFrom plot Singer Popularity Distribution, it can be observed that Hip-hop and Pop singers have the highest median popularity, Jazz singers have the lowest popularity distribution and the largest dispersion, and Electronic and Rock singers are at a medium level of popularity.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\n\n\n# 3. Cross-tabulation Analysis\n# Create a cross-tab of genre and popularity categories\ndf['popularity_category'] = pd.qcut(df['singer_popularity'], \n                                  q=3, \n                                  labels=['Low', 'Medium', 'High'])\n\n# Select the relevant numeric columns for cross-tabulation analysis\nrelevant_cols_for_crosstab = ['singer_popularity', 'View Count','Days Since Published', 'Mean Sentiment Score']\n\n# Create a cross-tab of genre and popularity category\ngenre_popularity_crosstab = pd.crosstab(df['genre'], \n                                       df['popularity_category'], \n                                       normalize='index') * 100\n\nprint(\"\\nGenre-Popularity Cross-tabulation (%):\")\nprint(genre_popularity_crosstab)\n\n# Calculate summary statistics by genre for selected relevant columns\ngenre_summary = df.groupby('genre').agg({\n    'singer_popularity': 'mean',\n    'View Count': 'mean',\n    'like_ratio': 'mean',\n    'comment_ratio': 'mean',\n    'Mean Sentiment Score': 'mean',\n}).round(2)\n\nprint(\"\\nSummary Statistics by Genre:\")\nprint(genre_summary)\n\n\nGenre-Popularity Cross-tabulation (%):\npopularity_category        Low     Medium       High\ngenre                                               \nelectronic           57.894737  42.105263   0.000000\nhip-pop              12.857143  24.285714  62.857143\njazz                 86.111111   6.944444   6.944444\npop                   4.545455  38.636364  56.818182\nrock                 19.318182  55.681818  25.000000\n\nSummary Statistics by Genre:\n            singer_popularity    View Count  like_ratio  comment_ratio  \\\ngenre                                                                    \nelectronic              78.04  4.812136e+08        1.25           0.05   \nhip-pop                 86.93  2.120770e+08        1.73           0.10   \njazz                    63.74  1.062045e+07        1.26           0.05   \npop                     87.49  6.973413e+08        1.15           0.06   \nrock                    82.09  3.915309e+08        0.69           0.03   \n\n            Mean Sentiment Score  \ngenre                             \nelectronic                  0.27  \nhip-pop                     0.18  \njazz                        0.47  \npop                         0.32  \nrock                        0.37  \n\n\nThe Genre-Popularity crosstab shows that Hip-hop and Pop are the strongest performers, Rock is in the middle of the pack, while Electronic and Jazz are weak.\nAccording to the summary statistics by genre, we can see that Pop has the highest View Count and singer_popularity. In terms of interaction metrics, Hip-hop has the highest interaction rate, while Rock has the lowest. As for the Mean Sentiment Score, the rankings are Jazz, Rock, Electronic and Hip-hop.\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\n\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n# 1. Calculate Skewness and Kurtosis\nprint(\"Skewness and Kurtosis Analysis:\")\nfor col in numeric_cols:\n    skew = stats.skew(df[col].dropna())\n    kurt = stats.kurtosis(df[col].dropna())\n    print(f\"\\n{col}:\")\n    print(f\"Skewness: {skew:.2f}\")\n    print(f\"Kurtosis: {kurt:.2f}\")\n    if skew &gt; 1 or skew &lt; -1:  # Heavily skewed data\n        df[f'{col}_log'] = np.log1p(df[col])  # log(1 + value) to avoid log(0)\n        print(f\"Log Transformation Applied to {col}\")\n\nSkewness and Kurtosis Analysis:\n\nsinger_popularity:\nSkewness: -1.24\nKurtosis: 2.09\nLog Transformation Applied to singer_popularity\n\nDays Since Published:\nSkewness: -0.20\nKurtosis: -1.33\n\nSubscriber Count:\nSkewness: 2.39\nKurtosis: 6.14\nLog Transformation Applied to Subscriber Count\n\nView Count:\nSkewness: 3.68\nKurtosis: 18.98\nLog Transformation Applied to View Count\n\nlike_ratio:\nSkewness: 4.72\nKurtosis: 33.35\nLog Transformation Applied to like_ratio\n\ncomment_ratio:\nSkewness: 5.67\nKurtosis: 45.55\nLog Transformation Applied to comment_ratio\n\nDuration_seconds:\nSkewness: 8.98\nKurtosis: 86.03\nLog Transformation Applied to Duration_seconds\n\nMean Sentiment Score:\nSkewness: -0.09\nKurtosis: -0.36\n\nPopularity:\nSkewness: -1.27\nKurtosis: 1.42\nLog Transformation Applied to Popularity\n\n\n\n# 2. Normalization (Min-Max and Z-score scaling)\nscaler_minmax = MinMaxScaler()\nscaler_standard = StandardScaler()\n\n# Apply Min-Max scaling to numeric columns\ndf_minmax = df.copy()\ndf_minmax[numeric_cols] = scaler_minmax.fit_transform(df_minmax[numeric_cols])\n\n# Apply Z-score normalization to numeric columns\ndf_standard = df.copy()\ndf_standard[numeric_cols] = scaler_standard.fit_transform(df_standard[numeric_cols])\n\n\n# 3. Visualize the impact of normalization for all numeric columns\nplt.figure(figsize=(13,12))\n\n# Plot the original, Min-Max scaled, and Z-score normalized distributions for each numeric column\nfor i, col in enumerate(numeric_cols, start=1):\n    # Original distribution\n    plt.subplot(len(numeric_cols), 3, i*3-2)\n    sns.histplot(df[col], kde=True, color='blue', bins=30)\n    plt.title(f'Original {col} Distribution')\n\n    # Min-Max scaled distribution\n    plt.subplot(len(numeric_cols), 3, i*3-1)\n    sns.histplot(df_minmax[col], kde=True, color='green', bins=30)\n    plt.title(f'Min-Max Scaled {col}')\n\n    # Z-score normalized distribution\n    plt.subplot(len(numeric_cols), 3, i*3)\n    sns.histplot(df_standard[col], kde=True, color='red', bins=30)\n    plt.title(f'Z-score Normalized {col}')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\n\n# ANOVA test for view counts across genres\nfrom scipy.stats import f_oneway\ngenres = df['Genre'].unique()\ngenre_views = [df[df['Genre'] == genre]['View Count'] for genre in genres]\nf_stat, p_value = f_oneway(*genre_views)\nprint(\"\\n=== One-way ANOVA Test for View Count Across Genres ===\")\nprint(f\"F-statistic: {f_stat}\")\nprint(f\"p-value: {p_value}\")\n\ngenres = df['Duration_seconds'].unique()\ngenre_views = [df[df['Duration_seconds'] == genre]['Popularity'] for genre in genres]\nf_stat, p_value = f_oneway(*genre_views)\nprint(\"\\n=== One-way ANOVA Test for Popularity Across Duration_seconds===\")\nprint(f\"F-statistic: {f_stat}\")\nprint(f\"p-value: {p_value}\")\n\n\nlike_ratio_bins = df['like_ratio'].unique()\nlike_ratio_views = [df[df['like_ratio'] == like_ratio]['View Count'] for like_ratio in like_ratio_bins]\nf_stat_like_ratio, p_value_like_ratio = f_oneway(*like_ratio_views)\nprint(\"\\n=== One-way ANOVA Test for View Count Across Like Ratio ===\")\nprint(f\"F-statistic: {f_stat_like_ratio}\")\nprint(f\"p-value: {p_value_like_ratio}\")\n\n\n=== One-way ANOVA Test for View Count Across Genres ===\nF-statistic: 11.944468793592273\np-value: 3.871891640564041e-09\n\n=== One-way ANOVA Test for Popularity Across Duration_seconds===\nF-statistic: 1.1090156863397904\np-value: 0.24201725292066867\n\n=== One-way ANOVA Test for View Count Across Like Ratio ===\nF-statistic: 12.005339777316456\np-value: 0.07990432736153048\n\n\nFor the one-way ANOVA: Genres vs View Count the results showed an F-value of 11.9445 while the p-value was 3.87e-09, which is much less than 0.05. This indicates that there is a statistically significant difference in video viewership across music genres.\nFor the one-way ANOVA: Popularity vs Duration_seconds, the results showed an F-value of 1.1090 while the p-value was 0.2420, which is greater than 0.05. This indicates that there is no statistically significant difference in song popularity across different durations.\nFor the one-way ANOVA: Like Ratio vs View Count , an F value of 12.0053 was obtained, but the p-value was 0.0799, which is greater than 0.05, which suggests that there may be a certain correlation between the LIKE Ratio and the amount of viewing, but from a statistical point of view, this difference is not statistically significant.\n\n# T-test for 'View Count' based on 'mean_sentiment_score'\nfrom scipy.stats import ttest_ind\nimport numpy as np\nmedian_sentiment = df['Mean Sentiment Score'].median()\nhigh_sentiment = df[df['Mean Sentiment Score'] &gt; median_sentiment]['View Count']\nlow_sentiment = df[df['Mean Sentiment Score'] &lt;= median_sentiment]['View Count']\n\n# T-test for 'View Count' based on 'mean_sentiment_score'\nt_stat_sentiment, p_value_sentiment = ttest_ind(high_sentiment, low_sentiment)\nprint(\"\\n=== T-test for View Count Based on Mean Sentiment Score ===\")\nprint(f\"T-statistic: {t_stat_sentiment}\")\nprint(f\"p-value: {p_value_sentiment}\")\n\n\n=== T-test for View Count Based on Mean Sentiment Score ===\nT-statistic: -2.6437887108864815\np-value: 0.008544340977117903\n\n\nAs for T-test: View Count vs Mean Sentiment Score,p-value of 0.0085, which is less than 0.05, suggesting that differences in sentiment scores do indeed make a significant difference in viewings. The negative t-value further implies that there may be a negative correlation.\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\n\n\n# 1. Statistical Tests and Visualizations for Engagement Metrics\ndef analyze_engagement_metrics():\n    # Calculate engagement rate\n    df['engagement_rate'] = (df['Like Count'] + df['Comment Count']) / df['View Count'] * 100\n    \n    # Create visualization for engagement metrics\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # Scatter plot: Views vs Likes with regression line\n    sns.regplot(data=df, x='Popularity', y='Like Count', ax=axes[0,0])\n    axes[0,0].set_title('Correlation: Popularity vs Likes')\n    \n    # Calculate correlation\n    correlation = df['Popularity'].corr(df['Like Count'])\n    print(f\"\\nCorrelation between Popularity and Likes: {correlation:.3f}\")\n    \n    # Box plot: Engagement rate by genre\n    sns.boxplot(data=df, x='genre', y='engagement_rate', ax=axes[0,1])\n    axes[0,1].set_title('Engagement Rate by Genre')\n    axes[0,1].tick_labels = rotation=45\n    \n    # Violin plot: Sentiment distribution by genre\n    sns.violinplot(data=df, x='genre', y='Mean Sentiment Score', ax=axes[1,0])\n    axes[1,0].set_title('Sentiment Distribution by Genre')\n    axes[1,0].tick_labels = rotation=45\n    \n    # Scatter plot: Duration vs Views\n    sns.scatterplot(data=df, x='Duration_seconds', y='View Count', hue='genre', ax=axes[1,1])\n    axes[1,1].set_title('Duration vs Views by Genre')\n    \n    plt.tight_layout()\n    \n    return fig\n\nfig = analyze_engagement_metrics()\n\n\nCorrelation between Popularity and Likes: 0.361\n\n\n\n\n\n\n\n\n\nIn the terms of music genre, we can observe some interesting trends.Hip-hop has higher interaction rates but lower sentiment scores, indicates that despite frequent and positive viewer reactions and interactions to Hip-hop MVs, the comments or sentiment expressed in these videos may be more negatively charged. On the other hand, Jazz has lower view count but higher affective sentiment score, suggesting that despite fewer plays of this type of video, viewers generally rate its content more favorably, with more positive affective tendencies. This also reflects the fact that Jazz’s viewer base, although smaller, has a higher appreciation of the content.\nFor Pop, the view count is very high and the affective scores are at a medium level, showing that Pop music has a wide acceptance and popularity among viewers, and although the emotional expression is not as extreme as that of Jazz, it is still able to attract a large number of viewers.Rock, on the other hand, exhibits a more stable interaction rate and medium affective scores, which means that the Rock category has a relatively balanced audience base with a stable level of interaction, but its affective responses and affective scores are more positive. level is stable, but its emotional response is not as intense as Hip-hop.\nAs for Electronic, has a higher number of views but a lower interaction rate, reflecting that although music videos of Electronic music can attract a large number of viewers, the sense of audience engagement and interactive behavior is lower, probably because the audience group of this type of music is less interested in interaction and watches it more as background music.\nAll in all, we can conclude that Popularity is not the only factor that predicts the number of likes.. Meanwhile, short videos are more likely to inspire user interaction and sharing due to their short and concise nature, which coincides with the current hot trend of short video platforms.\nDespite Jazz’s better performance in sentiment scores and content ratings, its play count is generally low, which indicates that this music style has a more niche audience and is only enjoyed by specific groups. Pop and Electronic music videos, on the other hand, typically have higher plays, suggesting that if one wishes to create music content that is popular and can be distributed quickly, choosing the Pop or Electronic genres will undoubtedly be a more attractive option.\n\n\nConclusions and Next Steps\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling.\n\nIn addition to the findings in the previous section, we can also observe some trends and conclusions. First, both the like and comment rates show a heavily right-skewed distribution, implying that most music videos have a relatively small like count and comment count, while a few videos generate a large number of likes and comments, which is in line with the common pattern of interactions on social platforms: only a very small amount of content generates a wide range of attention and interactions. Meanwhile, the interaction rate is negatively correlated with video duration, which reflects an interesting phenomenon. Longer duration videos tend to lead to a decrease in viewers’ willingness to interact, probably because longer videos tend to fatigue viewers, while shorter videos are more likely to stimulate users’ attention and interaction.\nIn the correlation analysis of viewership, we find that it is moderately positively correlated with the number of subscribers, suggesting that the more subscribers a channel has, the higher the view count of the video is usually, which is consistent with our intuitive experience that the more popular a channel is, the more viewers will watch new videos accordingly. On the other hand, view count shows a weak positive correlation with singer_popularity and although this correlation is not strong, it implies that an singer popularity may have some effect on the viewership of his or her videos. It is worth noting that shorter videos are more likely to receive a higher number of views, which is in line with the current characteristics of user behavior on social platforms, especially short video platforms-users prefer to watch short and concise content.\nFrom the analysis of each music genre, we can also see that the sentiment scores of different genres differ significantly. When we conduct regression analysis, we can use the above observed characteristics as the main variables for modeling. First, genre is a key categorization feature that can effectively help us classify different types of music videos, affecting behaviors such as the view count and engagement rate. Second, video duration is also an important factor, but it may have a non-linear relationship with other features, so the effect of video duration needs to be specifically addressed in the modeling. The number of subscribers, as a reflection of the channel’s popularity, has a direct impact on the number of video views and should be used as one of the important input features. Singer popularity, on the other hand, can further help us analyze and predict video performance, especially when we want to delve into the role of singer influence on video interaction."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html",
    "href": "technical-details/data-cleaning/main.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "This document outlines the steps taken to clean and preprocess the data from a dataset containing view/like/comment counts, relevant comments and populariy value from Spotify and YouTube. The primary goal is to prepare the data for sentiment analysis and further statistical modeling, focusing on extracting meaningful insights about song popularity and engagement.\n\n\n\nThe methods used include text cleaning, language detection, sentiment analysis, data transformation, and normalization. These techniques ensure that the data is in an appropriate format for analysis, removing any inconsistencies and ensuring quality and accuracy in the results.\n\n\n\n\n\nThe raw data is loaded from a CSV file using Pandas, which provides a convenient framework for data manipulation in Python.\nimport pandas as pd\ndf = pd.read_csv('../../data/raw-data/spotify_youtube.csv', encoding='iso-8859-1')\ndf.head()\n\n\n\nComments are cleaned by removing HTML tags, punctuation, numbers, and ensuring they are in English. This is crucial for the accuracy of the sentiment analysis.\ndef clean_comments(comments):\n    # Cleaning code here...\n    return english_comments\n\n\n\nUsing NLTK’s VADER, we perform sentiment analysis on the cleaned English comments. This provides a mean sentiment score for each record, indicating the overall sentiment of the comments associated with each song.\ndef average_sentiment_score(comments):\n    # Sentiment analysis code here...\n    return score\n\n\n\nSeveral transformations are applied to the dataset: - Converting video duration from ISO 8601 format to seconds. - Binary encoding of video definition (HD or SD). - Factorizing the ‘genre’ column to prepare for modeling.\ndf['Duration_seconds'] = df['Duration'].apply(duration_to_seconds)\ndf['Definition'] = df['Definition'].apply(convert_definition)\ndf['genre_label'] = pd.factorize(df['genre'])[0]\nFor future analysis, some categorical variables are converted into numerical style.\n\n\n\nUsing StandardScaler, numerical columns are scaled to have zero mean and unit variance. This step is important for models that are sensitive to the magnitude of input features.\nscaler = StandardScaler()\ndf[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the normalized data\ndf_normalized = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\n\ndef plot_data_comparisons(df_original, df_normalized, column_name):\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # Original Data Histogram\n    sns.histplot(df_original[column_name], ax=axes[0], kde=True, color='skyblue')\n    axes[0].set_title(f'Original {column_name}')\n    \n    # Normalized Data Histogram\n    sns.histplot(df_normalized[column_name], ax=axes[1], kde=True, color='olive')\n    axes[1].set_title(f'Normalized {column_name}')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Assuming the original data is still available and named df\ndf_original = pd.read_csv('../../data/raw-data/spotify_youtube.csv', encoding='iso-8859-1')\n\n# Example variables to visualize\nvariables_to_visualize = ['View Count', 'Like Count', 'Subscriber Count']\n\nfor variable in variables_to_visualize:\n    plot_data_comparisons(df_original, df_normalized, variable)\nThe standardization has transformed the data into a scale where it is centered around zero, greatly reducing the range of values and making the distribution more compact. The normalization process highlights the underlying data structure more clearly by smoothing out extreme variations and focusing on the distribution’s shape, facilitating more effective data analysis and model training.\n\n\n\nThe processed data is saved back to a CSV file, ensuring that all modifications are preserved for subsequent analysis.\ndf.to_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv', index=False)\ndf = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\ndf.head()\nReview Data Types\n\n\n\n\n\n\n\n\nVariable Name\nData Type\nDescription\n\n\n\n\nsinger\nCategorical\nArtist or band name\n\n\ngenre\nCategorical\nGenre of the music\n\n\nVideo ID\nCategorical\nUnique identifier for the video\n\n\nTitle\nCategorical\nTitle of the video\n\n\nDescription\nCategorical\nDescription of the video\n\n\nPublished At\nDate-time\nDate and time the video was published\n\n\nDays Since Published\nNumerical\nNumber of days since the video was published\n\n\nView Count\nNumerical\nTotal number of views on the video\n\n\nLike Count\nNumerical\nNumber of likes on the video\n\n\nComment Count\nNumerical\nNumber of comments on the video\n\n\nComments\nCategorical\nList of 10 comments on the video\n\n\nSubscriber Count\nNumerical\nNumber of subscribers to the channel\n\n\nCategory ID\nCategorical\nYouTube category ID for the video\n\n\nDefinition\nCategorical\nQuality definition of the video\n\n\nDuration\nCategorical\nDuration of the video in a human-readable format\n\n\nTrack Name\nCategorical\nName of the track\n\n\nArtist Name\nCategorical\nName of the artist\n\n\nArtist\nCategorical\nName of the artist\n\n\nsinger_followers\nNumerical\nNumber of followers the singer has on Spotify\n\n\nsinger_popularity\nNumerical\nPopularity rating of the singer\n\n\nAlbum Name\nCategorical\nName of the album\n\n\nPopularity\nNumerical\nPopularity rating of the track\n\n\nDuration (ms)\nNumerical\nDuration of the track in milliseconds\n\n\nTrack ID\nCategorical\nUnique identifier for the track\n\n\nSpotify URL\nCategorical\nURL to the track on Spotify\n\n\nMean Sentiment Score\nNumerical\nAverage sentiment score of comments\n\n\nProcessed_Comments\nCategorical\nProcessed list of comments\n\n\nDuration_seconds\nNumerical\nDuration of the video in seconds\n\n\ngenre_label\nCategorical\nCategorical label for the genre\n\n\n\n\n\n\n\nThe preprocessing steps significantly cleaned and transformed the raw data, making it suitable for accurate and insightful analysis. The sentiment scores provide a quantitative measure of public perception, which, combined with other song metrics, can be used to gauge popularity and engagement."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#introduction-and-motivation",
    "href": "technical-details/data-cleaning/main.html#introduction-and-motivation",
    "title": "Data Cleaning",
    "section": "",
    "text": "This document outlines the steps taken to clean and preprocess the data from a dataset containing view/like/comment counts, relevant comments and populariy value from Spotify and YouTube. The primary goal is to prepare the data for sentiment analysis and further statistical modeling, focusing on extracting meaningful insights about song popularity and engagement."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#overview-of-methods",
    "href": "technical-details/data-cleaning/main.html#overview-of-methods",
    "title": "Data Cleaning",
    "section": "",
    "text": "The methods used include text cleaning, language detection, sentiment analysis, data transformation, and normalization. These techniques ensure that the data is in an appropriate format for analysis, removing any inconsistencies and ensuring quality and accuracy in the results."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#code-implementation-and-description",
    "href": "technical-details/data-cleaning/main.html#code-implementation-and-description",
    "title": "Data Cleaning",
    "section": "",
    "text": "The raw data is loaded from a CSV file using Pandas, which provides a convenient framework for data manipulation in Python.\nimport pandas as pd\ndf = pd.read_csv('../../data/raw-data/spotify_youtube.csv', encoding='iso-8859-1')\ndf.head()\n\n\n\nComments are cleaned by removing HTML tags, punctuation, numbers, and ensuring they are in English. This is crucial for the accuracy of the sentiment analysis.\ndef clean_comments(comments):\n    # Cleaning code here...\n    return english_comments\n\n\n\nUsing NLTK’s VADER, we perform sentiment analysis on the cleaned English comments. This provides a mean sentiment score for each record, indicating the overall sentiment of the comments associated with each song.\ndef average_sentiment_score(comments):\n    # Sentiment analysis code here...\n    return score\n\n\n\nSeveral transformations are applied to the dataset: - Converting video duration from ISO 8601 format to seconds. - Binary encoding of video definition (HD or SD). - Factorizing the ‘genre’ column to prepare for modeling.\ndf['Duration_seconds'] = df['Duration'].apply(duration_to_seconds)\ndf['Definition'] = df['Definition'].apply(convert_definition)\ndf['genre_label'] = pd.factorize(df['genre'])[0]\nFor future analysis, some categorical variables are converted into numerical style.\n\n\n\nUsing StandardScaler, numerical columns are scaled to have zero mean and unit variance. This step is important for models that are sensitive to the magnitude of input features.\nscaler = StandardScaler()\ndf[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the normalized data\ndf_normalized = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\n\ndef plot_data_comparisons(df_original, df_normalized, column_name):\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # Original Data Histogram\n    sns.histplot(df_original[column_name], ax=axes[0], kde=True, color='skyblue')\n    axes[0].set_title(f'Original {column_name}')\n    \n    # Normalized Data Histogram\n    sns.histplot(df_normalized[column_name], ax=axes[1], kde=True, color='olive')\n    axes[1].set_title(f'Normalized {column_name}')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Assuming the original data is still available and named df\ndf_original = pd.read_csv('../../data/raw-data/spotify_youtube.csv', encoding='iso-8859-1')\n\n# Example variables to visualize\nvariables_to_visualize = ['View Count', 'Like Count', 'Subscriber Count']\n\nfor variable in variables_to_visualize:\n    plot_data_comparisons(df_original, df_normalized, variable)\nThe standardization has transformed the data into a scale where it is centered around zero, greatly reducing the range of values and making the distribution more compact. The normalization process highlights the underlying data structure more clearly by smoothing out extreme variations and focusing on the distribution’s shape, facilitating more effective data analysis and model training.\n\n\n\nThe processed data is saved back to a CSV file, ensuring that all modifications are preserved for subsequent analysis.\ndf.to_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv', index=False)\ndf = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\ndf.head()\nReview Data Types\n\n\n\n\n\n\n\n\nVariable Name\nData Type\nDescription\n\n\n\n\nsinger\nCategorical\nArtist or band name\n\n\ngenre\nCategorical\nGenre of the music\n\n\nVideo ID\nCategorical\nUnique identifier for the video\n\n\nTitle\nCategorical\nTitle of the video\n\n\nDescription\nCategorical\nDescription of the video\n\n\nPublished At\nDate-time\nDate and time the video was published\n\n\nDays Since Published\nNumerical\nNumber of days since the video was published\n\n\nView Count\nNumerical\nTotal number of views on the video\n\n\nLike Count\nNumerical\nNumber of likes on the video\n\n\nComment Count\nNumerical\nNumber of comments on the video\n\n\nComments\nCategorical\nList of 10 comments on the video\n\n\nSubscriber Count\nNumerical\nNumber of subscribers to the channel\n\n\nCategory ID\nCategorical\nYouTube category ID for the video\n\n\nDefinition\nCategorical\nQuality definition of the video\n\n\nDuration\nCategorical\nDuration of the video in a human-readable format\n\n\nTrack Name\nCategorical\nName of the track\n\n\nArtist Name\nCategorical\nName of the artist\n\n\nArtist\nCategorical\nName of the artist\n\n\nsinger_followers\nNumerical\nNumber of followers the singer has on Spotify\n\n\nsinger_popularity\nNumerical\nPopularity rating of the singer\n\n\nAlbum Name\nCategorical\nName of the album\n\n\nPopularity\nNumerical\nPopularity rating of the track\n\n\nDuration (ms)\nNumerical\nDuration of the track in milliseconds\n\n\nTrack ID\nCategorical\nUnique identifier for the track\n\n\nSpotify URL\nCategorical\nURL to the track on Spotify\n\n\nMean Sentiment Score\nNumerical\nAverage sentiment score of comments\n\n\nProcessed_Comments\nCategorical\nProcessed list of comments\n\n\nDuration_seconds\nNumerical\nDuration of the video in seconds\n\n\ngenre_label\nCategorical\nCategorical label for the genre"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#summary-and-interpretation-of-results",
    "href": "technical-details/data-cleaning/main.html#summary-and-interpretation-of-results",
    "title": "Data Cleaning",
    "section": "",
    "text": "The preprocessing steps significantly cleaned and transformed the raw data, making it suitable for accurate and insightful analysis. The sentiment scores provide a quantitative measure of public perception, which, combined with other song metrics, can be used to gauge popularity and engagement."
  },
  {
    "objectID": "technical-details/data-collection/main.html",
    "href": "technical-details/data-collection/main.html",
    "title": "Data Collection",
    "section": "",
    "text": "In this section, provide a high-level overview for technical staff, summarizing the key tasks and processes carried out here. Include the following elements:\n\n\nIdentify the key factors affecting the popularity official music video( view count) on Youtube by analyzing various features of official mv on Youtube and characteristics of corresponding songs on Spotify.\n\n\n\nView count of music video on YouTube, the world’s largest video- sharing platform is crucial for measuring the popularity and success of mv and song.\nUnderstanding the key factors that drive and impact the mv popularity can help music producers, singers improve mv content and promotion strategies.\nNowdays, it seems that few people have quantified the key factors needed to achieve a successful mv based on mv data, and most people trust their own industry experience, intuition, and artistic aesthetics, etc.\nCombing the data of Youtube and Spotify provides us with a more comprehensive view, since the user behavior on these two platforms are likely to influence each other.\n\n\n\nDetermine the primary factors that impact the mv view count, including: - YouTube features: days since published, like count, comment count, mean comment sentiment score, etc. - Spotify features: singer_popularity, duration , song_popularity, etc.\nExplore the correlation between Youtube and Spotify features and mv view count to identify the key traits.\nBased on the analysis, summarize and propose mv optimization suggestions to increase mv view count for singers, music producers and record labels.\nThis overview should give technical staff a clear understanding of the context and importance of the work, while guiding them on what to focus on in the details that follow."
  },
  {
    "objectID": "technical-details/data-collection/main.html#goals",
    "href": "technical-details/data-collection/main.html#goals",
    "title": "Data Collection",
    "section": "",
    "text": "Identify the key factors affecting the popularity official music video( view count) on Youtube by analyzing various features of official mv on Youtube and characteristics of corresponding songs on Spotify."
  },
  {
    "objectID": "technical-details/data-collection/main.html#motivation",
    "href": "technical-details/data-collection/main.html#motivation",
    "title": "Data Collection",
    "section": "",
    "text": "View count of music video on YouTube, the world’s largest video- sharing platform is crucial for measuring the popularity and success of mv and song.\nUnderstanding the key factors that drive and impact the mv popularity can help music producers, singers improve mv content and promotion strategies.\nNowdays, it seems that few people have quantified the key factors needed to achieve a successful mv based on mv data, and most people trust their own industry experience, intuition, and artistic aesthetics, etc.\nCombing the data of Youtube and Spotify provides us with a more comprehensive view, since the user behavior on these two platforms are likely to influence each other."
  },
  {
    "objectID": "technical-details/data-collection/main.html#objectives",
    "href": "technical-details/data-collection/main.html#objectives",
    "title": "Data Collection",
    "section": "",
    "text": "Determine the primary factors that impact the mv view count, including: - YouTube features: days since published, like count, comment count, mean comment sentiment score, etc. - Spotify features: singer_popularity, duration , song_popularity, etc.\nExplore the correlation between Youtube and Spotify features and mv view count to identify the key traits.\nBased on the analysis, summarize and propose mv optimization suggestions to increase mv view count for singers, music producers and record labels.\nThis overview should give technical staff a clear understanding of the context and importance of the work, while guiding them on what to focus on in the details that follow."
  },
  {
    "objectID": "technical-details/data-collection/main.html#data-collection-process",
    "href": "technical-details/data-collection/main.html#data-collection-process",
    "title": "Data Collection",
    "section": "Data Collection Process",
    "text": "Data Collection Process\nThis project collects music data through YouTube and Spotify APIs, covering information on the works of 20 representative artists in five genres: Electronic, Jazz, Hip-Hop, Pop and Rock. The data processing flow is as follows:\n\n1. YouTube Data Collection\n\n1.1 Acquiring Official Music Video Data\n\nfrom googleapiclient.discovery import build\nimport pandas as pd\nfrom datetime import datetime\nimport dateutil.parser\n\n# API Key\napi_key = \"AIzaSyDtKE-4QZj6EA-rwG7cj5gMJxdt4Fe14Nw\"\n\n# Initialize YouTube API client\nyoutube = build('youtube', 'v3', developerKey=api_key)\n\n# List to store data\nall_data = []\n\n# Read song data and fetch YouTube statistics\n# We should have put 20*5 names of singers, but for the sake of presentation, we choose three singers as a demonstration here \nwith open('find.txt', 'r') as file:\n    for line in file:\n        artist = line.strip()\n        query = f\"{artist} official music video\"\n\n        # Search request for the query\n        search_request = youtube.search().list(\n            part=\"snippet\",\n            q=query,\n            maxResults=5,\n            type=\"video\",\n            order='relevance'\n        )\n        search_response = search_request.execute()\n\n        for item in search_response['items']:\n            video_id = item['id']['videoId']\n            video_request = youtube.videos().list(\n                part=\"snippet,contentDetails,statistics\",\n                id=video_id\n            )\n            video_response = video_request.execute()\n\n            for video in video_response['items']:\n                snippet = video['snippet']\n                content_details = video['contentDetails']\n                statistics = video['statistics']\n\n                # Calculate days since video was published\n                published_at = dateutil.parser.parse(snippet['publishedAt'])\n                days_since_published = (datetime.now(published_at.tzinfo) - published_at).days\n\n                # Fetch channel details for subscriber count\n                channel_response = youtube.channels().list(\n                    part='statistics',\n                    id=snippet['channelId']\n                ).execute()\n                subscriber_count = channel_response['items'][0]['statistics'].get('subscriberCount', '0')\n\n                # Fetch comments\n                comments_request = youtube.commentThreads().list(\n                    part='snippet',\n                    videoId=video_id,\n                    order='relevance',\n                    maxResults=10\n                )\n                comments_response = comments_request.execute()\n\n                comments = [comment['snippet']['topLevelComment']['snippet']['textDisplay']\n                            for comment in comments_response.get('items', [])]\n\n                # Store all data in a dictionary\n                video_data = {\n                    'Video ID': video_id,\n                    'Title': snippet['title'],\n                    'Description': snippet['description'],\n                    'Published At': snippet['publishedAt'],\n                    'Days Since Published': days_since_published,\n                    'View Count': statistics.get('viewCount', '0'),\n                    'Like Count': statistics.get('likeCount', '0'),\n                    'Comment Count': statistics.get('commentCount', '0'),\n                    'Comments': comments,\n                    'Subscriber Count': subscriber_count,\n                    'Category ID': snippet['categoryId'],\n                    'Definition': content_details['definition'],\n                    'Duration': content_details['duration']\n                }\n\n                all_data.append(video_data)\n\n# Convert the list to a DataFrame\nfinal_df = pd.DataFrame(all_data)\n\n# Optionally save the DataFrame to a CSV file\nfinal_df.to_csv('Example_Detailed_YouTube_Video_Data.csv', index=False)\n\n# Display the DataFrame\nprint(final_df.head())\n\n      Video ID                                              Title  \\\n0  1VQ_3sBZEm0    Foo Fighters - Learn To Fly (Official HD Video)   \n1  eBG7P-K-r1Y        Foo Fighters - Everlong (Official HD Video)   \n2  SBjQ9tuuTJQ                       Foo Fighters - The Pretender   \n3  EqWRaAF6_WY         Foo Fighters - My Hero (Official HD Video)   \n4  h_L4Rixya64  Foo Fighters - Best Of You (Official Music Video)   \n\n                                         Description          Published At  \\\n0  Foo Fighters' official music video for 'Learn ...  2009-10-03T04:46:13Z   \n1  \"Everlong\" by Foo Fighters \\nListen to Foo Fig...  2009-10-03T04:49:58Z   \n2  Watch the official music video for \"The Preten...  2009-10-03T04:46:14Z   \n3  \"My Hero\" by Foo Fighters \\nListen to Foo Figh...  2011-03-18T19:35:42Z   \n4  Watch the official music video for \"Best Of Yo...  2009-10-03T20:49:33Z   \n\n   Days Since Published View Count Like Count Comment Count  \\\n0                  5550  183921366     808172         33856   \n1                  5550  324414087    1821270         53201   \n2                  5550  588092620    2785700         92245   \n3                  5018   87531478     564448         26099   \n4                  5549  265573360    1281212         34999   \n\n                                            Comments Subscriber Count  \\\n0  [I’m just realising how great Dave grohls acti...          1290000   \n1  [Dad died today. \\r&lt;br&gt;1:20 am.\\r&lt;br&gt;A five da...          1290000   \n2  [So thankful for this awesome song. I&#39;ll b...          1290000   \n3  [My son and I was supposed to spend the summer...          1290000   \n4  [The emotion in his face and his voice transce...          1290000   \n\n  Category ID Definition Duration  \n0          10         hd  PT4M37S  \n1          10         hd  PT4M52S  \n2          10         hd  PT4M31S  \n3          10         hd   PT4M3S  \n4          10         hd  PT4M16S  \n\n\n\n\n1.2 Merge artist name and music genre into csv\nThe row of the initial find.csv(include artist name and genre) is repeated five times per row to correspond to the five mv chosen by each artist (python) Then merge these two columns into the csv (copy manually)\n\nimport pandas as pd\n\ninput_file = './find.csv'  \noutput_file = './Example_singer_info.csv'  \n\n# Load the input CSV file\ndf = pd.read_csv(input_file)\n\n# Create an empty DataFrame to store repeated rows\nrepeated_df = pd.DataFrame()\n\n# Repeat each row 5 times and append it to the new DataFrame\nfor i in range(len(df)):\n    repeated_df = pd.concat([repeated_df, pd.DataFrame([df.iloc[i]] * 5)], ignore_index=True)\n\n# Save the processed DataFrame to a new CSV file\nrepeated_df.to_csv(output_file, index=False)\n\nprint(f\"Data processed successfully and saved as {output_file}\")\n\nData processed successfully and saved as ./Example_singer_info.csv\n\n\n\n\n1.3 Data preprocessing\nExtract the song name from the csv’s title and generate a new CSV file containing the singer’s and song’s name.\n\nimport pandas as pd\nimport re\n\n# Load the CSV file with YouTube video data\ndf = pd.read_csv('./Example_Detailed_YouTube_Video_Data.csv', encoding='MacRoman')\n\n# Function to extract the song name from the title\ndef extract_song_name(title):\n    # Use regular expression to find text between \" - \" and \"(\"\n    match = re.search(r' - (.*?) \\(.*\\)', title)\n    if match:\n        return match.group(1) \n    else:\n        return title  \n\n# Create a new DataFrame with extracted song names\nnew_df = pd.DataFrame({\n    'Extracted Song Name': df['Title'].apply(extract_song_name)\n})\n\n# Save the new DataFrame to a CSV file in the current directory\noutput_file = './Example_extracted_song_names.csv'\nnew_df.to_csv(output_file, index=False, encoding='MacRoman')\n\nprint(f\"Song titles extracted and saved to {output_file}\")\n\nSong titles extracted and saved to ./Example_extracted_song_names.csv\n\n\n\n\ninput_file = './Example_extracted_song_names.csv'\ndf = pd.read_csv(input_file, encoding='MacRoman')\n\n# Function to remove content inside brackets (e.g., [example])\ndef remove_brackets(text):\n    return re.sub(r'\\[.*?\\]', '', text)\n\n# Apply the function to the 'Extracted Song Name' column\ndf['Extracted Song Name'] = df['Extracted Song Name'].apply(remove_brackets)\n\n\noutput_file = './Example_extracted_song_names_cleaned.csv'\ndf.to_csv(output_file, index=False)\n\nprint(f\"Processed data saved to {output_file}\")\n\nProcessed data saved to ./Example_extracted_song_names_cleaned.csv\n\n\nManually merge example_extracted_song_name_cleaned.csv with example_artist_info.csv\n\n\n\n2. Spotify Collection\n\n2.1 Acquiring Track Information\n\nimport pandas as pd\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\n\n# Set up Spotify client credentials\nclient_credentials_manager = SpotifyClientCredentials(\n    client_id='3e1596de002340b898f5d10c9aeae4ea',\n    client_secret='526fa44678974475b0f6ba5d8efd16c4'\n)\nsp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n\n# Input and output file paths\ninput_file = './Example_extracted_song_names_cleaned.csv'\noutput_file = './Example_spotify_track_info.csv'\n\n# Load the input CSV file\ndf = pd.read_csv(input_file, encoding='MacRoman')\n\n# Initialize a list to store results\nresults = []\n\n# Process each row in the DataFrame\nfor _, row in df.iterrows():\n    song = row['Extracted Song Name']\n    artist = row['singer']\n    query = f'{song} {artist}'  # Concatenate song and artist for the search query\n\n    try:\n        # Search for the track on Spotify\n        result = sp.search(q=query, limit=1, type='track')\n        tracks = result.get('tracks', {}).get('items', [])\n\n        if tracks:\n            # If a track is found, extract its details\n            track = tracks[0]\n            track_info = {\n                'Track Name': track['name'],\n                'Artist Name': track['artists'][0]['name'],\n                'Album Name': track['album']['name'],\n                'Popularity': track['popularity'],\n                'Duration (ms)': track['duration_ms'],\n                'Track ID': track['id'],\n                'Spotify URL': track['external_urls']['spotify']\n            }\n            print(f\"Found: {track_info['Track Name']} by {track_info['Artist Name']}\")\n        else:\n            # If no track is found, append placeholders\n            print(f\"Track not found: {query}\")\n            track_info = {\n                'Track Name': song,\n                'Artist Name': artist,\n                'Album Name': None,\n                'Popularity': None,\n                'Duration (ms)': None,\n                'Track ID': None,\n                'Spotify URL': None\n            }\n\n        # Append the result to the list\n        results.append(track_info)\n\n    except Exception as e:\n        # Handle exceptions during the search\n        print(f\"Error processing query '{query}': {e}\")\n        results.append({\n            'Track Name': song,\n            'Artist Name': artist,\n            'Album Name': None,\n            'Popularity': None,\n            'Duration (ms)': None,\n            'Track ID': None,\n            'Spotify URL': None\n        })\n\n# Create a DataFrame from the results\noutput_df = pd.DataFrame(results)\n\n# Save the output DataFrame to a CSV file\noutput_df.to_csv(output_file, index=False, encoding='utf-8')\n\nprint(f\"Processed data saved to: {output_file}\")\n\nFound: Learn to Fly by Foo Fighters\nFound: Everlong by Foo Fighters\nFound: The Pretender by Foo Fighters\nFound: My Hero by Foo Fighters\nFound: Best of You by Foo Fighters\nFound: Mr. Brightside by The Killers\nFound: When You Were Young by The Killers\nFound: Mr. Brightside by The Killers\nFound: Somebody Told Me by The Killers\nFound: One Empty Grave by A Sound of Thunder\nFound: Basket Case by Green Day\nFound: When I Come Around by Green Day\nFound: American Idiot by Green Day\nFound: Boulevard of Broken Dreams by Green Day\nFound: Wake Me up When September Ends by Green Day\nProcessed data saved to: ./Example_spotify_track_info.csv\n\n\n\n\n2.2 Obtaining Artist Information\n\nimport pandas as pd\nimport requests\n\n# Function to obtain an access token for Spotify API\ndef get_access_token(client_id, client_secret):\n    auth_url = 'https://accounts.spotify.com/api/token'\n    auth_data = {\n        'grant_type': 'client_credentials',\n        'client_id': client_id,\n        'client_secret': client_secret\n    }\n    response = requests.post(auth_url, data=auth_data)\n    if response.status_code == 200:\n        return response.json()['access_token']\n    else:\n        raise Exception('Failed to obtain access token')\n\n# Function to get the Spotify artist ID using the artist name\ndef get_artist_id(artist_name, access_token):\n    search_url = f'https://api.spotify.com/v1/search?q={artist_name}&type=artist&limit=1'\n    headers = {'Authorization': f'Bearer {access_token}'}\n    response = requests.get(search_url, headers=headers)\n    if response.status_code == 200:\n        search_results = response.json()\n        artists = search_results['artists']['items']\n        if artists:\n            return artists[0]['id']\n        else:\n            return None\n    else:\n        return None\n\n# Function to retrieve the artist's followers and popularity\ndef get_artist_followers(artist_id, access_token):\n    artist_url = f'https://api.spotify.com/v1/artists/{artist_id}'\n    headers = {'Authorization': f'Bearer {access_token}'}\n    response = requests.get(artist_url, headers=headers)\n    if response.status_code == 200:\n        artist_data = response.json()\n        return artist_data['followers']['total'], artist_data['popularity']\n    else:\n        return None, None\n\n# Load the input CSV file \ninput_file = './Example_singer_info.csv' \ndf = pd.read_csv(input_file)\n\n# Spotify API credentials\nclient_id = '31aba57b31344fdebf98f51375d07834' \nclient_secret = '2c4f929786784910bc9a843518785cae'  \n\n# Obtain Spotify API access token\naccess_token = get_access_token(client_id, client_secret)\n\n# Initialize lists to store followers and popularity data\nfollowers_list = []\npopularity_list = []\n\n# Process each artist in the DataFrame\nfor artist_name in df['artist']:\n    artist_id = get_artist_id(artist_name, access_token)\n    if artist_id:\n        followers, popularity = get_artist_followers(artist_id, access_token)\n        followers_list.append(followers)\n        popularity_list.append(popularity)\n    else:\n        # If the artist is not found, append None\n        followers_list.append(None)\n        popularity_list.append(None)\n\n# Add followers and popularity data to the DataFrame\ndf['Followers'] = followers_list\ndf['Popularity'] = popularity_list\n\n# Save the updated DataFrame to a new CSV file\noutput_file = './Example_artist_data_with_followers_and_popularity.csv'  \ndf.to_csv(output_file, index=False)\n\nprint(f\"Processed data saved to: {output_file}\")\n\nProcessed data saved to: ./Example_artist_data_with_followers_and_popularity.csv\n\n\n\n\n\n3. Data integration and cleaning\nFirst manually merge Spotify and YouTube csv. Then using Spotify and YouTube artists as the matching key, match to verify artist match, if not, then delete the mismatched rows.\n\nimport pandas as pd\n\ninput_file = './Example_Detailed_YouTube_Video_Data.csv' \ndf = pd.read_csv(input_file, encoding='MacRoman')\n\n# Filter rows where 'Artist Name' matches 'Artist'\ndf_cleaned = df[df['Artist Name'] == df['artist']]\n\n# Save the cleaned DataFrame to a new CSV file\noutput_file = './Example_spotify_youtube.csv'  \ndf_cleaned.to_csv(output_file, index=False)\nprint(f\"Cleaned data saved to: {output_file}\")\n\nCleaned data saved to: ./Example_spotify_youtube.csv\n\n\nThen we completed all the data collection steps!"
  },
  {
    "objectID": "technical-details/data-collection/main.html#challenges",
    "href": "technical-details/data-collection/main.html#challenges",
    "title": "Data Collection",
    "section": "Challenges",
    "text": "Challenges\nI encountered several technical challenges during the data collection process. First, when selecting artists, some singers may have turned off the music video comment section on YouTube, which caused me to have to re-search for other artists. In addition, Spotify’s search engine is not always accurate, and even if I provide the song title and artist name, I still may not be able to find the target song. This is an issue for which I haven’t found a perfect solution yet, considering that I may need to further clean up the song titles to make them more accurate in order to improve the accuracy of my searches. Although the original plan was to acquire 500 pieces of data, in the end only 376 pieces of data were successfully acquired. So we may have to make real-time adjustments when it comes to data collection.\nAnother technical difficulty is that the YouTube API sometimes returns irrelevant data, so it is important to carefully review and filter out the results that meet the requirements. In addition, the use of matching key to find relevant data in the YouTube and Spotify APIs is very important to improve the matching accuracy and reliability of the data."
  },
  {
    "objectID": "technical-details/data-collection/main.html#benchmarks",
    "href": "technical-details/data-collection/main.html#benchmarks",
    "title": "Data Collection",
    "section": "Benchmarks",
    "text": "Benchmarks\nThe challenge of the project was to optimize the use of the API and reduce the interference of irrelevant data. While most projects can collect large amounts of data through APIs, accurately finding the target data is still a common difficulty. Therefore, how to improve the quality of data through data cleansing and precise matching algorithms remains a direction for further research and improvement in the future."
  },
  {
    "objectID": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "title": "Data Collection",
    "section": "Conclusion and Future Steps",
    "text": "Conclusion and Future Steps\nWe can conclude that firstly, during the data collection process, despite the convenience provided by the use of APIs, the issues of exact matching and removal of irrelevant data still need to be addressed. Second, Spotify’s search engine may need further optimization or more detailed preprocessing of the input data to improve the accuracy of the matches.\nFuture steps could focus on optimizing the process of data collection and cleansing. For example, for the Spotify search problem, attempts could be made to improve the method of standardization of song titles or validation in combination with other data sources. On the other hand, further research can be done to improve the efficiency of data filtering in the YouTube API to ensure that the data collected is more in line with expectations and less interfered by irrelevant information. These improvements will help enhance the efficiency and quality of data collection and lay a solid foundation for subsequent analysis."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html",
    "href": "technical-details/supervised-learning/main.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "This analysis aims to explore the factors influencing the popularity of content on a digital platform. We will use various regression models to predict popularity based on multiple features extracted from our dataset.\n\n\n\nWe will use Support Vector Regression (SVR) and linear models like Ridge and Lasso regression.\nimport pandas as pd\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\ndf = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\n\n# Specify the feature columns\nfeatures = [\n    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',\n    'Subscriber Count', 'Definition', 'Mean Sentiment Score',\n    'Duration_seconds', 'genre_label', 'singer_followers', 'singer_popularity'\n]\n\n# Ensure the target column 'popularity' exists in DataFrame\nif 'Popularity' not in df.columns:\n    raise ValueError(\"The 'popularity' column is missing from the DataFrame.\")\n\n# Split into input (X) and target (y)\nX = df[features]  # Inputs\ny = df['Popularity']  # Target\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Assuming the SVR model is already trained as svr_model\nsvr_model = SVR(kernel='rbf', C=100, gamma='auto')\nsvr_model.fit(X_train, y_train)\n\n# Predict on the testing data\ny_pred = svr_model.predict(X_test)\n# Calculate mean squared error and R^2 score\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Mean Squared Error: {mse}, R^2 Score: {r2}\")\n\n# Perform permutation importance\nperm_importance = permutation_importance(svr_model, X_test, y_test, n_repeats=30, random_state=42)\n\n# Get importance scores\nimportance_scores = perm_importance.importances_mean\n\n# Print feature importance\nprint(\"Feature importances:\")\nfor i, feature in enumerate(features):\n    print(f\"{feature}: {importance_scores[i]}\")\n\n\n\n\n\n\n\n\n\n\n\nModel\nMSE\nR² Score\n\n\n\n\nSVR\n99.74699515216331\n0.568546463925952\n\n\nLinear Regression\n123.52107678768438\n0.46571217229730644\n\n\nRidge Regression\n125.61840469866303\n0.4566402243943334\n\n\nLasso Regression\n124.26131518891665\n0.46251028661381044\n\n\n\nFeature importances from SVR: - Days Since Published: 0.14590051030422282 - View Count: 0.0689113148502238 - Like Count: 0.06300639647560934 - Comment Count: 0.004829835437846231 - Subscriber Count: 0.15936065653760564 - Definition: 0.05992188839233011 - Mean Sentiment Score: 0.037774091955604584 - Duration_seconds: 0.00110286673615743 - genre_label: 0.1581978297916415 - singer_followers: 0.0360773423716881 - singer_popularity: 0.6712934520249385\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.barh(features, importance_scores, color='skyblue')\nplt.xlabel('Importance Score')\nplt.title('Feature Importance Scores')\nplt.gca().invert_yaxis()  # Invert y-axis to have the highest score on top\nplt.show()\n\n\nThe Support Vector Regression (SVR) model outperformed the linear models in terms of Mean Squared Error (MSE) and R² score. The lower MSE and higher R² of the SVR indicate better performance in fitting the data compared to the Linear, Ridge, and Lasso regressions. The R² scores suggest that the SVR model was able to explain approximately 56.85% of the variance in the dataset, which is more than the approximately 46.57% by the Linear Regression, 45.66% by Ridge, and 46.25% by Lasso Regression.\n\n\n\nFrom the SVR model’s permutation importance, ‘singer_popularity’ emerged as the most influential feature, significantly impacting the prediction of a song’s popularity. This suggests that more popular singers tend to have more popular songs, highlighting the influence of an artist’s existing reputation on new releases."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#introduction-and-motivation",
    "href": "technical-details/supervised-learning/main.html#introduction-and-motivation",
    "title": "Supervised Learning",
    "section": "",
    "text": "This analysis aims to explore the factors influencing the popularity of content on a digital platform. We will use various regression models to predict popularity based on multiple features extracted from our dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#overview-of-methods",
    "href": "technical-details/supervised-learning/main.html#overview-of-methods",
    "title": "Supervised Learning",
    "section": "",
    "text": "We will use Support Vector Regression (SVR) and linear models like Ridge and Lasso regression.\nimport pandas as pd\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\ndf = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\n\n# Specify the feature columns\nfeatures = [\n    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',\n    'Subscriber Count', 'Definition', 'Mean Sentiment Score',\n    'Duration_seconds', 'genre_label', 'singer_followers', 'singer_popularity'\n]\n\n# Ensure the target column 'popularity' exists in DataFrame\nif 'Popularity' not in df.columns:\n    raise ValueError(\"The 'popularity' column is missing from the DataFrame.\")\n\n# Split into input (X) and target (y)\nX = df[features]  # Inputs\ny = df['Popularity']  # Target\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Assuming the SVR model is already trained as svr_model\nsvr_model = SVR(kernel='rbf', C=100, gamma='auto')\nsvr_model.fit(X_train, y_train)\n\n# Predict on the testing data\ny_pred = svr_model.predict(X_test)\n# Calculate mean squared error and R^2 score\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Mean Squared Error: {mse}, R^2 Score: {r2}\")\n\n# Perform permutation importance\nperm_importance = permutation_importance(svr_model, X_test, y_test, n_repeats=30, random_state=42)\n\n# Get importance scores\nimportance_scores = perm_importance.importances_mean\n\n# Print feature importance\nprint(\"Feature importances:\")\nfor i, feature in enumerate(features):\n    print(f\"{feature}: {importance_scores[i]}\")"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#conclusion",
    "href": "technical-details/supervised-learning/main.html#conclusion",
    "title": "Supervised Learning",
    "section": "",
    "text": "Model\nMSE\nR² Score\n\n\n\n\nSVR\n99.74699515216331\n0.568546463925952\n\n\nLinear Regression\n123.52107678768438\n0.46571217229730644\n\n\nRidge Regression\n125.61840469866303\n0.4566402243943334\n\n\nLasso Regression\n124.26131518891665\n0.46251028661381044\n\n\n\nFeature importances from SVR: - Days Since Published: 0.14590051030422282 - View Count: 0.0689113148502238 - Like Count: 0.06300639647560934 - Comment Count: 0.004829835437846231 - Subscriber Count: 0.15936065653760564 - Definition: 0.05992188839233011 - Mean Sentiment Score: 0.037774091955604584 - Duration_seconds: 0.00110286673615743 - genre_label: 0.1581978297916415 - singer_followers: 0.0360773423716881 - singer_popularity: 0.6712934520249385\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.barh(features, importance_scores, color='skyblue')\nplt.xlabel('Importance Score')\nplt.title('Feature Importance Scores')\nplt.gca().invert_yaxis()  # Invert y-axis to have the highest score on top\nplt.show()\n\n\nThe Support Vector Regression (SVR) model outperformed the linear models in terms of Mean Squared Error (MSE) and R² score. The lower MSE and higher R² of the SVR indicate better performance in fitting the data compared to the Linear, Ridge, and Lasso regressions. The R² scores suggest that the SVR model was able to explain approximately 56.85% of the variance in the dataset, which is more than the approximately 46.57% by the Linear Regression, 45.66% by Ridge, and 46.25% by Lasso Regression.\n\n\n\nFrom the SVR model’s permutation importance, ‘singer_popularity’ emerged as the most influential feature, significantly impacting the prediction of a song’s popularity. This suggests that more popular singers tend to have more popular songs, highlighting the influence of an artist’s existing reputation on new releases."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#introduction-and-motivation-1",
    "href": "technical-details/supervised-learning/main.html#introduction-and-motivation-1",
    "title": "Supervised Learning",
    "section": "Introduction and Motivation",
    "text": "Introduction and Motivation\nThe goal of this section is to predict whether a song is considered “popular” using binary classification methods. We aim to understand the features that significantly influence song popularity on digital platforms."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#overview-of-methods-1",
    "href": "technical-details/supervised-learning/main.html#overview-of-methods-1",
    "title": "Supervised Learning",
    "section": "Overview of Methods",
    "text": "Overview of Methods\nIn this section, we focus on Logistic Regression for binary classification. Logistic Regression is chosen for its ability to provide probabilities for outcomes and its interpretability.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import train_test_split\n\n# Assume df is your DataFrame and the preprocessing has been done to define 'is_popular'\nfeatures = [\n    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',\n    'Subscriber Count', 'Definition', 'Mean Sentiment Score',\n    'Duration_seconds', 'genre_label', 'singer_followers', 'singer_popularity'\n]\nX = df[features]\ny = df['is_popular']\n\n# Splitting the dataset and under-sampling\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n\n# Logistic Regression model\nlr_model = LogisticRegression(random_state=42, max_iter=1000)\nlr_model.fit(X_resampled, y_resampled)\n\n# Making predictions and evaluating the model\ny_pred = lr_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclass_report = classification_report(y_test, y_pred)\n\nprint(\"Logistic Regression Model Evaluation\")\nprint(\"Accuracy:\", accuracy)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\nprint(\"Classification Report:\\n\", class_report)"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#results",
    "href": "technical-details/supervised-learning/main.html#results",
    "title": "Supervised Learning",
    "section": "Results",
    "text": "Results\nThe Logistic Regression model showed an accuracy of 77.33%, with a detailed classification report indicating precision, recall, and F1-score for both classes.\n\nModel Performance Summary\nComparison of model performance metrics for different classification models used in predicting song popularity:\n\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nPrecision\nRecall\nF1-Score\n\n\n\n\nLogistic Regression\n77.33%\n0.96 (False), 0.35 (True)\n0.77 (False), 0.80 (True)\n0.85 (False), 0.48 (True)\n\n\nSVM (Best Kernel)\n75.00%\n0.91 (False), 0.26 (True)\n0.78 (False), 0.50 (True)\n0.84 (False), 0.34 (True)\n\n\nRandom Forest\n71.00%\n0.92 (False), 0.25 (True)\n0.72 (False), 0.60 (True)\n0.81 (False), 0.35 (True)"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#conclusion-1",
    "href": "technical-details/supervised-learning/main.html#conclusion-1",
    "title": "Supervised Learning",
    "section": "Conclusion",
    "text": "Conclusion\nThe Logistic Regression model, adjusted for class imbalance via under-sampling, provided satisfactory classification results, proving effective for identifying popular songs."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#result-interpretation-1",
    "href": "technical-details/supervised-learning/main.html#result-interpretation-1",
    "title": "Supervised Learning",
    "section": "Result Interpretation",
    "text": "Result Interpretation\nThe model was particularly strong in identifying non-popular songs (class ‘False’) with high precision and recall. And it has a higher recall score on popular songs compared to other models."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-performance-comparison",
    "href": "technical-details/supervised-learning/main.html#model-performance-comparison",
    "title": "Supervised Learning",
    "section": "Model Performance Comparison",
    "text": "Model Performance Comparison\nThis model was compared to other binary classification models such as SVM and Random Forest. Logistic Regression was chosen for its balance between performance and interpretability in this specific context. And it has a higher recall score on popular songs compared to other models."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#introduction-and-motivation-2",
    "href": "technical-details/supervised-learning/main.html#introduction-and-motivation-2",
    "title": "Supervised Learning",
    "section": "Introduction and Motivation",
    "text": "Introduction and Motivation\nThe goal of this multi- class classification is to predict the interaction rate (like ratio= like count/ view count) of youtube mv. We want to predict the interaction rate of the content by features such as number of days since posting and number of comments.\nThis analysis firstly hopes to improve data analysis and model selection capabilities, and secondly tries to help content creators optimize their creation strategies."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#overview-of-methods-2",
    "href": "technical-details/supervised-learning/main.html#overview-of-methods-2",
    "title": "Supervised Learning",
    "section": "Overview of Methods",
    "text": "Overview of Methods\nMultiple classification algorithms were used to predict the popularity (like ratio) of short video content. The like ratio was first categorized into three categories: low (≤0.7), medium (0.7-1.6), and high (&gt;1.6), and then predicted using three models: logistic regression, decision tree, and random forest. To address data imbalance, SMOTE oversampling technique was used. The input features of the model included nine variables such as days of posting/comments, number of subscribers, sentiment score, video duration, number of creator followers, and popularity. The decision tree model was hyper-parametrically optimized by grid search (GridSearchCV) and the model performance was evaluated using cross-validation. This combination of methods enables comprehensive assessment and prediction of video content popularity, providing data support for content creation and platform operation."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#code",
    "href": "technical-details/supervised-learning/main.html#code",
    "title": "Supervised Learning",
    "section": "Code",
    "text": "Code\nlogistic regression\nX = df[features]\ny = df['Like Ratio Category']\n\n# Data set splitting\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Feature standardization\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n\n\n# Initialize the Logistic Regression model (One-vs-Rest for multi-class)\nmodel = LogisticRegression(multi_class='ovr', max_iter=1000, random_state=42)\n\n# Train the model\nmodel.fit(X_train_scaled, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test_scaled)\n\nX_scaled = scaler.fit_transform(X)  # Fit and transform the entire dataset\n\n# Cross-validation to get a better estimate of the model performance\ncross_val_scores = cross_val_score(model, X_scaled, y, cv=5)\nprint(\"\\nCross-validation scores:\", cross_val_scores)\nprint(\"\\nMean cross-validation score:\", cross_val_scores.mean())\n\n# Evaluate the model\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n#param_grid\nparam_grid_refined = {\n    'max_depth': [3,4,5],  \n    'min_samples_split': [8,10,15],\n    'min_samples_leaf': [3,4,5,6],   \n    'criterion': ['entropy'],        \n    'class_weight': ['balanced']  \n}\n\ngrid_search_refined = GridSearchCV(\n    DecisionTreeClassifier(random_state=42),\n    param_grid_refined,\n    cv=5,\n    scoring='f1_macro', \n    n_jobs=-1\n)\n\n# train mode\ngrid_search_refined.fit(X_train_scaled, y_train)\n\nprint(\"Refined Best parameters:\", grid_search_refined.best_params_)\nprint(\"Refined Best cross-validation score:\", grid_search_refined.best_score_)\n\nbest_model_refined = DecisionTreeClassifier(\n    **grid_search_refined.best_params_\n)\n\nbest_model_refined.fit(X_train_scaled, y_train)\n\ny_pred_refined = best_model_refined.predict(X_test_scaled)\nprint(\"\\nRefined Test set accuracy:\", accuracy_score(y_test, y_pred_refined))\nprint(\"\\nRefined Classification Report:\\n\", classification_report(y_test, y_pred_refined))\nrandom forest\nf_model = RandomForestClassifier(n_estimators=100)\n\n# Train the model\nrf_model.fit(X_train_scaled, y_train)\n\n# Predict on the test set\ny_pred_rf = rf_model.predict(X_test_scaled)\n\n# Evaluate the model\nprint(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\nprint(\"\\nRandom Forest Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n\n# Cross-validation to get a better estimate of the model performance\ncross_val_scores_rf = cross_val_score(rf_model, X_train_scaled, y_train, cv=5)\nprint(\"\\nRandom Forest Cross-validation scores:\", cross_val_scores_rf)\nprint(\"\\nMean cross-validation score (Random Forest):\", cross_val_scores_rf.mean())"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#result",
    "href": "technical-details/supervised-learning/main.html#result",
    "title": "Supervised Learning",
    "section": "Result",
    "text": "Result\n\nModel performance Result\nComparison of Model Performance Metrics for Different Classification Models Used in Predicting like_ratio (Multi-Class Classification)\n\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nPrecision\nRecall\nF1-Score\n\n\n\n\nLogistic Regression\n77.33%\n0.64 (High), 0.86 (Low), 0.73 (Medium)\n0.82 (High), 0.86 (Low), 0.66 (Medium)\n0.72 (High), 0.86 (Low), 0.69 (Medium)\n\n\nDecision Tree\n74.67%\n0.56 (High), 0.85 (Low), 0.74 (Medium)\n0.91 (High), 0.83 (Low), 0.59 (Medium)\n0.69 (High), 0.84 (Low), 0.65 (Medium)\n\n\nRandom Forest\n78.67%\n0.69 (High), 0.88 (Low), 0.73 (Medium)\n0.82 (High), 0.80 (Low), 0.76 (Medium)\n0.75 (High), 0.84 (Low), 0.75 (Medium)\n\n\n\nCross-Validation Scores\n\n\n\n\n\n\n\n\nModel\nCross-Validation Scores\nMean Cross-Validation Score\n\n\n\n\nLogistic Regression\n[0.6533, 0.6133, 0.8133, 0.72, 0.4667]\n0.6533\n\n\nDecision Tree\nRefined Best CV: [0.6208 (balanced parameters)]\n0.6208\n\n\nRandom Forest\n[0.75, 0.6833, 0.6833, 0.7, 0.7833]\n0.72\n\n\n\n\n\nResult Interpretation\nWe can see from the table that the Random Forest model is the most effective model for predicting the like_ratio, achieving the highest accuracy (78.67%) and a balanced performance in terms of Precision, Recall, and F1-Score across all classes (High, Low, and Medium). Its average cross-validation score is 0.72, demonstrating good generalization.\nThe Logistic regression model is second effective one with an overall accuracy of 77.33%. While the low rank Precision (0.86) is strong, the medium rank Recall (0.66) performed relatively weakly. Nonetheless, the model is metrically consistent, showing its suitability for applications that require interpretability and stable performance.\nFor the Decision Tree model, it has the lowest overall accuracy (74.67%). While it performs well in recognizing high levels with a recall of 0.91, it performs poorly in recognizing intermediate levels where it has the lowest accuracy (0.74) and recall (0.59). But it can distinguish between high & low cases well."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#insights-1",
    "href": "technical-details/supervised-learning/main.html#insights-1",
    "title": "Supervised Learning",
    "section": "Insights",
    "text": "Insights\nFrom the results, we can find that the Random Forest model shows the best balance of accuracy, precision, recall, and F1 value in predicting the like_ratio of songs, especially in the medium class (Medium). This indicates that Random Forest has a strong ability in dealing with multi-categorization problems. The Logistic Regression model, on the other hand, has high precision and recall in identifying the low like_ratio category, suggesting that it is more effective in distinguishing between low popularity songs. while the Decision Tree model’s perform is not so good as we expected among all these three models, with low recall especially in the Medium category, suggesting that it may be unsuitable for dealing with such a complex multi-class classification task\nThis multi-class classification enables me to better understand the advantages and limitations of different models in multi-class classification tasks, and teach me lesson that we should choose the right model for future projects based on the specific needs of the task."
  },
  {
    "objectID": "report/report.html",
    "href": "report/report.html",
    "title": "Final Report",
    "section": "",
    "text": "Unlocking the Power of Cross-Platform Synergy—This analysis utilizes advanced data analytics techniques across both YouTube and Spotify to understand how video views and song popularity influence each other. By integrating these platforms, we discovered significant cross-platform effects where YouTube’s metrics notably influence Spotify’s track popularity. This cross-platform synergy offers strategies to enhance content engagement across media, with potential to increase overall audience engagement.\n\n\n\nOur project aims to quantitatively assess how YouTube music video (MV) characteristics affect the popularity of corresponding songs on Spotify. This cross-platform analysis helps in understanding which factors most significantly affect song popularity on Spotify when correlated with YouTube performance metrics.\n\n\n\n\nCross-Platform Influence: A major insight is the strong correlation between YouTube MV views and Spotify song popularity, indicating that successful video marketing significantly boosts song streams.\nArtist Popularity: Both platforms show that higher artist popularity metrics significantly correlate with better performance metrics, suggesting a strong influence of artist reputation across media.\nEngagement Metrics: High engagement rates on YouTube, including likes and comments, are closely associated with higher popularity scores on Spotify, emphasizing the importance of active viewer interaction.\nGenre Impact: Specific genres that perform well on YouTube also see high stream numbers on Spotify, indicating similar audience preferences across platforms.\n\n\n\n\n\nFeature Importance:\n\n From the plot, we can see the most influential factor is Singer Popularity, markedly outstripping others like Subscriber Count and Genre Label. Features like View Count, Like Count, and Comment Count demonstrate moderate importance.\n\nDistribution of Popularity:\n\n The box plot shows the distribution of popularity scores mainly concentrated around the median near 70, indicating high overall popularity. The narrow interquartile range suggests little variability, with several outliers highlighting items of significantly lower popularity.\n\nDistribution of like_ratio:\n\n This plot shows the distribution of like ratio ,and this is a clear right-skewed distribution, meaning that most videos have like ratios clustered in the lower region, with a long tail on the right side, indicating that a few videos have unusually high like ratios. This is a reflection of the fact that most music videos have a close and low like ratio, which may represent a “normal” level of interaction. Only a few videos have a particularly high like rate, and these are likely to be “viral” or particularly successful videos.\n\nCorrelation Heatmap:\n\n Positive correlation: Like rate and comment rate are highly correlated (0.72), indicating that users’ interactive behaviors (likes and comments) are closely related. Singer popularity is highly correlated with platform popularity (0.72), indicating consistency in popularity measures between the two platforms The number of subscriptions is moderately correlated with the number of views (0.45), the more subscribers, the higher the number of views Singer popularity is also moderately correlated with channel subscriptions (0.41)\nNegative correlation: days since release is strongly negatively correlated with likes (-0.56), indicating that the longer the video has been released, the lower the likes are Singer popularity is weakly negatively correlated with comment sentiment score (-0.30), possibly indicating that more popular singers receive more negative comments\nFinally, it can be seen that singer popularity shows significant correlations with several features that can be used as important predictors of singer popularity.\n\nDistribution of Key YouTube Music Video Metrics\n\n For plot Distribution of Log10(View Count), it shows normal distribution characteristics. We can conclude that most of the videos have view counts between 107-109. For plot Like Ratio Distribution, it shows right skewed distribution. The Like Ratio of most videos is concentrated between 0-2%. For plot Comment Ratio Distribution, it shows a strong right-skewed distribution. Most videos have a comment ratio of less than 0.2%. For plot Singer Popularity Distribution, it ranges from 40-100 scores. There is a clear peak at around 80.\n\n\n\nThis project aims to help music industry practitioners better understand and grasp the market. Music producers, companies and singer teams can analyze the correlation between YouTube and Spotify platforms to allocate marketing budgets more rationally and produce more targeted content based on the characteristics of different platforms.\nAt the same time, this data can also help agencies assess the development potential of singers on Youtube, Spotify, meanwhile customize more appropriate content strategies for artists. Plus, music platform operators can draw on these analytics to improve their recommendation algorithms and optimize their content layout to enhance user experience. In addition, the data can also help industry players have a better acknowledge of the trends of the music market and understand the differences in user preferences across different platforms, so that they can formulate more accurate marketing strategies.\n\n\n\n\nTargeted Marketing Campaigns: Focus promotional efforts on artists and genres that exhibit strong cross-platform synergy.\nEnhanced Engagement Strategies: Develop content strategies that encourage viewer interaction on YouTube to leverage these engagements for increased Spotify streams.\nData-Driven Content Creation: Utilize insights from data to tailor content that resonates well across both platforms, optimizing both video and music production to cater to proven audience preferences.\n\n\n\n\nThis comprehensive analysis employs advanced data analytics techniques across both YouTube and Spotify to uncover how video views and song popularity influence each other. By integrating insights from these platforms, we have identified significant cross-platform effects where YouTube metrics considerably impact Spotify’s track popularity. Leveraging this synergy provides actionable strategies for content creators and marketers to enhance their reach and engagement across media, ultimately boosting profitability and expanding market presence while increasing overall audience engagement.\n\n\n\n\n\n\nDokuz, Y. (2024). Discovering popular and persistent tags from YouTube trending video big dataset. Multimedia Tools and Applications, 83, 10779–10797.\nKhin Nyunt, N. T., & Khin, T. (2024). YouTube Career Analysis with the Combination of Trending Analysis and Sentiments Analysis. ESS Open Archive.\nMeghana, K. (2024). Artificial Intelligence and Sentiment Analysis in YouTube Comments: A Comprehensive Overview. 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT), 1565-1572.\nGiri, R., Sirsath, M., & Kanakia, H. T. (2024). YouTube Comments Sentiment Analysis. IEEE 9th International Conference for Convergence in Technology (I2CT), Pune, India, 1-4.\nDasovich-Wilson, J. N., Thompson, M., & Saarikallio, S. (2024). The characteristics of music video experiences and their relationship to future listening outcomes. Psychology of Music, 0(0).\nEfe, I. E., Tesch, C., & Subedi, P. (2024). YouTube as a source of patient information on awake craniotomy: Analysis of content quality and user engagement. World Neurosurgery: X, 21, 100249."
  },
  {
    "objectID": "report/report.html#executive-summary",
    "href": "report/report.html#executive-summary",
    "title": "Final Report",
    "section": "",
    "text": "Unlocking the Power of Cross-Platform Synergy—This analysis utilizes advanced data analytics techniques across both YouTube and Spotify to understand how video views and song popularity influence each other. By integrating these platforms, we discovered significant cross-platform effects where YouTube’s metrics notably influence Spotify’s track popularity. This cross-platform synergy offers strategies to enhance content engagement across media, with potential to increase overall audience engagement."
  },
  {
    "objectID": "report/report.html#objective",
    "href": "report/report.html#objective",
    "title": "Final Report",
    "section": "",
    "text": "Our project aims to quantitatively assess how YouTube music video (MV) characteristics affect the popularity of corresponding songs on Spotify. This cross-platform analysis helps in understanding which factors most significantly affect song popularity on Spotify when correlated with YouTube performance metrics."
  },
  {
    "objectID": "report/report.html#key-insights",
    "href": "report/report.html#key-insights",
    "title": "Final Report",
    "section": "",
    "text": "Cross-Platform Influence: A major insight is the strong correlation between YouTube MV views and Spotify song popularity, indicating that successful video marketing significantly boosts song streams.\nArtist Popularity: Both platforms show that higher artist popularity metrics significantly correlate with better performance metrics, suggesting a strong influence of artist reputation across media.\nEngagement Metrics: High engagement rates on YouTube, including likes and comments, are closely associated with higher popularity scores on Spotify, emphasizing the importance of active viewer interaction.\nGenre Impact: Specific genres that perform well on YouTube also see high stream numbers on Spotify, indicating similar audience preferences across platforms."
  },
  {
    "objectID": "report/report.html#visualizations",
    "href": "report/report.html#visualizations",
    "title": "Final Report",
    "section": "",
    "text": "Feature Importance:\n\n From the plot, we can see the most influential factor is Singer Popularity, markedly outstripping others like Subscriber Count and Genre Label. Features like View Count, Like Count, and Comment Count demonstrate moderate importance.\n\nDistribution of Popularity:\n\n The box plot shows the distribution of popularity scores mainly concentrated around the median near 70, indicating high overall popularity. The narrow interquartile range suggests little variability, with several outliers highlighting items of significantly lower popularity.\n\nDistribution of like_ratio:\n\n This plot shows the distribution of like ratio ,and this is a clear right-skewed distribution, meaning that most videos have like ratios clustered in the lower region, with a long tail on the right side, indicating that a few videos have unusually high like ratios. This is a reflection of the fact that most music videos have a close and low like ratio, which may represent a “normal” level of interaction. Only a few videos have a particularly high like rate, and these are likely to be “viral” or particularly successful videos.\n\nCorrelation Heatmap:\n\n Positive correlation: Like rate and comment rate are highly correlated (0.72), indicating that users’ interactive behaviors (likes and comments) are closely related. Singer popularity is highly correlated with platform popularity (0.72), indicating consistency in popularity measures between the two platforms The number of subscriptions is moderately correlated with the number of views (0.45), the more subscribers, the higher the number of views Singer popularity is also moderately correlated with channel subscriptions (0.41)\nNegative correlation: days since release is strongly negatively correlated with likes (-0.56), indicating that the longer the video has been released, the lower the likes are Singer popularity is weakly negatively correlated with comment sentiment score (-0.30), possibly indicating that more popular singers receive more negative comments\nFinally, it can be seen that singer popularity shows significant correlations with several features that can be used as important predictors of singer popularity.\n\nDistribution of Key YouTube Music Video Metrics\n\n For plot Distribution of Log10(View Count), it shows normal distribution characteristics. We can conclude that most of the videos have view counts between 107-109. For plot Like Ratio Distribution, it shows right skewed distribution. The Like Ratio of most videos is concentrated between 0-2%. For plot Comment Ratio Distribution, it shows a strong right-skewed distribution. Most videos have a comment ratio of less than 0.2%. For plot Singer Popularity Distribution, it ranges from 40-100 scores. There is a clear peak at around 80."
  },
  {
    "objectID": "report/report.html#business-implications",
    "href": "report/report.html#business-implications",
    "title": "Final Report",
    "section": "",
    "text": "This project aims to help music industry practitioners better understand and grasp the market. Music producers, companies and singer teams can analyze the correlation between YouTube and Spotify platforms to allocate marketing budgets more rationally and produce more targeted content based on the characteristics of different platforms.\nAt the same time, this data can also help agencies assess the development potential of singers on Youtube, Spotify, meanwhile customize more appropriate content strategies for artists. Plus, music platform operators can draw on these analytics to improve their recommendation algorithms and optimize their content layout to enhance user experience. In addition, the data can also help industry players have a better acknowledge of the trends of the music market and understand the differences in user preferences across different platforms, so that they can formulate more accurate marketing strategies."
  },
  {
    "objectID": "report/report.html#recommendations",
    "href": "report/report.html#recommendations",
    "title": "Final Report",
    "section": "",
    "text": "Targeted Marketing Campaigns: Focus promotional efforts on artists and genres that exhibit strong cross-platform synergy.\nEnhanced Engagement Strategies: Develop content strategies that encourage viewer interaction on YouTube to leverage these engagements for increased Spotify streams.\nData-Driven Content Creation: Utilize insights from data to tailor content that resonates well across both platforms, optimizing both video and music production to cater to proven audience preferences."
  },
  {
    "objectID": "report/report.html#conclusion",
    "href": "report/report.html#conclusion",
    "title": "Final Report",
    "section": "",
    "text": "This comprehensive analysis employs advanced data analytics techniques across both YouTube and Spotify to uncover how video views and song popularity influence each other. By integrating insights from these platforms, we have identified significant cross-platform effects where YouTube metrics considerably impact Spotify’s track popularity. Leveraging this synergy provides actionable strategies for content creators and marketers to enhance their reach and engagement across media, ultimately boosting profitability and expanding market presence while increasing overall audience engagement."
  },
  {
    "objectID": "report/report.html#appendix",
    "href": "report/report.html#appendix",
    "title": "Final Report",
    "section": "",
    "text": "Dokuz, Y. (2024). Discovering popular and persistent tags from YouTube trending video big dataset. Multimedia Tools and Applications, 83, 10779–10797.\nKhin Nyunt, N. T., & Khin, T. (2024). YouTube Career Analysis with the Combination of Trending Analysis and Sentiments Analysis. ESS Open Archive.\nMeghana, K. (2024). Artificial Intelligence and Sentiment Analysis in YouTube Comments: A Comprehensive Overview. 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT), 1565-1572.\nGiri, R., Sirsath, M., & Kanakia, H. T. (2024). YouTube Comments Sentiment Analysis. IEEE 9th International Conference for Convergence in Technology (I2CT), Pune, India, 1-4.\nDasovich-Wilson, J. N., Thompson, M., & Saarikallio, S. (2024). The characteristics of music video experiences and their relationship to future listening outcomes. Psychology of Music, 0(0).\nEfe, I. E., Tesch, C., & Subedi, P. (2024). YouTube as a source of patient information on awake craniotomy: Analysis of content quality and user engagement. World Neurosurgery: X, 21, 100249."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html",
    "href": "technical-details/data-collection/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling.\n\n\n\nBegin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work.\n\n\n\n\nDuring the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder\n\n\n\n\n\nYour data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "href": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#what-to-address",
    "href": "technical-details/data-collection/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#start-collecting-data",
    "href": "technical-details/data-collection/instructions.html#start-collecting-data",
    "title": "Instructions",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "href": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "title": "Instructions",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#requirements",
    "href": "technical-details/data-collection/instructions.html#requirements",
    "title": "Instructions",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/overview.html",
    "href": "technical-details/data-collection/overview.html",
    "title": "Overview",
    "section": "",
    "text": "In this section, provide a high-level overview for technical staff, summarizing the key tasks and processes carried out here. Include the following elements:\n\n\nIdentify the key factors affecting the popularity official music video( view count) on Youtube by analyzing various features of official mv on Youtube and characteristics of corresponding songs on Spotify.\n\n\n\nView count of music video on YouTube, the world’s largest video- sharing platform is crucial for measuring the popularity and success of mv and song.\nUnderstanding the key factors that drive and impact the mv popularity can help music producers, singers improve mv content and promotion strategies.\nNowdays, it seems that few people have quantified the key factors needed to achieve a successful mv based on mv data, and most people trust their own industry experience, intuition, and artistic aesthetics, etc.\nCombing the data of Youtube and Spotify provides us with a more comprehensive view, since the user behavior on these two platforms are likely to influence each other.\n\n\n\nDetermine the primary factors that impact the mv view count, including: - YouTube features: days since published, like count, comment count, mean comment sentiment score, etc. - Spotify features: singer_popularity, duration , song_popularity, etc.\nExplore the correlation between Youtube and Spotify features and mv view count to identify the key traits.\nBased on the analysis, summarize and propose mv optimization suggestions to increase mv view count for singers, music producers and record labels.\nThis overview should give technical staff a clear understanding of the context and importance of the work, while guiding them on what to focus on in the details that follow."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#goals",
    "href": "technical-details/data-collection/overview.html#goals",
    "title": "Overview",
    "section": "",
    "text": "Identify the key factors affecting the popularity official music video( view count) on Youtube by analyzing various features of official mv on Youtube and characteristics of corresponding songs on Spotify."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#motivation",
    "href": "technical-details/data-collection/overview.html#motivation",
    "title": "Overview",
    "section": "",
    "text": "View count of music video on YouTube, the world’s largest video- sharing platform is crucial for measuring the popularity and success of mv and song.\nUnderstanding the key factors that drive and impact the mv popularity can help music producers, singers improve mv content and promotion strategies.\nNowdays, it seems that few people have quantified the key factors needed to achieve a successful mv based on mv data, and most people trust their own industry experience, intuition, and artistic aesthetics, etc.\nCombing the data of Youtube and Spotify provides us with a more comprehensive view, since the user behavior on these two platforms are likely to influence each other."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#objectives",
    "href": "technical-details/data-collection/overview.html#objectives",
    "title": "Overview",
    "section": "",
    "text": "Determine the primary factors that impact the mv view count, including: - YouTube features: days since published, like count, comment count, mean comment sentiment score, etc. - Spotify features: singer_popularity, duration , song_popularity, etc.\nExplore the correlation between Youtube and Spotify features and mv view count to identify the key traits.\nBased on the analysis, summarize and propose mv optimization suggestions to increase mv view count for singers, music producers and record labels.\nThis overview should give technical staff a clear understanding of the context and importance of the work, while guiding them on what to focus on in the details that follow."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html",
    "href": "technical-details/supervised-learning/instructions.html",
    "title": "Regression",
    "section": "",
    "text": "This analysis aims to explore the factors influencing the popularity of content on a digital platform. We will use various regression models to predict popularity based on multiple features extracted from our dataset.\n\n\n\nWe will use Support Vector Regression (SVR) and linear models like Ridge and Lasso regression.\nimport pandas as pd\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\ndf = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\n\n# Specify the feature columns\nfeatures = [\n    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',\n    'Subscriber Count', 'Definition', 'Mean Sentiment Score',\n    'Duration_seconds', 'genre_label', 'singer_followers', 'singer_popularity'\n]\n\n# Ensure the target column 'popularity' exists in DataFrame\nif 'Popularity' not in df.columns:\n    raise ValueError(\"The 'popularity' column is missing from the DataFrame.\")\n\n# Split into input (X) and target (y)\nX = df[features]  # Inputs\ny = df['Popularity']  # Target\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Assuming the SVR model is already trained as svr_model\nsvr_model = SVR(kernel='rbf', C=100, gamma='auto')\nsvr_model.fit(X_train, y_train)\n\n# Predict on the testing data\ny_pred = svr_model.predict(X_test)\n# Calculate mean squared error and R^2 score\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Mean Squared Error: {mse}, R^2 Score: {r2}\")\n\n# Perform permutation importance\nperm_importance = permutation_importance(svr_model, X_test, y_test, n_repeats=30, random_state=42)\n\n# Get importance scores\nimportance_scores = perm_importance.importances_mean\n\n# Print feature importance\nprint(\"Feature importances:\")\nfor i, feature in enumerate(features):\n    print(f\"{feature}: {importance_scores[i]}\")\n\n\n\n\n\n\n\n\n\n\n\nModel\nMSE\nR² Score\n\n\n\n\nSVR\n99.74699515216331\n0.568546463925952\n\n\nLinear Regression\n123.52107678768438\n0.46571217229730644\n\n\nRidge Regression\n125.61840469866303\n0.4566402243943334\n\n\nLasso Regression\n124.26131518891665\n0.46251028661381044\n\n\n\nFeature importances from SVR: - Days Since Published: 0.14590051030422282 - View Count: 0.0689113148502238 - Like Count: 0.06300639647560934 - Comment Count: 0.004829835437846231 - Subscriber Count: 0.15936065653760564 - Definition: 0.05992188839233011 - Mean Sentiment Score: 0.037774091955604584 - Duration_seconds: 0.00110286673615743 - genre_label: 0.1581978297916415 - singer_followers: 0.0360773423716881 - singer_popularity: 0.6712934520249385\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.barh(features, importance_scores, color='skyblue')\nplt.xlabel('Importance Score')\nplt.title('Feature Importance Scores')\nplt.gca().invert_yaxis()  # Invert y-axis to have the highest score on top\nplt.show()\n\n\nThe Support Vector Regression (SVR) model outperformed the linear models in terms of Mean Squared Error (MSE) and R² score. The lower MSE and higher R² of the SVR indicate better performance in fitting the data compared to the Linear, Ridge, and Lasso regressions. The R² scores suggest that the SVR model was able to explain approximately 56.85% of the variance in the dataset, which is more than the approximately 46.57% by the Linear Regression, 45.66% by Ridge, and 46.25% by Lasso Regression.\n\n\n\nFrom the SVR model’s permutation importance, ‘singer_popularity’ emerged as the most influential feature, significantly impacting the prediction of a song’s popularity. This suggests that more popular singers tend to have more popular songs, highlighting the influence of an artist’s existing reputation on new releases."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#introduction-and-motivation",
    "href": "technical-details/supervised-learning/instructions.html#introduction-and-motivation",
    "title": "Regression",
    "section": "",
    "text": "This analysis aims to explore the factors influencing the popularity of content on a digital platform. We will use various regression models to predict popularity based on multiple features extracted from our dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#overview-of-methods",
    "href": "technical-details/supervised-learning/instructions.html#overview-of-methods",
    "title": "Regression",
    "section": "",
    "text": "We will use Support Vector Regression (SVR) and linear models like Ridge and Lasso regression.\nimport pandas as pd\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\ndf = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\n\n# Specify the feature columns\nfeatures = [\n    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',\n    'Subscriber Count', 'Definition', 'Mean Sentiment Score',\n    'Duration_seconds', 'genre_label', 'singer_followers', 'singer_popularity'\n]\n\n# Ensure the target column 'popularity' exists in DataFrame\nif 'Popularity' not in df.columns:\n    raise ValueError(\"The 'popularity' column is missing from the DataFrame.\")\n\n# Split into input (X) and target (y)\nX = df[features]  # Inputs\ny = df['Popularity']  # Target\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Assuming the SVR model is already trained as svr_model\nsvr_model = SVR(kernel='rbf', C=100, gamma='auto')\nsvr_model.fit(X_train, y_train)\n\n# Predict on the testing data\ny_pred = svr_model.predict(X_test)\n# Calculate mean squared error and R^2 score\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Mean Squared Error: {mse}, R^2 Score: {r2}\")\n\n# Perform permutation importance\nperm_importance = permutation_importance(svr_model, X_test, y_test, n_repeats=30, random_state=42)\n\n# Get importance scores\nimportance_scores = perm_importance.importances_mean\n\n# Print feature importance\nprint(\"Feature importances:\")\nfor i, feature in enumerate(features):\n    print(f\"{feature}: {importance_scores[i]}\")"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#conclusion",
    "href": "technical-details/supervised-learning/instructions.html#conclusion",
    "title": "Regression",
    "section": "",
    "text": "Model\nMSE\nR² Score\n\n\n\n\nSVR\n99.74699515216331\n0.568546463925952\n\n\nLinear Regression\n123.52107678768438\n0.46571217229730644\n\n\nRidge Regression\n125.61840469866303\n0.4566402243943334\n\n\nLasso Regression\n124.26131518891665\n0.46251028661381044\n\n\n\nFeature importances from SVR: - Days Since Published: 0.14590051030422282 - View Count: 0.0689113148502238 - Like Count: 0.06300639647560934 - Comment Count: 0.004829835437846231 - Subscriber Count: 0.15936065653760564 - Definition: 0.05992188839233011 - Mean Sentiment Score: 0.037774091955604584 - Duration_seconds: 0.00110286673615743 - genre_label: 0.1581978297916415 - singer_followers: 0.0360773423716881 - singer_popularity: 0.6712934520249385\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.barh(features, importance_scores, color='skyblue')\nplt.xlabel('Importance Score')\nplt.title('Feature Importance Scores')\nplt.gca().invert_yaxis()  # Invert y-axis to have the highest score on top\nplt.show()\n\n\nThe Support Vector Regression (SVR) model outperformed the linear models in terms of Mean Squared Error (MSE) and R² score. The lower MSE and higher R² of the SVR indicate better performance in fitting the data compared to the Linear, Ridge, and Lasso regressions. The R² scores suggest that the SVR model was able to explain approximately 56.85% of the variance in the dataset, which is more than the approximately 46.57% by the Linear Regression, 45.66% by Ridge, and 46.25% by Lasso Regression.\n\n\n\nFrom the SVR model’s permutation importance, ‘singer_popularity’ emerged as the most influential feature, significantly impacting the prediction of a song’s popularity. This suggests that more popular singers tend to have more popular songs, highlighting the influence of an artist’s existing reputation on new releases."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#introduction-and-motivation-1",
    "href": "technical-details/supervised-learning/instructions.html#introduction-and-motivation-1",
    "title": "Regression",
    "section": "Introduction and Motivation",
    "text": "Introduction and Motivation\nThe goal of this section is to predict whether a song is considered “popular” using binary classification methods. We aim to understand the features that significantly influence song popularity on digital platforms."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#overview-of-methods-1",
    "href": "technical-details/supervised-learning/instructions.html#overview-of-methods-1",
    "title": "Regression",
    "section": "Overview of Methods",
    "text": "Overview of Methods\nIn this section, we focus on Logistic Regression for binary classification. Logistic Regression is chosen for its ability to provide probabilities for outcomes and its interpretability.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import train_test_split\n\n# Assume df is your DataFrame and the preprocessing has been done to define 'is_popular'\nfeatures = [\n    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',\n    'Subscriber Count', 'Definition', 'Mean Sentiment Score',\n    'Duration_seconds', 'genre_label', 'singer_followers', 'singer_popularity'\n]\nX = df[features]\ny = df['is_popular']\n\n# Splitting the dataset and under-sampling\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n\n# Logistic Regression model\nlr_model = LogisticRegression(random_state=42, max_iter=1000)\nlr_model.fit(X_resampled, y_resampled)\n\n# Making predictions and evaluating the model\ny_pred = lr_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclass_report = classification_report(y_test, y_pred)\n\nprint(\"Logistic Regression Model Evaluation\")\nprint(\"Accuracy:\", accuracy)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\nprint(\"Classification Report:\\n\", class_report)"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#results",
    "href": "technical-details/supervised-learning/instructions.html#results",
    "title": "Regression",
    "section": "Results",
    "text": "Results\nThe Logistic Regression model showed an accuracy of 77.33%, with a detailed classification report indicating precision, recall, and F1-score for both classes.\n\nModel Performance Summary\nComparison of model performance metrics for different classification models used in predicting song popularity:\n\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nPrecision\nRecall\nF1-Score\n\n\n\n\nLogistic Regression\n77.33%\n0.96 (False), 0.35 (True)\n0.77 (False), 0.80 (True)\n0.85 (False), 0.48 (True)\n\n\nSVM (Best Kernel)\n75.00%\n0.91 (False), 0.26 (True)\n0.78 (False), 0.50 (True)\n0.84 (False), 0.34 (True)\n\n\nRandom Forest\n71.00%\n0.92 (False), 0.25 (True)\n0.72 (False), 0.60 (True)\n0.81 (False), 0.35 (True)"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#conclusion-1",
    "href": "technical-details/supervised-learning/instructions.html#conclusion-1",
    "title": "Regression",
    "section": "Conclusion",
    "text": "Conclusion\nThe Logistic Regression model, adjusted for class imbalance via under-sampling, provided satisfactory classification results, proving effective for identifying popular songs."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#result-interpretation-1",
    "href": "technical-details/supervised-learning/instructions.html#result-interpretation-1",
    "title": "Regression",
    "section": "Result Interpretation",
    "text": "Result Interpretation\nThe model was particularly strong in identifying non-popular songs (class ‘False’) with high precision and recall. And it has a higher recall score on popular songs compared to other models."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-performance-comparison",
    "href": "technical-details/supervised-learning/instructions.html#model-performance-comparison",
    "title": "Regression",
    "section": "Model Performance Comparison",
    "text": "Model Performance Comparison\nThis model was compared to other binary classification models such as SVM and Random Forest. Logistic Regression was chosen for its balance between performance and interpretability in this specific context. And it has a higher recall score on popular songs compared to other models."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#introduction-and-motivation-2",
    "href": "technical-details/supervised-learning/instructions.html#introduction-and-motivation-2",
    "title": "Regression",
    "section": "Introduction and Motivation",
    "text": "Introduction and Motivation\nThe goal of this multi- class classification is to predict the interaction rate (like ratio= like count/ view count) of youtube mv. We want to predict the interaction rate of the content by features such as number of days since posting and number of comments.\nThis analysis firstly hopes to improve data analysis and model selection capabilities, and secondly tries to help content creators optimize their creation strategies."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#overview-of-methods-2",
    "href": "technical-details/supervised-learning/instructions.html#overview-of-methods-2",
    "title": "Regression",
    "section": "Overview of Methods",
    "text": "Overview of Methods\nMultiple classification algorithms were used to predict the popularity (like ratio) of short video content. The like ratio was first categorized into three categories: low (≤0.7), medium (0.7-1.6), and high (&gt;1.6), and then predicted using three models: logistic regression, decision tree, and random forest. To address data imbalance, SMOTE oversampling technique was used. The input features of the model included nine variables such as days of posting/comments, number of subscribers, sentiment score, video duration, number of creator followers, and popularity. The decision tree model was hyper-parametrically optimized by grid search (GridSearchCV) and the model performance was evaluated using cross-validation. This combination of methods enables comprehensive assessment and prediction of video content popularity, providing data support for content creation and platform operation."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#code",
    "href": "technical-details/supervised-learning/instructions.html#code",
    "title": "Regression",
    "section": "Code",
    "text": "Code\nlogistic regression\nX = df[features]\ny = df['Like Ratio Category']\n\n# Data set splitting\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Feature standardization\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n\n\n# Initialize the Logistic Regression model (One-vs-Rest for multi-class)\nmodel = LogisticRegression(multi_class='ovr', max_iter=1000, random_state=42)\n\n# Train the model\nmodel.fit(X_train_scaled, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test_scaled)\n\nX_scaled = scaler.fit_transform(X)  # Fit and transform the entire dataset\n\n# Cross-validation to get a better estimate of the model performance\ncross_val_scores = cross_val_score(model, X_scaled, y, cv=5)\nprint(\"\\nCross-validation scores:\", cross_val_scores)\nprint(\"\\nMean cross-validation score:\", cross_val_scores.mean())\n\n# Evaluate the model\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n#param_grid\nparam_grid_refined = {\n    'max_depth': [3,4,5],  \n    'min_samples_split': [8,10,15],\n    'min_samples_leaf': [3,4,5,6],   \n    'criterion': ['entropy'],        \n    'class_weight': ['balanced']  \n}\n\ngrid_search_refined = GridSearchCV(\n    DecisionTreeClassifier(random_state=42),\n    param_grid_refined,\n    cv=5,\n    scoring='f1_macro', \n    n_jobs=-1\n)\n\n# train mode\ngrid_search_refined.fit(X_train_scaled, y_train)\n\nprint(\"Refined Best parameters:\", grid_search_refined.best_params_)\nprint(\"Refined Best cross-validation score:\", grid_search_refined.best_score_)\n\nbest_model_refined = DecisionTreeClassifier(\n    **grid_search_refined.best_params_\n)\n\nbest_model_refined.fit(X_train_scaled, y_train)\n\ny_pred_refined = best_model_refined.predict(X_test_scaled)\nprint(\"\\nRefined Test set accuracy:\", accuracy_score(y_test, y_pred_refined))\nprint(\"\\nRefined Classification Report:\\n\", classification_report(y_test, y_pred_refined))\nrandom forest\nf_model = RandomForestClassifier(n_estimators=100)\n\n# Train the model\nrf_model.fit(X_train_scaled, y_train)\n\n# Predict on the test set\ny_pred_rf = rf_model.predict(X_test_scaled)\n\n# Evaluate the model\nprint(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\nprint(\"\\nRandom Forest Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n\n# Cross-validation to get a better estimate of the model performance\ncross_val_scores_rf = cross_val_score(rf_model, X_train_scaled, y_train, cv=5)\nprint(\"\\nRandom Forest Cross-validation scores:\", cross_val_scores_rf)\nprint(\"\\nMean cross-validation score (Random Forest):\", cross_val_scores_rf.mean())"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#result",
    "href": "technical-details/supervised-learning/instructions.html#result",
    "title": "Regression",
    "section": "Result",
    "text": "Result\n\nModel performance Result\nComparison of Model Performance Metrics for Different Classification Models Used in Predicting like_ratio (Multi-Class Classification)\n\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nPrecision\nRecall\nF1-Score\n\n\n\n\nLogistic Regression\n77.33%\n0.64 (High), 0.86 (Low), 0.73 (Medium)\n0.82 (High), 0.86 (Low), 0.66 (Medium)\n0.72 (High), 0.86 (Low), 0.69 (Medium)\n\n\nDecision Tree\n74.67%\n0.56 (High), 0.85 (Low), 0.74 (Medium)\n0.91 (High), 0.83 (Low), 0.59 (Medium)\n0.69 (High), 0.84 (Low), 0.65 (Medium)\n\n\nRandom Forest\n78.67%\n0.69 (High), 0.88 (Low), 0.73 (Medium)\n0.82 (High), 0.80 (Low), 0.76 (Medium)\n0.75 (High), 0.84 (Low), 0.75 (Medium)\n\n\n\nCross-Validation Scores\n\n\n\n\n\n\n\n\nModel\nCross-Validation Scores\nMean Cross-Validation Score\n\n\n\n\nLogistic Regression\n[0.6533, 0.6133, 0.8133, 0.72, 0.4667]\n0.6533\n\n\nDecision Tree\nRefined Best CV: [0.6208 (balanced parameters)]\n0.6208\n\n\nRandom Forest\n[0.75, 0.6833, 0.6833, 0.7, 0.7833]\n0.72\n\n\n\n\n\nResult Interpretation\nWe can see from the table that the Random Forest model is the most effective model for predicting the like_ratio, achieving the highest accuracy (78.67%) and a balanced performance in terms of Precision, Recall, and F1-Score across all classes (High, Low, and Medium). Its average cross-validation score is 0.72, demonstrating good generalization.\nThe Logistic regression model is second effective one with an overall accuracy of 77.33%. While the low rank Precision (0.86) is strong, the medium rank Recall (0.66) performed relatively weakly. Nonetheless, the model is metrically consistent, showing its suitability for applications that require interpretability and stable performance.\nFor the Decision Tree model, it has the lowest overall accuracy (74.67%). While it performs well in recognizing high levels with a recall of 0.91, it performs poorly in recognizing intermediate levels where it has the lowest accuracy (0.74) and recall (0.59). But it can distinguish between high & low cases well."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#insights-1",
    "href": "technical-details/supervised-learning/instructions.html#insights-1",
    "title": "Regression",
    "section": "Insights",
    "text": "Insights\nFrom the results, we can find that the Random Forest model shows the best balance of accuracy, precision, recall, and F1 value in predicting the like_ratio of songs, especially in the medium class (Medium). This indicates that Random Forest has a strong ability in dealing with multi-categorization problems. The Logistic Regression model, on the other hand, has high precision and recall in identifying the low like_ratio category, suggesting that it is more effective in distinguishing between low popularity songs. while the Decision Tree model’s perform is not so good as we expected among all these three models, with low recall especially in the Medium category, suggesting that it may be unsuitable for dealing with such a complex multi-class classification task\nThis multi-class classification enables me to better understand the advantages and limitations of different models in multi-class classification tasks, and teach me lesson that we should choose the right model for future projects based on the specific needs of the task."
  },
  {
    "objectID": "technical-details/llm-usage-log.html",
    "href": "technical-details/llm-usage-log.html",
    "title": "LLM usage log",
    "section": "",
    "text": "This page can serve as a “catch-all” for LLM use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\nLLM tools were used in the following way for the tasks below"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#brainstorming",
    "href": "technical-details/llm-usage-log.html#brainstorming",
    "title": "LLM usage log",
    "section": "Brainstorming",
    "text": "Brainstorming\n\nLLM tools were used to brainstorm initial ideas for research topics. Potential topics considered include:\n\nAmazon purchase and review behavior analysis.\nSpotify music streaming data and factors influencing play counts.\nYouTube video view counts and the associated variables that impact popularity.\n\nAfter generating these ideas, LLM was consulted to evaluate their feasibility and relevance for the project. The model helped us refine these topics by providing insights on potential challenges and opportunities.\nAdditionally, LLM assisted in drafting a detailed project plan, including timelines and milestones, to ensure an organized workflow and a feasible approach for execution.\nBrainstorm for exploratory data analysis and determine what to include"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#writing",
    "href": "technical-details/llm-usage-log.html#writing",
    "title": "LLM usage log",
    "section": "Writing:",
    "text": "Writing:\n\nReformating text from bulleted lists into proses\nProofreading\nText summarization for literature review\nConcepts for unsupervised learning\nSuggestions for comparing models"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#code",
    "href": "technical-details/llm-usage-log.html#code",
    "title": "LLM usage log",
    "section": "Code:",
    "text": "Code:\n\nCode commenting and explanatory documentation\nDebugging\nImproving visualization effects"
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html",
    "href": "technical-details/data-cleaning/instructions.html",
    "title": "Data Cleaning and Preprocessing",
    "section": "",
    "text": "This document outlines the steps taken to clean and preprocess the data from a dataset containing view/like/comment counts, relevant comments and populariy value from Spotify and YouTube. The primary goal is to prepare the data for sentiment analysis and further statistical modeling, focusing on extracting meaningful insights about song popularity and engagement."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#introduction-and-motivation",
    "href": "technical-details/data-cleaning/instructions.html#introduction-and-motivation",
    "title": "Data Cleaning and Preprocessing",
    "section": "",
    "text": "This document outlines the steps taken to clean and preprocess the data from a dataset containing view/like/comment counts, relevant comments and populariy value from Spotify and YouTube. The primary goal is to prepare the data for sentiment analysis and further statistical modeling, focusing on extracting meaningful insights about song popularity and engagement."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#overview-of-methods",
    "href": "technical-details/data-cleaning/instructions.html#overview-of-methods",
    "title": "Data Cleaning and Preprocessing",
    "section": "Overview of Methods",
    "text": "Overview of Methods\nThe methods used include text cleaning, language detection, sentiment analysis, data transformation, and normalization. These techniques ensure that the data is in an appropriate format for analysis, removing any inconsistencies and ensuring quality and accuracy in the results."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#code-implementation-and-description",
    "href": "technical-details/data-cleaning/instructions.html#code-implementation-and-description",
    "title": "Data Cleaning and Preprocessing",
    "section": "Code Implementation and Description",
    "text": "Code Implementation and Description\n\nData Reading\nThe raw data is loaded from a CSV file using Pandas, which provides a convenient framework for data manipulation in Python.\n\nimport pandas as pd\ndf = pd.read_csv('../../data/raw-data/spotify_youtube.csv', encoding='iso-8859-1')\ndf.head()\n\n\n\n\n\n\n\n\nsinger\ngenre\nVideo ID\nTitle\nDescription\nPublished At\nDays Since Published\nView Count\nLike Count\nComment Count\n...\nArtist Name\nArtist\nGenre\nsinger_followers\nsinger_popularity\nAlbum Name\nPopularity\nDuration (ms)\nTrack ID\nSpotify URL\n\n\n\n\n0\nFoo Fighters\nrock\n1VQ_3sBZEm0\nFoo Fighters - Learn To Fly (Official HD Video)\nFoo Fighters' official music video for 'Learn ...\n2009-10-03T04:46:13Z\n5549\n183888273\n808087\n33852\n...\nFoo Fighters\nFoo Fighters\nrock\n12214832\n78\nThere Is Nothing Left To Lose\n73\n235293\n5OQsiBsky2k2kDKy2bX2eT\nhttps://open.spotify.com/track/5OQsiBsky2k2kDK...\n\n\n1\nFoo Fighters\nrock\neBG7P-K-r1Y\nFoo Fighters - Everlong (Official HD Video)\n\"Everlong\" by Foo Fighters \\nListen to Foo Fig...\n2009-10-03T04:49:58Z\n5549\n324339281\n1820753\n53186\n...\nFoo Fighters\nFoo Fighters\nrock\n12214832\n78\nThe Colour And The Shape\n80\n250546\n5UWwZ5lm5PKu6eKsHAGxOk\nhttps://open.spotify.com/track/5UWwZ5lm5PKu6eK...\n\n\n2\nFoo Fighters\nrock\nSBjQ9tuuTJQ\nFoo Fighters - The Pretender\nWatch the official music video for \"The Preten...\n2009-10-03T04:46:14Z\n5549\n588029134\n2785440\n92233\n...\nFoo Fighters\nFoo Fighters\nrock\n12214832\n78\nEchoes, Silence, Patience & Grace\n75\n269373\n7x8dCjCr0x6x2lXKujYD34\nhttps://open.spotify.com/track/7x8dCjCr0x6x2lX...\n\n\n3\nFoo Fighters\nrock\nEqWRaAF6_WY\nFoo Fighters - My Hero (Official HD Video)\n\"My Hero\" by Foo Fighters \\nListen to Foo Figh...\n2011-03-18T19:35:42Z\n5017\n87504683\n564313\n26089\n...\nFoo Fighters\nFoo Fighters\nrock\n12214832\n78\nThe Colour And The Shape\n72\n260026\n4dVbhS6OiYvFikshyaQaCN\nhttps://open.spotify.com/track/4dVbhS6OiYvFiks...\n\n\n4\nFoo Fighters\nrock\nh_L4Rixya64\nFoo Fighters - Best Of You (Official Music Video)\nWatch the official music video for \"Best Of Yo...\n2009-10-03T20:49:33Z\n5548\n265538783\n1281111\n34995\n...\nFoo Fighters\nFoo Fighters\nrock\n12214832\n78\nIn Your Honor\n72\n255626\n5FZxsHWIvUsmSK1IAvm2pp\nhttps://open.spotify.com/track/5FZxsHWIvUsmSK1...\n\n\n\n\n5 rows × 26 columns\n\n\n\n\n\nComment Cleaning and Language Filtering\nComments are cleaned by removing HTML tags, punctuation, numbers, and ensuring they are in English. This is crucial for the accuracy of the sentiment analysis.\ndef clean_comments(comments):\n    # Cleaning code here...\n    return english_comments\n\n\nSentiment Analysis\nUsing NLTK’s VADER, we perform sentiment analysis on the cleaned English comments. This provides a mean sentiment score for each record, indicating the overall sentiment of the comments associated with each song.\ndef average_sentiment_score(comments):\n    # Sentiment analysis code here...\n    return score\n\n\nData Transformations\nSeveral transformations are applied to the dataset: - Converting video duration from ISO 8601 format to seconds. - Binary encoding of video definition (HD or SD). - Factorizing the ‘genre’ column to prepare for modeling.\ndf['Duration_seconds'] = df['Duration'].apply(duration_to_seconds)\ndf['Definition'] = df['Definition'].apply(convert_definition)\ndf['genre_label'] = pd.factorize(df['genre'])[0]\nFor future analysis, some categorical variables are converted into numerical style.\n\n\nData Normalization\nUsing StandardScaler, numerical columns are scaled to have zero mean and unit variance. This step is important for models that are sensitive to the magnitude of input features.\nscaler = StandardScaler()\ndf[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n\n\nBefore-and-After Visualizations\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the normalized data\ndf_normalized = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\n\ndef plot_data_comparisons(df_original, df_normalized, column_name):\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # Original Data Histogram\n    sns.histplot(df_original[column_name], ax=axes[0], kde=True, color='skyblue')\n    axes[0].set_title(f'Original {column_name}')\n    \n    # Normalized Data Histogram\n    sns.histplot(df_normalized[column_name], ax=axes[1], kde=True, color='olive')\n    axes[1].set_title(f'Normalized {column_name}')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Assuming the original data is still available and named df\ndf_original = pd.read_csv('../../data/raw-data/spotify_youtube.csv', encoding='iso-8859-1')\n\n# Example variables to visualize\nvariables_to_visualize = ['View Count', 'Like Count', 'Subscriber Count']\n\nfor variable in variables_to_visualize:\n    plot_data_comparisons(df_original, df_normalized, variable)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe standardization has transformed the data into a scale where it is centered around zero, greatly reducing the range of values and making the distribution more compact. The normalization process highlights the underlying data structure more clearly by smoothing out extreme variations and focusing on the distribution’s shape, facilitating more effective data analysis and model training.\n\n\nData Storage\nThe processed data is saved back to a CSV file, ensuring that all modifications are preserved for subsequent analysis.\ndf.to_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv', index=False)\n\ndf = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\ndf.head()\n\n\n\n\n\n\n\n\nsinger\ngenre\nVideo ID\nTitle\nDescription\nPublished At\nDays Since Published\nView Count\nLike Count\nComment Count\n...\nsinger_popularity\nAlbum Name\nPopularity\nDuration (ms)\nTrack ID\nSpotify URL\nMean Sentiment Score\nProcessed_Comments\nDuration_seconds\ngenre_label\n\n\n\n\n0\nFoo Fighters\nrock\n1VQ_3sBZEm0\nFoo Fighters - Learn To Fly (Official HD Video)\nFoo Fighters' official music video for 'Learn ...\n2009-10-03T04:46:13Z\n1.168381\n-0.264997\n-0.403455\n-0.358746\n...\n-0.184437\nThere Is Nothing Left To Lose\n73\n0.024286\n5OQsiBsky2k2kDKy2bX2eT\nhttps://open.spotify.com/track/5OQsiBsky2k2kDK...\n0.366239\n['can we bring this back this feeling this mus...\n-0.039614\n-1.319198\n\n\n1\nFoo Fighters\nrock\neBG7P-K-r1Y\nFoo Fighters - Everlong (Official HD Video)\n\"Everlong\" by Foo Fighters \\nListen to Foo Fig...\n2009-10-03T04:49:58Z\n1.168381\n-0.065329\n-0.161714\n-0.250200\n...\n-0.184437\nThe Colour And The Shape\n80\n0.248671\n5UWwZ5lm5PKu6eKsHAGxOk\nhttps://open.spotify.com/track/5UWwZ5lm5PKu6eK...\n-0.984291\n['stop asking whos still listening we never st...\n-0.001872\n-1.319198\n\n\n2\nFoo Fighters\nrock\nSBjQ9tuuTJQ\nFoo Fighters - The Pretender\nWatch the official music video for \"The Preten...\n2009-10-03T04:46:14Z\n1.168381\n0.309538\n0.068574\n-0.030982\n...\n-0.184437\nEchoes, Silence, Patience & Grace\n75\n0.525632\n7x8dCjCr0x6x2lXKujYD34\nhttps://open.spotify.com/track/7x8dCjCr0x6x2lX...\n1.334540\n['how can you not get chills listening to this...\n-0.054710\n-1.319198\n\n\n3\nFoo Fighters\nrock\nEqWRaAF6_WY\nFoo Fighters - My Hero (Official HD Video)\n\"My Hero\" by Foo Fighters \\nListen to Foo Figh...\n2011-03-18T19:35:42Z\n0.902404\n-0.402018\n-0.461648\n-0.402329\n...\n-0.184437\nThe Colour And The Shape\n72\n0.388130\n4dVbhS6OiYvFikshyaQaCN\nhttps://open.spotify.com/track/4dVbhS6OiYvFiks...\n1.577078\n['my son and i was supposed to spend the summe...\n-0.125161\n-1.319198\n\n\n4\nFoo Fighters\nrock\nh_L4Rixya64\nFoo Fighters - Best Of You (Official Music Video)\nWatch the official music video for \"Best Of Yo...\n2009-10-03T20:49:33Z\n1.167881\n-0.148921\n-0.290536\n-0.352329\n...\n-0.184437\nIn Your Honor\n72\n0.323402\n5FZxsHWIvUsmSK1IAvm2pp\nhttps://open.spotify.com/track/5FZxsHWIvUsmSK1...\n0.468128\n['what a talent dave is drummer singer guitari...\n-0.092452\n-1.319198\n\n\n\n\n5 rows × 30 columns\n\n\n\nReview Data Types\n\n\n\n\n\n\n\n\nVariable Name\nData Type\nDescription\n\n\n\n\nsinger\nCategorical\nArtist or band name\n\n\ngenre\nCategorical\nGenre of the music\n\n\nVideo ID\nCategorical\nUnique identifier for the video\n\n\nTitle\nCategorical\nTitle of the video\n\n\nDescription\nCategorical\nDescription of the video\n\n\nPublished At\nDate-time\nDate and time the video was published\n\n\nDays Since Published\nNumerical\nNumber of days since the video was published\n\n\nView Count\nNumerical\nTotal number of views on the video\n\n\nLike Count\nNumerical\nNumber of likes on the video\n\n\nComment Count\nNumerical\nNumber of comments on the video\n\n\nComments\nCategorical\nList of 10 comments on the video\n\n\nSubscriber Count\nNumerical\nNumber of subscribers to the channel\n\n\nCategory ID\nCategorical\nYouTube category ID for the video\n\n\nDefinition\nCategorical\nQuality definition of the video\n\n\nDuration\nCategorical\nDuration of the video in a human-readable format\n\n\nTrack Name\nCategorical\nName of the track\n\n\nArtist Name\nCategorical\nName of the artist\n\n\nArtist\nCategorical\nName of the artist\n\n\nsinger_followers\nNumerical\nNumber of followers the singer has on Spotify\n\n\nsinger_popularity\nNumerical\nPopularity rating of the singer\n\n\nAlbum Name\nCategorical\nName of the album\n\n\nPopularity\nNumerical\nPopularity rating of the track\n\n\nDuration (ms)\nNumerical\nDuration of the track in milliseconds\n\n\nTrack ID\nCategorical\nUnique identifier for the track\n\n\nSpotify URL\nCategorical\nURL to the track on Spotify\n\n\nMean Sentiment Score\nNumerical\nAverage sentiment score of comments\n\n\nProcessed_Comments\nCategorical\nProcessed list of comments\n\n\nDuration_seconds\nNumerical\nDuration of the video in seconds\n\n\ngenre_label\nCategorical\nCategorical label for the genre"
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#summary-and-interpretation-of-results",
    "href": "technical-details/data-cleaning/instructions.html#summary-and-interpretation-of-results",
    "title": "Data Cleaning and Preprocessing",
    "section": "Summary and Interpretation of Results",
    "text": "Summary and Interpretation of Results\nThe preprocessing steps significantly cleaned and transformed the raw data, making it suitable for accurate and insightful analysis. The sentiment scores provide a quantitative measure of public perception, which, combined with other song metrics, can be used to gauge popularity and engagement."
  }
]