[
  {
    "objectID": "technical-details/unsupervised-learning/main.html",
    "href": "technical-details/unsupervised-learning/main.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations.\n\n\n\n\n\n\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the dataset\ndf = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\n\n# Specify the feature columns\nfeatures = [\n    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',\n    'Subscriber Count', 'Definition', 'Mean Sentiment Score',\n    'Duration_seconds', 'genre_label', 'singer_followers', 'singer_popularity'\n]\n\n# Ensure the target column 'popularity' exists in DataFrame\nif 'Popularity' not in df.columns:\n    raise ValueError(\"The 'popularity' column is missing from the DataFrame.\")\n\n# Split into input (X) and target (y)\nX = df[features]  # Inputs\ny = df['Popularity']  # Target\n\n# Applying PCA\npca = PCA(n_components=3)  # Reduce to 4 dimensions for visualization\nX_pca = pca.fit_transform(X)\n# How much variance was retained?\nprint(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\nprint(\"Total Variance Explained:\", sum(pca.explained_variance_ratio_))\n\n\n\nperplexities = [5, 30, 50, 100]\nfig, ax = plt.subplots(2, 2, figsize=(15, 10))\nfor i, perplexity in enumerate(perplexities):\n    # Applying t-SNE\n    tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=3000, random_state=42)\n    X_tsne = tsne.fit_transform(X)\n\n    # Plotting\n    ax[i//2, i%2].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', marker='o', alpha=0.5)\n    ax[i//2, i%2].set_title(f't-SNE with Perplexity={perplexity}')\n    ax[i//2, i%2].set_xlabel('t-SNE Feature 1')\n    ax[i//2, i%2].set_ylabel('t-SNE Feature 2')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nPCA: The PCA visualization shows a somewhat linear distribution of the data points, focusing on the major directions of data variance. It effectively captures the global structure of the data, emphasizing the spread along the directions of highest variance.\nt-SNE: In contrast, the t-SNE visualization exhibits a more scattered and differentiated structure with distinct clusters. This indicates t-SNE’s superior capability in capturing local structures within the data, potentially revealing intrinsic patterns that PCA might overlook.\n\n\n\n\nPCA: Provides a simplified overview, projecting the data into lower dimensions while trying to preserve the variance. It’s effective for quick explorations of the data to identify gross underlying patterns but might miss finer details.\nt-SNE: Produces more visually distinct clusters, making it easier to identify groups and patterns within the data. This makes t-SNE a better tool for tasks requiring detailed pattern recognition and cluster analysis.\n\n\n\n\n\nPCA:\n\nPros: Less computationally intensive, suitable for larger datasets, provides a quick overview.\nCons: Might miss non-linear relationships between features.\nBest for: Preliminary data analysis, reducing dimensionality for linear data, or when computational resources are limited.\n\nt-SNE:\n\nPros: Captures complex non-linear relationships, excellent for identifying clusters and local patterns.\nCons: Computationally expensive, sensitive to parameter settings (like perplexity), not suitable for very large datasets.\nBest for: Detailed exploratory data analysis, when clusters or local patterns within the data are of interest, and computational resources are adequate.\n\n\nBoth PCA and t-SNE offer valuable insights, but their applicability depends on the specific needs of the analysis. PCA can serve as a good starting point for linear dimensionality reduction, while t-SNE is more suited for in-depth analysis requiring detailed cluster identification."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/main.html#suggested-page-structure",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#what-to-address",
    "href": "technical-details/unsupervised-learning/main.html#what-to-address",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/eda/main.html",
    "href": "technical-details/eda/main.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Introduction and Motivation\nThis eda section focuses primarily on examining the relationship between track popularity, mv’s view count and youtube related factors. By analyzing the trends and correlations between these variables, we want to find which YouTube factors have a significant impact on Spotify track popularity,view count? This analysis can help us better understand the cross-platform interaction mechanism of the music market and provide data support for music promotion strategies.\n\n\nOverview of Methods\nFor Univariate Analysis, we use summary statistics to provide an overview of central tendencies and spread. Distributions were visualized using histograms with KDE (Kernel Density Estimation) overlays for key metrics such as the logarithm of View Count, like_ratio, comment_ratio and singer_popularity. For categorical variables, the distribution of music genres is examined using a bar plot, highlighting the frequency of each genre.\nFor Bivariate and Multivariate Analysis, we employ correlation analysis, visualization, and statistical summarization to explore relationships in the dataset. It calculates a correlation matrix to identify linear relationships between numerical variables and uses a heatmap for visualization. Scatterplots analyze pairwise relationships like logarithmic popularity and like count, days since published and subscriber count, and engagement rate versus duration, with genre as a categorical hue. A boxplot examines the distribution of singer popularity across genres. Cross-tabulation categorizes singer popularity into three levels (low, medium, high) and calculates genre-wise percentages, while groupby summarization provides mean values of key metrics by genre.\nFor Data Distribution and Normalization, We use skewness and kurtosis to assess the distribution of numeric variables. For heavily skewed data, log transformation was applied to reduce skewness. Normalization techniques were implemented, including Min-Max scaling to scale values between 0 and 1 and Z-score normalization to standardize data with a mean of 0 and a standard deviation of 1. The impact of these transformations was visualized using histograms for original, Min-Max scaled, and Z-score normalized distributions, highlighting changes in data distribution.\nFor Statistical Insights, Our method includes ANOVA and T-tests. One-way ANOVA was applied to compare means of ‘View Count’ across different genres, ‘Popularity’ across various durations, and ‘View Count’ across like ratio bins, providing F-statistics and p-values to assess significant differences. Additionally, a T-test was conducted to evaluate’View Count’ based on the median of ‘Mean Sentiment Score.’\nFor Visual Summary, An engagement rate was calculated as the combined percentage of likes and comments relative to views. To explore relationships, scatter plots with regression lines were used to assess the correlation between popularity and likes, while a box plot examined engagement rates across genres. A violin plot visualized the distribution of sentiment scores by genre, and a scatter plot highlighted the relationship between video duration and views, categorized by genre.\n\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\nCode\nProvide the source code used for this section of the project here.\nIf you’re using a package for code organization, you can import it at this point. However, make sure that the actual workflow steps—including data processing, analysis, and other key tasks—are conducted and clearly demonstrated on this page. The goal is to show the technical flow of your project, highlighting how the code is executed to achieve your results.\nIf relevant, link to additional documentation or external references that explain any complex components. This section should give readers a clear view of how the project is implemented from a technical perspective.\nRemember, this page is a technical narrative, NOT just a notebook with a collection of code cells, include in-line Prose, to describe what is going on.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nfile_path = \"../../data/processed-data/Updated_Data_with_Sentiments.csv\"\ndf = pd.read_csv(file_path)\n\n\n# include interaction metrics (like count/view count %, comment count/ view count%) as factors\ndf['like_ratio'] = df['Like Count'] / df['View Count'] *100\ndf['comment_ratio'] = df['Comment Count'] / df['View Count'] *100\n\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\n\n\nnumeric_cols = ['singer_popularity', 'Days Since Published', 'Subscriber Count', \n                  'View Count', 'Like Count', 'Comment Count', 'like_ratio', \n                  'comment_ratio', 'Duration_seconds', 'Mean Sentiment Score']\n                \nprint(\"\\nNumerical Variables Summary Statistics:\")\nprint(df[numeric_cols].describe())\n\n\nNumerical Variables Summary Statistics:\n       singer_popularity  Days Since Published  Subscriber Count  \\\ncount         375.000000            375.000000      3.750000e+02   \nmean           80.120000           3212.032000      8.392395e+06   \nstd            11.509773           2002.848485      1.220033e+07   \nmin            39.000000              0.000000      9.000000e+00   \n25%            77.000000           1473.500000      7.550000e+05   \n50%            81.000000           3391.000000      3.760000e+06   \n75%            87.000000           5374.500000      1.060000e+07   \nmax           100.000000           6794.000000      6.040000e+07   \n\n         View Count    Like Count  Comment Count  like_ratio  comment_ratio  \\\ncount  3.750000e+02  3.750000e+02   3.750000e+02  375.000000     375.000000   \nmean   3.702933e+08  2.498181e+06   9.775152e+04    1.187740       0.053684   \nstd    7.043627e+08  4.194654e+06   1.783573e+05    1.201180       0.083883   \nmin    2.760000e+02  0.000000e+00   0.000000e+00    0.000000       0.000000   \n25%    1.374476e+07  1.327810e+05   4.136500e+03    0.557778       0.016598   \n50%    9.101476e+07  7.850650e+05   2.807200e+04    0.787268       0.026350   \n75%    3.711109e+08  2.878034e+06   1.032220e+05    1.402276       0.059631   \nmax    6.379786e+09  3.378610e+07   1.184861e+06   12.401685       0.963266   \n\n       Duration_seconds  Mean Sentiment Score  \ncount        375.000000            375.000000  \nmean         292.744000              0.325594  \nstd          397.971143              0.225798  \nmin           21.000000             -0.336767  \n25%          200.500000              0.169655  \n50%          239.000000              0.319200  \n75%          284.500000              0.484267  \nmax         4835.000000              0.817367  \n\n\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 6))\n\n# subplot1: Distribution of Log10(View Count)\nsns.histplot(np.log10(df['View Count']), kde=True, ax=axes[0, 0])\naxes[0, 0].set_title('Distribution of Log10(View Count)')\naxes[0, 0].set_xlabel('Log10(View Count)')\n\n# subplot2: Like Ratio distribution\nsns.histplot(df['like_ratio'], kde=True, ax=axes[0, 1])\naxes[0, 1].set_title('Distribution of Like Ratio (%)')\naxes[0, 1].set_xlabel('Like Ratio (%)')\naxes[0, 1].set_xlim(-1, 7)\n\n# subplot3: Comment Ratio distribution\nsns.histplot(df['comment_ratio'], kde=True, ax=axes[1, 0])\naxes[1, 0].set_title('Distribution of Comment Ratio (%)')\naxes[1, 0].set_xlabel('Comment Ratio (%)')\n\n# subplot4: Distribution of Singer Popularity\nsns.histplot(df['singer_popularity'], kde=True, ax=axes[1, 1])\naxes[1, 1].set_title('Distribution of Singer Popularity')\naxes[1, 1].set_xlabel('Singer Popularity')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFor plot Distribution of Log10(View Count), it shows normal distribution characteristics. We can conclude that most of the videos have view counts between 107-109.\nFor plot Like Ratio Distribution, it shows right skewed distribution. The Like Ratio of most videos is concentrated between 0-2%.\nFor plot Comment Ratio Distribution, it shows a strong right-skewed distribution. Most videos have a comment ratio of less than 0.2%\nFor plot Singer Popularity Distribution, it ranges from 40-100 scores. There is a clear peak at around 80.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\n\n\n# Genre distribution\nplt.figure(figsize=(8, 5))\ngenre_counts = df['genre'].value_counts()\nsns.barplot(x=genre_counts.index, y=genre_counts.values)\nplt.title('Distribution of Music Genres')\nplt.xticks(rotation=45)\n\n([0, 1, 2, 3, 4],\n [Text(0, 0, 'rock'),\n  Text(1, 0, 'pop'),\n  Text(2, 0, 'jazz'),\n  Text(3, 0, 'hip-pop'),\n  Text(4, 0, 'electronic')])\n\n\n\n\n\n\n\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\n\n\nnumeric_cols = ['singer_popularity', 'Days Since Published', 'Subscriber Count', \n                  'View Count', 'like_ratio','comment_ratio', 'Duration_seconds', 'Mean Sentiment Score','Popularity']\ncorrelation_matrix = df[numeric_cols].corr()\n\n# Create correlation heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, \n            annot=True, \n            cmap='coolwarm', \n            center=0,  \n            fmt='.2f') \nplt.title('Correlation Matrix of Numerical Variables')\nplt.tight_layout()\n\n\n\n\n\n\n\n\nFrom this plot, we observe three key positive correlations: A strong relationship between the rate of likes and comments (0.72) suggests that sticky subscriber behaviors such as likes and comments are closely related. And correlation (0.72) between Singer_popularity and Popularity: reflects the consistency of the scoring system, suggesting that the two metrics may be measuring similar popularity dimensions A moderate correlation between the number of subscribers and the number of views (0.45) indicates that channels with a higher number of subscribers tend to get more views. Another moderate correlation (0.41) shows that more popular artists tend to attract more channel subscribers.\nFor negative correlations: A strong negative correlation (-0.56) between days of posting and the likes ratio demonstrates that videos posted for a longer period of time tend to have a lower likes ratio; A weak negative correlation (-0.30) between Singer_popularity and the average sentiment score interestingly suggests that more popular singers may receive lower sentiment scores in comments, which might be due to an increase in negative feedback as their popularity grows.\nIn addition, Singer_popularity showed significant correlations with several features, making these features valuable predictors for assessing singer popularity.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\n\n# Feature Pairing Analysis\nfig, axes = plt.subplots(2, 2, figsize=(10, 10))\n\n# 1. Log10(Popularity) vs Log10(Like Count) by Genre\nsns.scatterplot(data=df, \n                x=np.log10(df['Popularity']), \n                y=np.log10(df['Like Count']),\n                hue='genre',\n                ax=axes[0, 0])\naxes[0, 0].set_title('Log10(Popularity) vs Log10(Like Count) by Genre')\naxes[0, 0].set_xlabel('Log10(Popularity)')\naxes[0, 0].set_ylabel('Log10(Like Count)')\n\n# 2. Days Since Published vs Subscriber Count\nsns.scatterplot(data=df,\n                x='Days Since Published',\n                y=np.log10(df['Subscriber Count'] + 1),  # Log transform for scaling\n                hue='genre',\n                ax=axes[0, 1])\naxes[0, 1].set_title('Days Since Published vs Log10(Subscriber Count) by Genre')\naxes[0, 1].set_xlabel('Days Since Published')\naxes[0, 1].set_ylabel('Log10(Subscriber Count)')\n\n# 3. Duration vs Engagement Rate by Genre\ndf['Engagement_Rate'] = (df['Like Count'] + df['Comment Count']) / df['View Count'] * 100\nsns.scatterplot(data=df,\n                x='Duration_seconds',\n                y='Engagement_Rate',\n                hue='genre',\n                ax=axes[1, 0])\naxes[1, 0].set_title('Duration vs Engagement Rate by Genre')\naxes[1, 0].set_xlabel('Duration (seconds)')\naxes[1, 0].set_ylabel('Engagement Rate (%)')\n\n# 4. Singer Popularity Distribution by Genre\nsns.boxplot(data=df,\n            x='genre',\n            y='singer_popularity',\n            ax=axes[1, 1])\naxes[1, 1].set_title('Singer Popularity Distribution by Genre')\naxes[1, 1].set_xlabel('Genre')\naxes[1, 1].set_ylabel('Singer Popularity')\naxes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log10\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/var/folders/qx/qwpm5zm52fzd9w3sfybpybzh0000gn/T/ipykernel_35679/2832034967.py:43: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  axes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=45)\n\n\n\n\n\n\n\n\n\nFrom plot Log10(Popularity) and Log10(Like Count) show a strong positive correlation between genres.Jazz has a lower popularity and like count, concentrated in the lower left corner of the plot. In contrast, Pop, Hip-hop and Electronic music occupy the upper right region, with higher popularity and more likes, while Rock is more evenly distributed. Overall, the relationship is close to linear on a logarithmic scale, indicating that the more popular a song is, the more the number of likes increases exponentially, with some fluctuations.\nFrom plot days to release vs. number of subscribers (Log10), we observe that, similar to the chart of Log10 (number of views) vs. number of subscribers (number of likes), jazz have significantly fewer subscribers than other genres. In contrast, pop, hip-hop, rock, and electronic music typically have higher subscriber counts. The relationship between the time span of publication and the number of subscribers is not very clear and the distribution is spread out. Notably, from the dispersed distribution, the relatively low number of subscribers in the pop, hip-hop and electronic music genres compared to the other genres suggests that publication time has little effect on the number of subscribers.\nFrom plot Duration vs Engagement Rate, it can be observed that the engagement rate of most music videos is concentrated in the range of 0-4%, and the video duration is mainly concentrated in less than 1,000 seconds, which is shorter video may be more likely to get a higher engagement rate.\nFrom plot Singer Popularity Distribution, it can be observed that Hip-hop and Pop singers have the highest median popularity, Jazz singers have the lowest popularity distribution and the largest dispersion, and Electronic and Rock singers are at a medium level of popularity.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\n\n\n# 3. Cross-tabulation Analysis\n# Create a cross-tab of genre and popularity categories\ndf['popularity_category'] = pd.qcut(df['singer_popularity'], \n                                  q=3, \n                                  labels=['Low', 'Medium', 'High'])\n\n# Select the relevant numeric columns for cross-tabulation analysis\nrelevant_cols_for_crosstab = ['singer_popularity', 'View Count','Days Since Published', 'Mean Sentiment Score']\n\n# Create a cross-tab of genre and popularity category\ngenre_popularity_crosstab = pd.crosstab(df['genre'], \n                                       df['popularity_category'], \n                                       normalize='index') * 100\n\nprint(\"\\nGenre-Popularity Cross-tabulation (%):\")\nprint(genre_popularity_crosstab)\n\n# Calculate summary statistics by genre for selected relevant columns\ngenre_summary = df.groupby('genre').agg({\n    'singer_popularity': 'mean',\n    'View Count': 'mean',\n    'like_ratio': 'mean',\n    'comment_ratio': 'mean',\n    'Mean Sentiment Score': 'mean',\n}).round(2)\n\nprint(\"\\nSummary Statistics by Genre:\")\nprint(genre_summary)\n\n\nGenre-Popularity Cross-tabulation (%):\npopularity_category        Low     Medium       High\ngenre                                               \nelectronic           57.894737  42.105263   0.000000\nhip-pop              12.857143  24.285714  62.857143\njazz                 86.111111   6.944444   6.944444\npop                   4.545455  38.636364  56.818182\nrock                 19.318182  55.681818  25.000000\n\nSummary Statistics by Genre:\n            singer_popularity    View Count  like_ratio  comment_ratio  \\\ngenre                                                                    \nelectronic              78.04  4.812136e+08        1.25           0.05   \nhip-pop                 86.93  2.120770e+08        1.73           0.10   \njazz                    63.74  1.062045e+07        1.26           0.05   \npop                     87.49  6.973413e+08        1.15           0.06   \nrock                    82.09  3.915309e+08        0.69           0.03   \n\n            Mean Sentiment Score  \ngenre                             \nelectronic                  0.27  \nhip-pop                     0.18  \njazz                        0.47  \npop                         0.32  \nrock                        0.37  \n\n\nThe Genre-Popularity crosstab shows that Hip-hop and Pop are the strongest performers, Rock is in the middle of the pack, while Electronic and Jazz are weak.\nAccording to the summary statistics by genre, we can see that Pop has the highest View Count and singer_popularity. In terms of interaction metrics, Hip-hop has the highest interaction rate, while Rock has the lowest. As for the Mean Sentiment Score, the rankings are Jazz, Rock, Electronic and Hip-hop.\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\n\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n# 1. Calculate Skewness and Kurtosis\nprint(\"Skewness and Kurtosis Analysis:\")\nfor col in numeric_cols:\n    skew = stats.skew(df[col].dropna())\n    kurt = stats.kurtosis(df[col].dropna())\n    print(f\"\\n{col}:\")\n    print(f\"Skewness: {skew:.2f}\")\n    print(f\"Kurtosis: {kurt:.2f}\")\n    if skew &gt; 1 or skew &lt; -1:  # Heavily skewed data\n        df[f'{col}_log'] = np.log1p(df[col])  # log(1 + value) to avoid log(0)\n        print(f\"Log Transformation Applied to {col}\")\n\nSkewness and Kurtosis Analysis:\n\nsinger_popularity:\nSkewness: -1.24\nKurtosis: 2.09\nLog Transformation Applied to singer_popularity\n\nDays Since Published:\nSkewness: -0.20\nKurtosis: -1.33\n\nSubscriber Count:\nSkewness: 2.39\nKurtosis: 6.14\nLog Transformation Applied to Subscriber Count\n\nView Count:\nSkewness: 3.68\nKurtosis: 18.98\nLog Transformation Applied to View Count\n\nlike_ratio:\nSkewness: 4.72\nKurtosis: 33.35\nLog Transformation Applied to like_ratio\n\ncomment_ratio:\nSkewness: 5.67\nKurtosis: 45.55\nLog Transformation Applied to comment_ratio\n\nDuration_seconds:\nSkewness: 8.98\nKurtosis: 86.03\nLog Transformation Applied to Duration_seconds\n\nMean Sentiment Score:\nSkewness: -0.09\nKurtosis: -0.36\n\nPopularity:\nSkewness: -1.27\nKurtosis: 1.42\nLog Transformation Applied to Popularity\n\n\n\n# 2. Normalization (Min-Max and Z-score scaling)\nscaler_minmax = MinMaxScaler()\nscaler_standard = StandardScaler()\n\n# Apply Min-Max scaling to numeric columns\ndf_minmax = df.copy()\ndf_minmax[numeric_cols] = scaler_minmax.fit_transform(df_minmax[numeric_cols])\n\n# Apply Z-score normalization to numeric columns\ndf_standard = df.copy()\ndf_standard[numeric_cols] = scaler_standard.fit_transform(df_standard[numeric_cols])\n\n\n# 3. Visualize the impact of normalization for all numeric columns\nplt.figure(figsize=(13,12))\n\n# Plot the original, Min-Max scaled, and Z-score normalized distributions for each numeric column\nfor i, col in enumerate(numeric_cols, start=1):\n    # Original distribution\n    plt.subplot(len(numeric_cols), 3, i*3-2)\n    sns.histplot(df[col], kde=True, color='blue', bins=30)\n    plt.title(f'Original {col} Distribution')\n\n    # Min-Max scaled distribution\n    plt.subplot(len(numeric_cols), 3, i*3-1)\n    sns.histplot(df_minmax[col], kde=True, color='green', bins=30)\n    plt.title(f'Min-Max Scaled {col}')\n\n    # Z-score normalized distribution\n    plt.subplot(len(numeric_cols), 3, i*3)\n    sns.histplot(df_standard[col], kde=True, color='red', bins=30)\n    plt.title(f'Z-score Normalized {col}')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\n\n# ANOVA test for view counts across genres\nfrom scipy.stats import f_oneway\ngenres = df['Genre'].unique()\ngenre_views = [df[df['Genre'] == genre]['View Count'] for genre in genres]\nf_stat, p_value = f_oneway(*genre_views)\nprint(\"\\n=== One-way ANOVA Test for View Count Across Genres ===\")\nprint(f\"F-statistic: {f_stat}\")\nprint(f\"p-value: {p_value}\")\n\ngenres = df['Duration_seconds'].unique()\ngenre_views = [df[df['Duration_seconds'] == genre]['Popularity'] for genre in genres]\nf_stat, p_value = f_oneway(*genre_views)\nprint(\"\\n=== One-way ANOVA Test for Popularity Across Duration_seconds===\")\nprint(f\"F-statistic: {f_stat}\")\nprint(f\"p-value: {p_value}\")\n\n\nlike_ratio_bins = df['like_ratio'].unique()\nlike_ratio_views = [df[df['like_ratio'] == like_ratio]['View Count'] for like_ratio in like_ratio_bins]\nf_stat_like_ratio, p_value_like_ratio = f_oneway(*like_ratio_views)\nprint(\"\\n=== One-way ANOVA Test for View Count Across Like Ratio ===\")\nprint(f\"F-statistic: {f_stat_like_ratio}\")\nprint(f\"p-value: {p_value_like_ratio}\")\n\n\n=== One-way ANOVA Test for View Count Across Genres ===\nF-statistic: 11.944468793592273\np-value: 3.871891640564041e-09\n\n=== One-way ANOVA Test for Popularity Across Duration_seconds===\nF-statistic: 1.1090156863397904\np-value: 0.24201725292066867\n\n=== One-way ANOVA Test for View Count Across Like Ratio ===\nF-statistic: 12.005339777316456\np-value: 0.07990432736153048\n\n\nFor the one-way ANOVA: Genres vs View Count the results showed an F-value of 11.9445 while the p-value was 3.87e-09, which is much less than 0.05. This indicates that there is a statistically significant difference in video viewership across music genres.\nFor the one-way ANOVA: Popularity vs Duration_seconds, the results showed an F-value of 1.1090 while the p-value was 0.2420, which is greater than 0.05. This indicates that there is no statistically significant difference in song popularity across different durations.\nFor the one-way ANOVA: Like Ratio vs View Count , an F value of 12.0053 was obtained, but the p-value was 0.0799, which is greater than 0.05, which suggests that there may be a certain correlation between the LIKE Ratio and the amount of viewing, but from a statistical point of view, this difference is not statistically significant.\n\n# T-test for 'View Count' based on 'mean_sentiment_score'\nfrom scipy.stats import ttest_ind\nimport numpy as np\nmedian_sentiment = df['Mean Sentiment Score'].median()\nhigh_sentiment = df[df['Mean Sentiment Score'] &gt; median_sentiment]['View Count']\nlow_sentiment = df[df['Mean Sentiment Score'] &lt;= median_sentiment]['View Count']\n\n# T-test for 'View Count' based on 'mean_sentiment_score'\nt_stat_sentiment, p_value_sentiment = ttest_ind(high_sentiment, low_sentiment)\nprint(\"\\n=== T-test for View Count Based on Mean Sentiment Score ===\")\nprint(f\"T-statistic: {t_stat_sentiment}\")\nprint(f\"p-value: {p_value_sentiment}\")\n\n\n=== T-test for View Count Based on Mean Sentiment Score ===\nT-statistic: -2.6437887108864815\np-value: 0.008544340977117903\n\n\nAs for T-test: View Count vs Mean Sentiment Score,p-value of 0.0085, which is less than 0.05, suggesting that differences in sentiment scores do indeed make a significant difference in viewings. The negative t-value further implies that there may be a negative correlation.\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\n\n\n# 1. Statistical Tests and Visualizations for Engagement Metrics\ndef analyze_engagement_metrics():\n    # Calculate engagement rate\n    df['engagement_rate'] = (df['Like Count'] + df['Comment Count']) / df['View Count'] * 100\n    \n    # Create visualization for engagement metrics\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # Scatter plot: Views vs Likes with regression line\n    sns.regplot(data=df, x='Popularity', y='Like Count', ax=axes[0,0])\n    axes[0,0].set_title('Correlation: Popularity vs Likes')\n    \n    # Calculate correlation\n    correlation = df['Popularity'].corr(df['Like Count'])\n    print(f\"\\nCorrelation between Popularity and Likes: {correlation:.3f}\")\n    \n    # Box plot: Engagement rate by genre\n    sns.boxplot(data=df, x='genre', y='engagement_rate', ax=axes[0,1])\n    axes[0,1].set_title('Engagement Rate by Genre')\n    axes[0,1].tick_labels = rotation=45\n    \n    # Violin plot: Sentiment distribution by genre\n    sns.violinplot(data=df, x='genre', y='Mean Sentiment Score', ax=axes[1,0])\n    axes[1,0].set_title('Sentiment Distribution by Genre')\n    axes[1,0].tick_labels = rotation=45\n    \n    # Scatter plot: Duration vs Views\n    sns.scatterplot(data=df, x='Duration_seconds', y='View Count', hue='genre', ax=axes[1,1])\n    axes[1,1].set_title('Duration vs Views by Genre')\n    \n    plt.tight_layout()\n    \n    return fig\n\nfig = analyze_engagement_metrics()\n\n\nCorrelation between Popularity and Likes: 0.361\n\n\n\n\n\n\n\n\n\nIn the terms of music genre, we can observe some interesting trends.Hip-hop has higher interaction rates but lower sentiment scores, indicates that despite frequent and positive viewer reactions and interactions to Hip-hop MVs, the comments or sentiment expressed in these videos may be more negatively charged. On the other hand, Jazz has lower view count but higher affective sentiment score, suggesting that despite fewer plays of this type of video, viewers generally rate its content more favorably, with more positive affective tendencies. This also reflects the fact that Jazz’s viewer base, although smaller, has a higher appreciation of the content.\nFor Pop, the view count is very high and the affective scores are at a medium level, showing that Pop music has a wide acceptance and popularity among viewers, and although the emotional expression is not as extreme as that of Jazz, it is still able to attract a large number of viewers.Rock, on the other hand, exhibits a more stable interaction rate and medium affective scores, which means that the Rock category has a relatively balanced audience base with a stable level of interaction, but its affective responses and affective scores are more positive. level is stable, but its emotional response is not as intense as Hip-hop.\nAs for Electronic, has a higher number of views but a lower interaction rate, reflecting that although music videos of Electronic music can attract a large number of viewers, the sense of audience engagement and interactive behavior is lower, probably because the audience group of this type of music is less interested in interaction and watches it more as background music.\nAll in all, we can conclude that Popularity is not the only factor that predicts the number of likes.. Meanwhile, short videos are more likely to inspire user interaction and sharing due to their short and concise nature, which coincides with the current hot trend of short video platforms.\nDespite Jazz’s better performance in sentiment scores and content ratings, its play count is generally low, which indicates that this music style has a more niche audience and is only enjoyed by specific groups. Pop and Electronic music videos, on the other hand, typically have higher plays, suggesting that if one wishes to create music content that is popular and can be distributed quickly, choosing the Pop or Electronic genres will undoubtedly be a more attractive option.\n\n\nConclusions and Next Steps\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling.\n\nIn addition to the findings in the previous section, we can also observe some trends and conclusions. First, both the like and comment rates show a heavily right-skewed distribution, implying that most music videos have a relatively small like count and comment count, while a few videos generate a large number of likes and comments, which is in line with the common pattern of interactions on social platforms: only a very small amount of content generates a wide range of attention and interactions. Meanwhile, the interaction rate is negatively correlated with video duration, which reflects an interesting phenomenon. Longer duration videos tend to lead to a decrease in viewers’ willingness to interact, probably because longer videos tend to fatigue viewers, while shorter videos are more likely to stimulate users’ attention and interaction.\nIn the correlation analysis of viewership, we find that it is moderately positively correlated with the number of subscribers, suggesting that the more subscribers a channel has, the higher the view count of the video is usually, which is consistent with our intuitive experience that the more popular a channel is, the more viewers will watch new videos accordingly. On the other hand, view count shows a weak positive correlation with singer_popularity and although this correlation is not strong, it implies that an singer popularity may have some effect on the viewership of his or her videos. It is worth noting that shorter videos are more likely to receive a higher number of views, which is in line with the current characteristics of user behavior on social platforms, especially short video platforms-users prefer to watch short and concise content.\nFrom the analysis of each music genre, we can also see that the sentiment scores of different genres differ significantly. When we conduct regression analysis, we can use the above observed characteristics as the main variables for modeling. First, genre is a key categorization feature that can effectively help us classify different types of music videos, affecting behaviors such as the view count and engagement rate. Second, video duration is also an important factor, but it may have a non-linear relationship with other features, so the effect of video duration needs to be specifically addressed in the modeling. The number of subscribers, as a reflection of the channel’s popularity, has a direct impact on the number of video views and should be used as one of the important input features. Singer popularity, on the other hand, can further help us analyze and predict video performance, especially when we want to delve into the role of singer influence on video interaction."
  },
  {
    "objectID": "technical-details/eda/main.html#suggested-page-structure",
    "href": "technical-details/eda/main.html#suggested-page-structure",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts."
  },
  {
    "objectID": "technical-details/eda/main.html#what-to-address",
    "href": "technical-details/eda/main.html#what-to-address",
    "title": "Exploratory Data Analysis",
    "section": "What to address",
    "text": "What to address\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html",
    "href": "technical-details/data-cleaning/main.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you on this page will be specific you your project and data. Some things might “make more sense” on other pages, depending on your workflow, for example, you might feel that normalization and scaling should be included in a later section, dealing with machine learning, rather than here, that is totally fine. Organize your project in the way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\n\nIterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#suggested-page-structure",
    "href": "technical-details/data-cleaning/main.html#suggested-page-structure",
    "title": "Data Cleaning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#general-comments",
    "href": "technical-details/data-cleaning/main.html#general-comments",
    "title": "Data Cleaning",
    "section": "",
    "text": "Iterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#what-to-address",
    "href": "technical-details/data-cleaning/main.html#what-to-address",
    "title": "Data Cleaning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html",
    "href": "technical-details/supervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results.\n\n\n\n\nNormalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling.\n\n\n\n\n\nModel Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used\n\n\n\n\n\nSplit Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset.\n\n\n\n\n\nBinary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc.\n\n\n\n\n\nModel Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots).\n\n\n\n\n\nResult Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#what-to-address",
    "href": "technical-details/supervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "href": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "title": "Instructions",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-selection",
    "href": "technical-details/supervised-learning/instructions.html#model-selection",
    "title": "Instructions",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "title": "Instructions",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "title": "Instructions",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#results",
    "href": "technical-details/supervised-learning/instructions.html#results",
    "title": "Instructions",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#discussion",
    "href": "technical-details/supervised-learning/instructions.html#discussion",
    "title": "Instructions",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/llm-usage-log.html",
    "href": "technical-details/llm-usage-log.html",
    "title": "LLM usage log",
    "section": "",
    "text": "This page can serve as a “catch-all” for LLM use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\nLLM tools were used in the following way for the tasks below"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#brainstorming",
    "href": "technical-details/llm-usage-log.html#brainstorming",
    "title": "LLM usage log",
    "section": "Brainstorming",
    "text": "Brainstorming\n\nLLM tools were used to brainstorm initial ideas for research topics. Potential topics considered include:\n\nAmazon purchase and review behavior analysis.\nSpotify music streaming data and factors influencing play counts.\nYouTube video view counts and the associated variables that impact popularity.\n\nAfter generating these ideas, LLM was consulted to evaluate their feasibility and relevance for the project. The model helped us refine these topics by providing insights on potential challenges and opportunities.\nAdditionally, LLM assisted in drafting a detailed project plan, including timelines and milestones, to ensure an organized workflow and a feasible approach for execution."
  },
  {
    "objectID": "technical-details/llm-usage-log.html#writing",
    "href": "technical-details/llm-usage-log.html#writing",
    "title": "LLM usage log",
    "section": "Writing:",
    "text": "Writing:\n\nReformating text from bulleted lists into proses\nProofreading\nText summarization for literature review"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#code",
    "href": "technical-details/llm-usage-log.html#code",
    "title": "LLM usage log",
    "section": "Code:",
    "text": "Code:\n\nCode commenting and explanatory documentation"
  },
  {
    "objectID": "technical-details/data-collection/overview.html",
    "href": "technical-details/data-collection/overview.html",
    "title": "Overview",
    "section": "",
    "text": "In this section, provide a high-level overview for technical staff, summarizing the key tasks and processes carried out here. Include the following elements:\n\n\nIdentify the key factors affecting the popularity official music video( view count) on Youtube by analyzing various features of official mv on Youtube and characteristics of corresponding songs on Spotify.\n\n\n\nView count of music video on YouTube, the world’s largest video- sharing platform is crucial for measuring the popularity and success of mv and song.\nUnderstanding the key factors that drive and impact the mv popularity can help music producers, singers improve mv content and promotion strategies.\nNowdays, it seems that few people have quantified the key factors needed to achieve a successful mv based on mv data, and most people trust their own industry experience, intuition, and artistic aesthetics, etc.\nCombing the data of Youtube and Spotify provides us with a more comprehensive view, since the user behavior on these two platforms are likely to influence each other.\n\n\n\nDetermine the primary factors that impact the mv view count, including: - YouTube features: days since published, like count, comment count, mean comment sentiment score, etc. - Spotify features: singer_popularity, duration , song_popularity, etc.\nExplore the correlation between Youtube and Spotify features and mv view count to identify the key traits.\nBased on the analysis, summarize and propose mv optimization suggestions to increase mv view count for singers, music producers and record labels.\nThis overview should give technical staff a clear understanding of the context and importance of the work, while guiding them on what to focus on in the details that follow."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html",
    "href": "technical-details/data-collection/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include &gt;}} tag.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling.\n\n\n\nBegin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work.\n\n\n\n\nDuring the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder\n\n\n\n\n\nYour data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "href": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#what-to-address",
    "href": "technical-details/data-collection/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#start-collecting-data",
    "href": "technical-details/data-collection/instructions.html#start-collecting-data",
    "title": "Instructions",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "href": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "title": "Instructions",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#requirements",
    "href": "technical-details/data-collection/instructions.html#requirements",
    "title": "Instructions",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html",
    "href": "technical-details/data-cleaning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you on this page will be specific you your project and data. Some things might “make more sense” on other pages, depending on your workflow, for example, you might feel that normalization and scaling should be included in a later section, dealing with machine learning, rather than here, that is totally fine. Organize your project in the way that makes the most sense to you."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "href": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "Suggested page structure",
    "text": "Suggested page structure\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#general-comments",
    "href": "technical-details/data-cleaning/instructions.html#general-comments",
    "title": "Instructions",
    "section": "General comments:",
    "text": "General comments:\n\nIterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#what-to-address",
    "href": "technical-details/data-cleaning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "What to address",
    "text": "What to address\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "instructions/website-structure.html",
    "href": "instructions/website-structure.html",
    "title": "Website project structure",
    "section": "",
    "text": "Please at miniumum include the following pages in your website:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\nSupervised-learning\nLLM-usage\nProgress-log\n\n\nPlease adhere closely to this structure, for consistency accross projects.\nSub-sections can be handles as markdown headers in the respective pages.\nYou can add more pages,and if you want, you can merge EDA and unsupervised learning into one page, since the are similar. Or make these section headers in the dropdown menu, for further sub-sections creation.\nFor example:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\n\nClustering\nDimensionality Reduction\n\nSupervised-learning\n\nFeature selection\n\nregression\nclassification\n\nClassification\n\nBinary classification\nMulti-class classification\n\nRegression\n\nLLM-usage\nProgress-log\n\n\nImportant: Exactly what you put on these pages will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\nA skeleton of the recommended version of the website is provided in the github classroom repository.\n./\n├── README.md\n├── _quarto.yml\n├── assets\n│   ├── gu-logo.png\n│   ├── nature.csl\n│   └── references.bib\n├── build.sh\n├── data\n│   ├── processed-data\n│   │   └── countries_population.csv\n│   └── raw-data\n│       └── countries_population.csv\n├── index.qmd\n├── instructions\n│   ├── expectations.qmd\n│   ├── github-usage.qmd\n│   ├── llm-usage.qmd\n│   ├── overview.qmd\n│   ├── quarto-tips.qmd\n│   ├── topic-selection.qmd\n│   └── website-structure.qmd\n├── report\n│   └── report.qmd\n└── technical-details\n    ├── data-cleaning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── data-collection\n    │   ├── closing.qmd\n    │   ├── instructions.qmd\n    │   ├── main.ipynb\n    │   ├── methods.qmd\n    │   └── overview.qmd\n    ├── eda\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── llm-usage-log.qmd\n    ├── progress-log.qmd\n    ├── supervised-learning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    └── unsupervised-learning\n        ├── instructions.qmd\n        └── main.ipynb\nAlways strive to incorporate the following:\n\nStructure: Use clear headings and subheadings to break down each section of your EDA.\nClarity: Provide concise explanations for all tables and visualizations, ensuring they are easy to interpret.\nCode Links: Link to relevant code (e.g., GitHub) or embed code snippets for transparency and reproducibility.\nReproducibility: Make your EDA reproducible by providing access to the dataset, scripts, and tools you used.\nVisualization: Use visualizations to convey key insights\n\n\n\nIt is required that you build your website with Quarto.\n\n\n\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO.\n\n\n\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/website-structure.html#website-development",
    "href": "instructions/website-structure.html#website-development",
    "title": "Website project structure",
    "section": "",
    "text": "It is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/website-structure.html#website-hosting",
    "href": "instructions/website-structure.html#website-hosting",
    "title": "Website project structure",
    "section": "",
    "text": "You MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/website-structure.html#the-two-audiences",
    "href": "instructions/website-structure.html#the-two-audiences",
    "title": "Website project structure",
    "section": "",
    "text": "Knowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/quarto-tips.html",
    "href": "instructions/quarto-tips.html",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton.\n\n\n\n\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/quarto-tips.html#file-types",
    "href": "instructions/quarto-tips.html#file-types",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/quarto-tips.html#quarto-includes",
    "href": "instructions/quarto-tips.html#quarto-includes",
    "title": "Quarto Tips",
    "section": "",
    "text": "Quarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/llm-usage.html",
    "href": "instructions/llm-usage.html",
    "title": "LLM usage",
    "section": "",
    "text": "We believe that the adoption of LLM tools is inevitable and will be an important skill for success in your future career. Therefore, appropriate and acceptable use of LLM tools is encouraged for this project. Use them to accelerate your workflow and learning, but not as a replacement for critical thinking and understanding. Carefully review and process their output, use them judiciously, and avoid bloating your text with LLM-generated content. Overusing these tools often degrades the quality of your work rather than enhancing it.\nRemember the following guidelines:\n\nUse common sense: If you feel like you’re doing something questionable, you probably are. A good test is to ask yourself, “Would I openly tell the professor or classmates what I’m doing right now?” If the answer is no, you’re probably doing something you shouldn’t.\nCite your LLM use cases: Always cite when and how you’ve used LLM tools. This is a requirement for the project.\nIs your use helping you grow professionally?: If your use of LLM tools is making you a more competent, efficient, and knowledgeable professional, you’re probably using them in an appropriate manner. If you’re using them as a shortcut to avoid work and gain free time, you’re using them incorrectly.\n\n\n\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1\n\n\n\n\n\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code.\n\n\n\n\n\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/llm-usage.html#citation",
    "href": "instructions/llm-usage.html#citation",
    "title": "LLM usage",
    "section": "",
    "text": "ALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/llm-usage.html#acceptable-use-cases",
    "href": "instructions/llm-usage.html#acceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "Note: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/llm-usage.html#unacceptable-use-cases",
    "href": "instructions/llm-usage.html#unacceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "DO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/expectations.html",
    "href": "instructions/expectations.html",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare.\n\n\n\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable.\n\n\n\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields.\n\n\n\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field.\n\n\n\n\n\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important.\n\n\n\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point.\n\n\n\n\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual.\n\n\n\n\n\n\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project .\n\n\n\n\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/expectations.html#get-started-early",
    "href": "instructions/expectations.html#get-started-early",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/expectations.html#graduate-level-work",
    "href": "instructions/expectations.html#graduate-level-work",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "This is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/expectations.html#what-is-impact",
    "href": "instructions/expectations.html#what-is-impact",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Impact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/expectations.html#what-makes-a-good-research-project",
    "href": "instructions/expectations.html#what-makes-a-good-research-project",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Professional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/expectations.html#time-management",
    "href": "instructions/expectations.html#time-management",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/expectations.html#visualization-guidelines",
    "href": "instructions/expectations.html#visualization-guidelines",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Visualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/expectations.html#coding",
    "href": "instructions/expectations.html#coding",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "While not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/expectations.html#debugging",
    "href": "instructions/expectations.html#debugging",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Always remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Landing page",
    "section": "",
    "text": "Audio instructions:\nIf you want, you can listen to the instructions:\nSource: Text-to-speech conversion done with Amazon Polly on AWS\nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Landing page",
    "section": "Getting Started",
    "text": "Getting Started\nTo begin the project, first read the instruction document (click here). This document is also accessible from the navigation bar.\nOnce you’ve completed that, you can proceed with the instructions found throughout the website."
  },
  {
    "objectID": "index.html#what-to-include-on-this-page",
    "href": "index.html#what-to-include-on-this-page",
    "title": "Landing page",
    "section": "What to Include on This Page",
    "text": "What to Include on This Page\nThis is the landing page for your project. Content from this page can be reused in sections of your final report.\n\nCreate an “About You” Page\n\nDevelop your “About You” page. You can reuse content from previous assignments.\nYou can include the content here or on a separate page.\n\nIt’s recommended to create one “About You” page for all DSAN projects, with links to your various class projects.\n\n\n\n\nCreate a Landing Page for Your Project\n\nSummarize your topic, its significance, related work, and the questions you plan to explore.\nDraft an introduction with at least 5 research questions. These may evolve as your project progresses, since data science is an iterative process.\nInclude your data science questions on this page.\n\n\n\nLiterature review\nOnce you decide on a topic, you should ALWAYS START WITH A LITERATURE REVIEW, this is particularly important for academic projects.\nThe literature review is the most important part of most projects.\nIt allows you to;\n\n\nDetermine what is already known and what has already been tried, so that you don't re-invent the wheel.\n\n\nIt makes you more of a subject matter expert, allowing you to ask the right questions, target impactful projects, and communicate with other professionals.\n\n\n\nDoing a project that you think will change the world, only to find at the end that a very similar version of your project was already done in the 1980’s, isn’t a great use of time.\n\nIn this section, please do a literature review and cite at least 3 academic publications per group member, and include internal academic citations.\nOptional: Consider using LLM tools to 10X your literature review, e.g. instead of focusing on 3 papers, aim for 30 or more\nBy following these steps, you can 10X the efficiency of your literature review process, gaining more insights while minimizing the time spent on manual reading.\n\nExpand Your Literature Search\nAim to gather a larger pool of papers. Use academic databases (Google Scholar, arXiv, etc.) to find relevant studies and ensure a broad scope.\nSkim the Abstracts\nRead the abstracts of each paper to quickly understand their focus and identify those most relevant to your topic. Prioritize these papers for deeper analysis.\nUse LLM Tools for Summarization\nUpload the selected papers to an LLM tool capable of text summarization. Have it condense the main points of each paper into a concise, manageable summary (e.g., condense hundreds of pages into a 10-page summary). Carefully review this summary to absorb the key insights.\nLeverage Interactive LLM Tools\nUse tools like NotebookLM or other AI-based text digesters to ask specific, targeted questions about the papers:\n\nExample questions:\n\n“In the papers uploaded, did any of them explore XYZ?”\n“Which paper is most closely related to the following project idea: [explain idea]?” These tools will help you quickly extract relevant information without re-reading entire papers."
  },
  {
    "objectID": "index.html#additional-ideas-for-things-to-include",
    "href": "index.html#additional-ideas-for-things-to-include",
    "title": "Landing page",
    "section": "Additional Ideas for things to include",
    "text": "Additional Ideas for things to include\n\nAudience: Who is this for? Data professionals, businesses, researchers, or curious readers.\nHeadline: A captivating title introducing the data science theme (e.g., “Unlocking Insights Through Data Stories”).\nIntroduction: A brief, engaging overview of what the website offers (e.g., data-driven stories, insights, or case studies).\nQuestions You Are Addressing: What do you hope to learn?\nMotivation: Explain why this topic matters, highlighting the importance of data in solving real-world problems.\nKey Topics: List the main focus areas (e.g., machine learning, data visualization, predictive modeling).\nUse Cases/Examples: A brief teaser of compelling stories or case studies you’ve worked on.\nCall to Action: Invite visitors to explore the content, follow along, or contact you for more information.\nVisual/Infographic: Add a simple graphic or visual element to make the page more dynamic."
  },
  {
    "objectID": "instructions/github-usage.html",
    "href": "instructions/github-usage.html",
    "title": "GitHub",
    "section": "",
    "text": "Your project will be fully transparent, with all source code hosted on GitHub. This platform will serve as the main repository for your project code, documentation, and website. Proper organization and regular updates are key for effective collaboration and project management.\n\nIMPORTANT: Proficiency in GitHub for collaboration is a valuable addition to your resume. Being able to join a team and immediately contribute by solving problems and adding value is a highly sought-after skill. Now is the time to develop this expertise—embrace Git fully, become proficient, and graduate with a critical skill for your future career.\n\n\n\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure.\n\n\n\n\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies.\n\n\n\n\n\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/github-usage.html#repository-setup",
    "href": "instructions/github-usage.html#repository-setup",
    "title": "GitHub",
    "section": "",
    "text": "You MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/github-usage.html#expectations-for-github-usage",
    "href": "instructions/github-usage.html#expectations-for-github-usage",
    "title": "GitHub",
    "section": "",
    "text": "Your grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "href": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "title": "GitHub",
    "section": "",
    "text": "If you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/overview.html",
    "href": "instructions/overview.html",
    "title": "Project instruction:",
    "section": "",
    "text": "Author(s): Dr. H and Gerard Pendleton Thurston the 4th\nNote: You can delete this folder and remove it from the project website once you have read and understood the instructions. It shouldn’t be part of your final submission.\nAudio instructions:\nIf you want, you can listen to the instructions:\n\n\n\n Source: Text-to-speech conversion done with Amazon Polly on AWS \nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website."
  },
  {
    "objectID": "instructions/overview.html#python-package-optional",
    "href": "instructions/overview.html#python-package-optional",
    "title": "Project instruction:",
    "section": "Python package (optional)",
    "text": "Python package (optional)\nThis is an optional component of the project, if you’d like, you can create a dedicated Python package for your project. The source folder for this package should be included in the root of your repository, and it can be imported into your processing scripts used in the various technical-details sections. If you create a package, it should be well-documented, with an additional tab on the navigation bar for the package documentation.\nWhile a package would typically have its own GitHub repository, for this project, please include it within the same repository.\nThe skeleton for the package is not provided in the repo, but you can recycle what you created in past assignments."
  },
  {
    "objectID": "instructions/overview.html#select-a-broad-topic-area",
    "href": "instructions/overview.html#select-a-broad-topic-area",
    "title": "Project instruction:",
    "section": "Select a broad topic area",
    "text": "Select a broad topic area\nStart by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/overview.html#narrow-your-focus",
    "href": "instructions/overview.html#narrow-your-focus",
    "title": "Project instruction:",
    "section": "Narrow your focus",
    "text": "Narrow your focus\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/overview.html#data-science-questions",
    "href": "instructions/overview.html#data-science-questions",
    "title": "Project instruction:",
    "section": "Data Science Questions",
    "text": "Data Science Questions\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/overview.html#repository-setup",
    "href": "instructions/overview.html#repository-setup",
    "title": "Project instruction:",
    "section": "Repository Setup",
    "text": "Repository Setup\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/overview.html#expectations-for-github-usage",
    "href": "instructions/overview.html#expectations-for-github-usage",
    "title": "Project instruction:",
    "section": "Expectations for GitHub Usage",
    "text": "Expectations for GitHub Usage\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n1. Use a Logical Repository Structure\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n2. Commit Regularly\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n3. Data Storage\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n4. Syncing with GU Domains\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n5. Code Documentation\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "href": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "title": "Project instruction:",
    "section": "Collaboration in Groups (If Applicable)",
    "text": "Collaboration in Groups (If Applicable)\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/overview.html#website-development",
    "href": "instructions/overview.html#website-development",
    "title": "Project instruction:",
    "section": "Website Development",
    "text": "Website Development\nIt is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/overview.html#website-hosting",
    "href": "instructions/overview.html#website-hosting",
    "title": "Project instruction:",
    "section": "Website Hosting",
    "text": "Website Hosting\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/overview.html#the-two-audiences",
    "href": "instructions/overview.html#the-two-audiences",
    "title": "Project instruction:",
    "section": "The two audiences",
    "text": "The two audiences\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/overview.html#get-started-early",
    "href": "instructions/overview.html#get-started-early",
    "title": "Project instruction:",
    "section": "Get started early",
    "text": "Get started early\nRemember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/overview.html#graduate-level-work",
    "href": "instructions/overview.html#graduate-level-work",
    "title": "Project instruction:",
    "section": "Graduate level work",
    "text": "Graduate level work\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "Project instruction:",
    "section": "The intersection of skills and domain knowledge",
    "text": "The intersection of skills and domain knowledge\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/overview.html#what-is-impact",
    "href": "instructions/overview.html#what-is-impact",
    "title": "Project instruction:",
    "section": "What is “impact”?",
    "text": "What is “impact”?\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/overview.html#what-makes-a-good-research-project",
    "href": "instructions/overview.html#what-makes-a-good-research-project",
    "title": "Project instruction:",
    "section": "What makes a good research project?",
    "text": "What makes a good research project?\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/overview.html#time-management",
    "href": "instructions/overview.html#time-management",
    "title": "Project instruction:",
    "section": "Time management",
    "text": "Time management\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/overview.html#visualization-guidelines",
    "href": "instructions/overview.html#visualization-guidelines",
    "title": "Project instruction:",
    "section": "Visualization guidelines",
    "text": "Visualization guidelines\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/overview.html#coding",
    "href": "instructions/overview.html#coding",
    "title": "Project instruction:",
    "section": "Coding",
    "text": "Coding\n\nPractice First\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\nPrototype and develop on a small data set\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/overview.html#debugging",
    "href": "instructions/overview.html#debugging",
    "title": "Project instruction:",
    "section": "Debugging",
    "text": "Debugging\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/overview.html#file-types",
    "href": "instructions/overview.html#file-types",
    "title": "Project instruction:",
    "section": "File Types",
    "text": "File Types\nYou can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/overview.html#quarto-includes",
    "href": "instructions/overview.html#quarto-includes",
    "title": "Project instruction:",
    "section": "Quarto Includes",
    "text": "Quarto Includes\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\nWhy Use Quarto Includes?\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/overview.html#citation",
    "href": "instructions/overview.html#citation",
    "title": "Project instruction:",
    "section": "Citation",
    "text": "Citation\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/overview.html#acceptable-use-cases",
    "href": "instructions/overview.html#acceptable-use-cases",
    "title": "Project instruction:",
    "section": "Acceptable use cases",
    "text": "Acceptable use cases\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/overview.html#unacceptable-use-cases",
    "href": "instructions/overview.html#unacceptable-use-cases",
    "title": "Project instruction:",
    "section": "Unacceptable use cases",
    "text": "Unacceptable use cases\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\nEffect on grade\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\nPlagiarism investigation\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/topic-selection.html",
    "href": "instructions/topic-selection.html",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena\n\n\n\n\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation.\n\n\n\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/topic-selection.html#select-a-broad-topic-area",
    "href": "instructions/topic-selection.html#select-a-broad-topic-area",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/topic-selection.html#narrow-your-focus",
    "href": "instructions/topic-selection.html#narrow-your-focus",
    "title": "Topic selection",
    "section": "",
    "text": "Narrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/topic-selection.html#data-science-questions",
    "href": "instructions/topic-selection.html#data-science-questions",
    "title": "Topic selection",
    "section": "",
    "text": "The data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "report/report.html",
    "href": "report/report.html",
    "title": "Final Report",
    "section": "",
    "text": "Audio instructions:\nIf you want, you can listen to the instructions:\n\n\n\n Source: Text-to-speech conversion done with Amazon Polly on AWS \nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website.\nThis report is designed for a non-technical audience (e.g., the general public, executives, marketing teams, or clients), focusing on high-level insights, actionable results, and visualizations to convey the impact without requiring technical knowledge. The goal is to highlight how a model affects business strategy or revenue without diving into complex methods.\n\n\n\nClear Purpose: Define the core message or objective early on.\nKnow Your Audience: Adjust language, tone, and detail based on the audience’s understanding.\nStrong Opening: Start with a hook that sets context and stakes.\nLogical Flow: Structure with a clear beginning, middle, and end.\nKey Insights: Highlight the most important points; avoid unnecessary details or jargon.\nData Support: Use data to enhance the narrative without overwhelming.\nVisuals: Incorporate charts to simplify ideas and engage the audience.\nActionable Takeaways: Conclude with recommendations or next steps.\nAuthenticity: Use storytelling to make the content engaging and relatable.\nRevise: Edit for clarity and impact, removing unnecessary content.\n\n\n\n\nThese are just examples, you can use any structure that is suitable for your project.\n\n\n\nIntroduction: Provide an accessible overview and explain the motivation and importance of the research. Example: “This study explores how climate change affects local ecosystems, vital for wildlife conservation.”\nObjective: Clearly define the research goal and relate it to real-world challenges. Example: “We aim to analyze the effects of air pollution on public health in urban areas.”\nKey Findings: Present insights without technical terms, focusing on the impact. Example: “Air pollution increases the risk of respiratory diseases by 20%.”\nMethodology Overview: Briefly explain relevant methods. Example: “We analyzed air quality data from 50 cities and surveyed 10,000 residents.”\nVisualizations: Use simple graphs and infographics to convey findings. Example: A map showing pollution levels and a bar chart of health risks.\nSocietal Implications: Highlight the broader impact. Example: “This study highlights the need for better air quality policies.”\nCall to Action: Offer recommendations based on findings. Example: “We recommend city planners invest in green spaces.”\nConclusion: Recap the main findings and societal impact. Example: “Understanding pollution’s health impacts will help create healthier cities.”\n\n\n\n\n\nExecutive Summary: Summarize key findings and their relevance to business goals. Example: “This report predicts customer churn and offers strategies to reduce churn by 15%.”\nObjective: Define the problem or question addressed. Example: “This project aims to identify the drivers of customer churn.”\nKey Insights: Present the most important, actionable insights. Example: “Customers who interact with support twice within 30 days are 25% less likely to churn.”\nVisualizations: Use clear graphs to convey key insights. Example: A bar chart showing churn likelihood based on engagement.\nBusiness Implications: Explain how findings impact business outcomes and offer specific recommendations. Example: “Focus retention efforts on low-engagement customers.”\nRecommendations: Provide clear, actionable steps with projections. Example: “Implement an automated retention campaign, reducing churn by 10%.”\nConclusion: Summarize findings and suggest next steps. Example: “Addressing churn drivers can reduce loss and improve profitability.”\nAppendix (Optional): Additional charts or explanations for further insights.\n\n\n\n\n\n\nSimplicity: Avoid jargon; focus on business-relevant insights.\nVisual Focus: Prioritize charts and graphs over dense text.\nEmphasize Impact: Always link data insights to business outcomes."
  },
  {
    "objectID": "report/report.html#guidelines-for-creating-a-good-narrative",
    "href": "report/report.html#guidelines-for-creating-a-good-narrative",
    "title": "Final Report",
    "section": "",
    "text": "Clear Purpose: Define the core message or objective early on.\nKnow Your Audience: Adjust language, tone, and detail based on the audience’s understanding.\nStrong Opening: Start with a hook that sets context and stakes.\nLogical Flow: Structure with a clear beginning, middle, and end.\nKey Insights: Highlight the most important points; avoid unnecessary details or jargon.\nData Support: Use data to enhance the narrative without overwhelming.\nVisuals: Incorporate charts to simplify ideas and engage the audience.\nActionable Takeaways: Conclude with recommendations or next steps.\nAuthenticity: Use storytelling to make the content engaging and relatable.\nRevise: Edit for clarity and impact, removing unnecessary content."
  },
  {
    "objectID": "report/report.html#report-content",
    "href": "report/report.html#report-content",
    "title": "Final Report",
    "section": "",
    "text": "These are just examples, you can use any structure that is suitable for your project.\n\n\n\nIntroduction: Provide an accessible overview and explain the motivation and importance of the research. Example: “This study explores how climate change affects local ecosystems, vital for wildlife conservation.”\nObjective: Clearly define the research goal and relate it to real-world challenges. Example: “We aim to analyze the effects of air pollution on public health in urban areas.”\nKey Findings: Present insights without technical terms, focusing on the impact. Example: “Air pollution increases the risk of respiratory diseases by 20%.”\nMethodology Overview: Briefly explain relevant methods. Example: “We analyzed air quality data from 50 cities and surveyed 10,000 residents.”\nVisualizations: Use simple graphs and infographics to convey findings. Example: A map showing pollution levels and a bar chart of health risks.\nSocietal Implications: Highlight the broader impact. Example: “This study highlights the need for better air quality policies.”\nCall to Action: Offer recommendations based on findings. Example: “We recommend city planners invest in green spaces.”\nConclusion: Recap the main findings and societal impact. Example: “Understanding pollution’s health impacts will help create healthier cities.”\n\n\n\n\n\nExecutive Summary: Summarize key findings and their relevance to business goals. Example: “This report predicts customer churn and offers strategies to reduce churn by 15%.”\nObjective: Define the problem or question addressed. Example: “This project aims to identify the drivers of customer churn.”\nKey Insights: Present the most important, actionable insights. Example: “Customers who interact with support twice within 30 days are 25% less likely to churn.”\nVisualizations: Use clear graphs to convey key insights. Example: A bar chart showing churn likelihood based on engagement.\nBusiness Implications: Explain how findings impact business outcomes and offer specific recommendations. Example: “Focus retention efforts on low-engagement customers.”\nRecommendations: Provide clear, actionable steps with projections. Example: “Implement an automated retention campaign, reducing churn by 10%.”\nConclusion: Summarize findings and suggest next steps. Example: “Addressing churn drivers can reduce loss and improve profitability.”\nAppendix (Optional): Additional charts or explanations for further insights."
  },
  {
    "objectID": "report/report.html#final-tips",
    "href": "report/report.html#final-tips",
    "title": "Final Report",
    "section": "",
    "text": "Simplicity: Avoid jargon; focus on business-relevant insights.\nVisual Focus: Prioritize charts and graphs over dense text.\nEmphasize Impact: Always link data insights to business outcomes."
  },
  {
    "objectID": "technical-details/data-collection/closing.html",
    "href": "technical-details/data-collection/closing.html",
    "title": "Summary",
    "section": "",
    "text": "This section should be written for a technical audience, focusing on detailed analysis, factual reporting, and clear presentation of data. The following serves as a guide, but feel free to adjust as needed.\n\n\nI encountered several technical challenges during the data collection process. First, when selecting artists, some singers may have turned off the music video comment section on YouTube, which caused me to have to re-search for other artists. In addition, Spotify’s search engine is not always accurate, and even if I provide the song title and artist name, I still may not be able to find the target song. This is an issue for which I haven’t found a perfect solution yet, considering that I may need to further clean up the song titles to make them more accurate in order to improve the accuracy of my searches. Although the original plan was to acquire 500 pieces of data, in the end only 376 pieces of data were successfully acquired. So we may have to make real-time adjustments when it comes to data collection.\nAnother technical difficulty is that the YouTube API sometimes returns irrelevant data, so it is important to carefully review and filter out the results that meet the requirements. In addition, the use of matching key to find relevant data in the YouTube and Spotify APIs is very important to improve the matching accuracy and reliability of the data.\n\n\n\nThe challenge of the project was to optimize the use of the API and reduce the interference of irrelevant data. While most projects can collect large amounts of data through APIs, accurately finding the target data is still a common difficulty. Therefore, how to improve the quality of data through data cleansing and precise matching algorithms remains a direction for further research and improvement in the future.\n\n\n\nWe can conclude that firstly, during the data collection process, despite the convenience provided by the use of APIs, the issues of exact matching and removal of irrelevant data still need to be addressed. Second, Spotify’s search engine may need further optimization or more detailed preprocessing of the input data to improve the accuracy of the matches.\nFuture steps could focus on optimizing the process of data collection and cleansing. For example, for the Spotify search problem, attempts could be made to improve the method of standardization of song titles or validation in combination with other data sources. On the other hand, further research can be done to improve the efficiency of data filtering in the YouTube API to ensure that the data collected is more in line with expectations and less interfered by irrelevant information. These improvements will help enhance the efficiency and quality of data collection and lay a solid foundation for subsequent analysis."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#challenges",
    "href": "technical-details/data-collection/closing.html#challenges",
    "title": "Summary",
    "section": "",
    "text": "I encountered several technical challenges during the data collection process. First, when selecting artists, some singers may have turned off the music video comment section on YouTube, which caused me to have to re-search for other artists. In addition, Spotify’s search engine is not always accurate, and even if I provide the song title and artist name, I still may not be able to find the target song. This is an issue for which I haven’t found a perfect solution yet, considering that I may need to further clean up the song titles to make them more accurate in order to improve the accuracy of my searches. Although the original plan was to acquire 500 pieces of data, in the end only 376 pieces of data were successfully acquired. So we may have to make real-time adjustments when it comes to data collection.\nAnother technical difficulty is that the YouTube API sometimes returns irrelevant data, so it is important to carefully review and filter out the results that meet the requirements. In addition, the use of matching key to find relevant data in the YouTube and Spotify APIs is very important to improve the matching accuracy and reliability of the data."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#benchmarks",
    "href": "technical-details/data-collection/closing.html#benchmarks",
    "title": "Summary",
    "section": "",
    "text": "The challenge of the project was to optimize the use of the API and reduce the interference of irrelevant data. While most projects can collect large amounts of data through APIs, accurately finding the target data is still a common difficulty. Therefore, how to improve the quality of data through data cleansing and precise matching algorithms remains a direction for further research and improvement in the future."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "title": "Summary",
    "section": "",
    "text": "We can conclude that firstly, during the data collection process, despite the convenience provided by the use of APIs, the issues of exact matching and removal of irrelevant data still need to be addressed. Second, Spotify’s search engine may need further optimization or more detailed preprocessing of the input data to improve the accuracy of the matches.\nFuture steps could focus on optimizing the process of data collection and cleansing. For example, for the Spotify search problem, attempts could be made to improve the method of standardization of song titles or validation in combination with other data sources. On the other hand, further research can be done to improve the efficiency of data filtering in the YouTube API to ensure that the data collected is more in line with expectations and less interfered by irrelevant information. These improvements will help enhance the efficiency and quality of data collection and lay a solid foundation for subsequent analysis."
  },
  {
    "objectID": "technical-details/data-collection/methods.html",
    "href": "technical-details/data-collection/methods.html",
    "title": "Methods",
    "section": "",
    "text": "Methods\nIn this section, provide an overview summary of the methods used on this page. This should include a brief description of the key techniques, algorithms, tools, or processes employed in your work. Make sure to outline the approach taken for data collection, processing, analysis, or any specific technical steps relevant to the project.\nIf you are developing a package, include a reference to the relevant documentation and provide a link here for easy access. Ensure that the package details are properly documented in its dedicated section, but mentioned and connected here for a complete understanding of the methods used in this project.\nmy method: This project aims to explore the key factors affecting the view count of official mv on Youtube. We employed the dual- platform data collection method, combing the API interfaces of Youtube and Spotify to get the overall horizon. As for the music genre selection, we chose pop, hip-pop, rock, electronic music, jazz, and selected 20 representative artists for each genre, as well as the top five official music videos, to ensure the diversity and representativeness of the sample.\nThe data collection method is divided into two main phases. Firstly, details of mv such as view count, like comment, top 10 comment, definition were obtained through Youtube API. Then, by using Spotify Web API, specialized traits on metrics on artists and tracks were collected.\nFinally, the data integration process uses artists name as the main matching key to ensure the data quality through a strict cleaning process."
  },
  {
    "objectID": "technical-details/eda/instructions.html",
    "href": "technical-details/eda/instructions.html",
    "title": "Introduction and Motivation",
    "section": "",
    "text": "Introduction and Motivation\nThis eda section focuses primarily on examining the relationship between track popularity, mv’s view count and youtube related factors. By analyzing the trends and correlations between these variables, we want to find which YouTube factors have a significant impact on Spotify track popularity,view count? This analysis can help us better understand the cross-platform interaction mechanism of the music market and provide data support for music promotion strategies.\n\n\nOverview of Methods\nFor Univariate Analysis, we use summary statistics to provide an overview of central tendencies and spread. Distributions were visualized using histograms with KDE (Kernel Density Estimation) overlays for key metrics such as the logarithm of View Count, like_ratio, comment_ratio and singer_popularity. For categorical variables, the distribution of music genres is examined using a bar plot, highlighting the frequency of each genre.\nFor Bivariate and Multivariate Analysis, we employ correlation analysis, visualization, and statistical summarization to explore relationships in the dataset. It calculates a correlation matrix to identify linear relationships between numerical variables and uses a heatmap for visualization. Scatterplots analyze pairwise relationships like logarithmic popularity and like count, days since published and subscriber count, and engagement rate versus duration, with genre as a categorical hue. A boxplot examines the distribution of singer popularity across genres. Cross-tabulation categorizes singer popularity into three levels (low, medium, high) and calculates genre-wise percentages, while groupby summarization provides mean values of key metrics by genre.\nFor Data Distribution and Normalization, We use skewness and kurtosis to assess the distribution of numeric variables. For heavily skewed data, log transformation was applied to reduce skewness. Normalization techniques were implemented, including Min-Max scaling to scale values between 0 and 1 and Z-score normalization to standardize data with a mean of 0 and a standard deviation of 1. The impact of these transformations was visualized using histograms for original, Min-Max scaled, and Z-score normalized distributions, highlighting changes in data distribution.\nFor Statistical Insights, Our method includes ANOVA and T-tests. One-way ANOVA was applied to compare means of ‘View Count’ across different genres, ‘Popularity’ across various durations, and ‘View Count’ across like ratio bins, providing F-statistics and p-values to assess significant differences. Additionally, a T-test was conducted to evaluate’View Count’ based on the median of ‘Mean Sentiment Score.’\nFor Visual Summary, An engagement rate was calculated as the combined percentage of likes and comments relative to views. To explore relationships, scatter plots with regression lines were used to assess the correlation between popularity and likes, while a box plot examined engagement rates across genres. A violin plot visualized the distribution of sentiment scores by genre, and a scatter plot highlighted the relationship between video duration and views, categorized by genre.\n\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/eda/instructions.html#suggested-page-structure",
    "href": "technical-details/eda/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts."
  },
  {
    "objectID": "technical-details/eda/instructions.html#what-to-address",
    "href": "technical-details/eda/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "What to address",
    "text": "What to address\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/progress-log.html",
    "href": "technical-details/progress-log.html",
    "title": "Progress log",
    "section": "",
    "text": "Use this page to track your progress and keep a log of your contributions to the project, please update this each time you work on your project, it is generally a good habit to adopt.\nIf you are working as a team, at the end, you can duplicate the project and add it to your individual portfolio websites. If you do, you MUST retain attribution to your teammates. Removing attribution would constitute plagiarism."
  },
  {
    "objectID": "technical-details/progress-log.html#to-do",
    "href": "technical-details/progress-log.html#to-do",
    "title": "Progress log",
    "section": "To-do",
    "text": "To-do\n\nConfirm the project topic\nConfirm the data sources\nCollect data using YouTube API\nCollect data using Spotify API\nClean the data\nNormalize the data\nPerform Exploratory Data Analysis (EDA)\nImplement regression analysis\nImplement binary classification\nImplement multi-class classification\nWork on unsupervised learning (e.g., clustering, dimensionality reduction)\nUpdate progress log regularly\nWork on the LLM (Language Model) part of the project\nWrite the final report"
  },
  {
    "objectID": "technical-details/progress-log.html#member-1",
    "href": "technical-details/progress-log.html#member-1",
    "title": "Progress log",
    "section": "Member-1:",
    "text": "Member-1:\nProvide their name, a link to their “About Me” page.\nAlso, describe a log of their project roles.\nWeekly project contribution log:\nT: 10-15-2024\n\nCoordinate with team member to set up weekly meeting time\n\nM: 10-14-2024\n\nDo a first draft of the project landing page"
  },
  {
    "objectID": "technical-details/progress-log.html#member-2",
    "href": "technical-details/progress-log.html#member-2",
    "title": "Progress log",
    "section": "Member-2",
    "text": "Member-2\nProvide their name, a link to their “About Me” page.\nAlso, describe a log of their project roles.\nWeekly project contribution log:\nW: 10-16-2024 （星期，和日期）从近到远的顺序罗列\n\nAttend first group meeting"
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html",
    "href": "technical-details/unsupervised-learning/instructions.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the dataset\ndf = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\n\n# Specify the feature columns\nfeatures = [\n    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',\n    'Subscriber Count', 'Definition', 'Mean Sentiment Score',\n    'Duration_seconds', 'genre_label', 'singer_followers', 'singer_popularity'\n]\n\n# Ensure the target column 'popularity' exists in DataFrame\nif 'Popularity' not in df.columns:\n    raise ValueError(\"The 'popularity' column is missing from the DataFrame.\")\n\n# Split into input (X) and target (y)\nX = df[features]  # Inputs\ny = df['Popularity']  # Target\n\n# Applying PCA\npca = PCA(n_components=3)  # Reduce to 3 dimensions for visualization\nX_pca = pca.fit_transform(X)\n# How much variance was retained?\nprint(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\nprint(\"Total Variance Explained:\", sum(pca.explained_variance_ratio_))\n\nExplained Variance Ratio: [0.35411601 0.1606999  0.12194563]\nTotal Variance Explained: 0.6367615347014879"
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "Suggested page structure",
    "text": "Suggested page structure\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "href": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "What to address",
    "text": "What to address\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\nPart 1: Dimensionality Reduction\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\nPart 2: Clustering Methods\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/data-collection/main.html",
    "href": "technical-details/data-collection/main.html",
    "title": "Data Collection",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include &gt;}} tag.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling.\n\n\n\nBegin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work.\n\n\n\n\nDuring the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder\n\n\n\n\n\nYour data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/main.html#suggested-page-structure",
    "href": "technical-details/data-collection/main.html#suggested-page-structure",
    "title": "Data Collection",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/main.html#what-to-address",
    "href": "technical-details/data-collection/main.html#what-to-address",
    "title": "Data Collection",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/main.html#start-collecting-data",
    "href": "technical-details/data-collection/main.html#start-collecting-data",
    "title": "Data Collection",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/main.html#saving-the-raw-data",
    "href": "technical-details/data-collection/main.html#saving-the-raw-data",
    "title": "Data Collection",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/main.html#requirements",
    "href": "technical-details/data-collection/main.html#requirements",
    "title": "Data Collection",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/main.html#example",
    "href": "technical-details/data-collection/main.html#example",
    "title": "Data Collection",
    "section": "Example",
    "text": "Example\nIn the following code, we first utilized the requests library to retrieve the HTML content from the Wikipedia page. Afterward, we employed BeautifulSoup to parse the HTML and locate the specific table of interest by using the find function. Once the table was identified, we extracted the relevant data by iterating through its rows, gathering country names and their respective populations. Finally, we used Pandas to store the collected data in a DataFrame, allowing for easy analysis and visualization. The data could also be optionally saved as a CSV file for further use.\n\nData Collection\nThis project collects music data through YouTube and Spotify APIs, covering information on the works of 20 representative artists in five genres: Electronic, Jazz, Hip-Hop, Pop and Rock. The data processing flow is as follows:\n\n\n1. YouTube Data Collection\n\n1.1 Acquiring Official Music Video Data\n\nfrom googleapiclient.discovery import build\nimport pandas as pd\nfrom datetime import datetime\nimport dateutil.parser\n\n# API Key\napi_key = \"AIzaSyDtKE-4QZj6EA-rwG7cj5gMJxdt4Fe14Nw\"\n\n# Initialize YouTube API client\nyoutube = build('youtube', 'v3', developerKey=api_key)\n\n# List to store data\nall_data = []\n\n# Read song data and fetch YouTube statistics\n# We should have put 20*5 names of singers, but for the sake of presentation, we choose three singers as a demonstration here \nwith open('find.txt', 'r') as file:\n    for line in file:\n        artist = line.strip()\n        query = f\"{artist} official music video\"\n\n        # Search request for the query\n        search_request = youtube.search().list(\n            part=\"snippet\",\n            q=query,\n            maxResults=5,\n            type=\"video\",\n            order='relevance'\n        )\n        search_response = search_request.execute()\n\n        for item in search_response['items']:\n            video_id = item['id']['videoId']\n            video_request = youtube.videos().list(\n                part=\"snippet,contentDetails,statistics\",\n                id=video_id\n            )\n            video_response = video_request.execute()\n\n            for video in video_response['items']:\n                snippet = video['snippet']\n                content_details = video['contentDetails']\n                statistics = video['statistics']\n\n                # Calculate days since video was published\n                published_at = dateutil.parser.parse(snippet['publishedAt'])\n                days_since_published = (datetime.now(published_at.tzinfo) - published_at).days\n\n                # Fetch channel details for subscriber count\n                channel_response = youtube.channels().list(\n                    part='statistics',\n                    id=snippet['channelId']\n                ).execute()\n                subscriber_count = channel_response['items'][0]['statistics'].get('subscriberCount', '0')\n\n                # Fetch comments\n                comments_request = youtube.commentThreads().list(\n                    part='snippet',\n                    videoId=video_id,\n                    order='relevance',\n                    maxResults=10\n                )\n                comments_response = comments_request.execute()\n\n                comments = [comment['snippet']['topLevelComment']['snippet']['textDisplay']\n                            for comment in comments_response.get('items', [])]\n\n                # Store all data in a dictionary\n                video_data = {\n                    'Video ID': video_id,\n                    'Title': snippet['title'],\n                    'Description': snippet['description'],\n                    'Published At': snippet['publishedAt'],\n                    'Days Since Published': days_since_published,\n                    'View Count': statistics.get('viewCount', '0'),\n                    'Like Count': statistics.get('likeCount', '0'),\n                    'Comment Count': statistics.get('commentCount', '0'),\n                    'Comments': comments,\n                    'Subscriber Count': subscriber_count,\n                    'Category ID': snippet['categoryId'],\n                    'Definition': content_details['definition'],\n                    'Duration': content_details['duration']\n                }\n\n                all_data.append(video_data)\n\n# Convert the list to a DataFrame\nfinal_df = pd.DataFrame(all_data)\n\n# Optionally save the DataFrame to a CSV file\nfinal_df.to_csv('Example_Detailed_YouTube_Video_Data.csv', index=False)\n\n# Display the DataFrame\nprint(final_df.head())\n\n      Video ID                                              Title  \\\n0  1VQ_3sBZEm0    Foo Fighters - Learn To Fly (Official HD Video)   \n1  eBG7P-K-r1Y        Foo Fighters - Everlong (Official HD Video)   \n2  SBjQ9tuuTJQ                       Foo Fighters - The Pretender   \n3  EqWRaAF6_WY         Foo Fighters - My Hero (Official HD Video)   \n4  h_L4Rixya64  Foo Fighters - Best Of You (Official Music Video)   \n\n                                         Description          Published At  \\\n0  Foo Fighters' official music video for 'Learn ...  2009-10-03T04:46:13Z   \n1  \"Everlong\" by Foo Fighters \\nListen to Foo Fig...  2009-10-03T04:49:58Z   \n2  Watch the official music video for \"The Preten...  2009-10-03T04:46:14Z   \n3  \"My Hero\" by Foo Fighters \\nListen to Foo Figh...  2011-03-18T19:35:42Z   \n4  Watch the official music video for \"Best Of Yo...  2009-10-03T20:49:33Z   \n\n   Days Since Published View Count Like Count Comment Count  \\\n0                  5550  183921366     808172         33856   \n1                  5550  324414087    1821270         53201   \n2                  5550  588092620    2785700         92245   \n3                  5018   87531478     564448         26099   \n4                  5549  265573360    1281212         34999   \n\n                                            Comments Subscriber Count  \\\n0  [I’m just realising how great Dave grohls acti...          1290000   \n1  [Dad died today. \\r&lt;br&gt;1:20 am.\\r&lt;br&gt;A five da...          1290000   \n2  [So thankful for this awesome song. I&#39;ll b...          1290000   \n3  [My son and I was supposed to spend the summer...          1290000   \n4  [The emotion in his face and his voice transce...          1290000   \n\n  Category ID Definition Duration  \n0          10         hd  PT4M37S  \n1          10         hd  PT4M52S  \n2          10         hd  PT4M31S  \n3          10         hd   PT4M3S  \n4          10         hd  PT4M16S  \n\n\n\n\n1.2 Merge artist name and music genre into csv\nThe row of the initial find.csv(include artist name and genre) is repeated five times per row to correspond to the five mv chosen by each artist (python) Then merge these two columns into the csv (copy manually)\n\nimport pandas as pd\n\ninput_file = './find.csv'  \noutput_file = './Example_singer_info.csv'  \n\n# Load the input CSV file\ndf = pd.read_csv(input_file)\n\n# Create an empty DataFrame to store repeated rows\nrepeated_df = pd.DataFrame()\n\n# Repeat each row 5 times and append it to the new DataFrame\nfor i in range(len(df)):\n    repeated_df = pd.concat([repeated_df, pd.DataFrame([df.iloc[i]] * 5)], ignore_index=True)\n\n# Save the processed DataFrame to a new CSV file\nrepeated_df.to_csv(output_file, index=False)\n\nprint(f\"Data processed successfully and saved as {output_file}\")\n\nData processed successfully and saved as ./Example_singer_info.csv\n\n\n\n\n1.3 Data preprocessing\nExtract the song name from the csv’s title and generate a new CSV file containing the singer’s and song’s name.\n\nimport pandas as pd\nimport re\n\n# Load the CSV file with YouTube video data\ndf = pd.read_csv('./Example_Detailed_YouTube_Video_Data.csv', encoding='MacRoman')\n\n# Function to extract the song name from the title\ndef extract_song_name(title):\n    # Use regular expression to find text between \" - \" and \"(\"\n    match = re.search(r' - (.*?) \\(.*\\)', title)\n    if match:\n        return match.group(1) \n    else:\n        return title  \n\n# Create a new DataFrame with extracted song names\nnew_df = pd.DataFrame({\n    'Extracted Song Name': df['Title'].apply(extract_song_name)\n})\n\n# Save the new DataFrame to a CSV file in the current directory\noutput_file = './Example_extracted_song_names.csv'\nnew_df.to_csv(output_file, index=False, encoding='MacRoman')\n\nprint(f\"Song titles extracted and saved to {output_file}\")\n\nSong titles extracted and saved to ./Example_extracted_song_names.csv\n\n\n\n\ninput_file = './Example_extracted_song_names.csv'\ndf = pd.read_csv(input_file, encoding='MacRoman')\n\n# Function to remove content inside brackets (e.g., [example])\ndef remove_brackets(text):\n    return re.sub(r'\\[.*?\\]', '', text)\n\n# Apply the function to the 'Extracted Song Name' column\ndf['Extracted Song Name'] = df['Extracted Song Name'].apply(remove_brackets)\n\n\noutput_file = './Example_extracted_song_names_cleaned.csv'\ndf.to_csv(output_file, index=False)\n\nprint(f\"Processed data saved to {output_file}\")\n\nProcessed data saved to ./Example_extracted_song_names_cleaned.csv\n\n\nManually merge example_extracted_song_name_cleaned.csv with example_artist_info.csv\n\n\n\n2. Spotify Collection\n\n2.1 Acquiring Track Information\n\nimport pandas as pd\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\n\n# Set up Spotify client credentials\nclient_credentials_manager = SpotifyClientCredentials(\n    client_id='3e1596de002340b898f5d10c9aeae4ea',\n    client_secret='526fa44678974475b0f6ba5d8efd16c4'\n)\nsp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n\n# Input and output file paths\ninput_file = './Example_extracted_song_names_cleaned.csv'\noutput_file = './Example_spotify_track_info.csv'\n\n# Load the input CSV file\ndf = pd.read_csv(input_file, encoding='MacRoman')\n\n# Initialize a list to store results\nresults = []\n\n# Process each row in the DataFrame\nfor _, row in df.iterrows():\n    song = row['Extracted Song Name']\n    artist = row['singer']\n    query = f'{song} {artist}'  # Concatenate song and artist for the search query\n\n    try:\n        # Search for the track on Spotify\n        result = sp.search(q=query, limit=1, type='track')\n        tracks = result.get('tracks', {}).get('items', [])\n\n        if tracks:\n            # If a track is found, extract its details\n            track = tracks[0]\n            track_info = {\n                'Track Name': track['name'],\n                'Artist Name': track['artists'][0]['name'],\n                'Album Name': track['album']['name'],\n                'Popularity': track['popularity'],\n                'Duration (ms)': track['duration_ms'],\n                'Track ID': track['id'],\n                'Spotify URL': track['external_urls']['spotify']\n            }\n            print(f\"Found: {track_info['Track Name']} by {track_info['Artist Name']}\")\n        else:\n            # If no track is found, append placeholders\n            print(f\"Track not found: {query}\")\n            track_info = {\n                'Track Name': song,\n                'Artist Name': artist,\n                'Album Name': None,\n                'Popularity': None,\n                'Duration (ms)': None,\n                'Track ID': None,\n                'Spotify URL': None\n            }\n\n        # Append the result to the list\n        results.append(track_info)\n\n    except Exception as e:\n        # Handle exceptions during the search\n        print(f\"Error processing query '{query}': {e}\")\n        results.append({\n            'Track Name': song,\n            'Artist Name': artist,\n            'Album Name': None,\n            'Popularity': None,\n            'Duration (ms)': None,\n            'Track ID': None,\n            'Spotify URL': None\n        })\n\n# Create a DataFrame from the results\noutput_df = pd.DataFrame(results)\n\n# Save the output DataFrame to a CSV file\noutput_df.to_csv(output_file, index=False, encoding='utf-8')\n\nprint(f\"Processed data saved to: {output_file}\")\n\nFound: Learn to Fly by Foo Fighters\nFound: Everlong by Foo Fighters\nFound: The Pretender by Foo Fighters\nFound: My Hero by Foo Fighters\nFound: Best of You by Foo Fighters\nFound: Mr. Brightside by The Killers\nFound: When You Were Young by The Killers\nFound: Mr. Brightside by The Killers\nFound: Somebody Told Me by The Killers\nFound: One Empty Grave by A Sound of Thunder\nFound: Basket Case by Green Day\nFound: When I Come Around by Green Day\nFound: American Idiot by Green Day\nFound: Boulevard of Broken Dreams by Green Day\nFound: Wake Me up When September Ends by Green Day\nProcessed data saved to: ./Example_spotify_track_info.csv\n\n\n\n\n2.2 Obtaining Artist Information\n\nimport pandas as pd\nimport requests\n\n# Function to obtain an access token for Spotify API\ndef get_access_token(client_id, client_secret):\n    auth_url = 'https://accounts.spotify.com/api/token'\n    auth_data = {\n        'grant_type': 'client_credentials',\n        'client_id': client_id,\n        'client_secret': client_secret\n    }\n    response = requests.post(auth_url, data=auth_data)\n    if response.status_code == 200:\n        return response.json()['access_token']\n    else:\n        raise Exception('Failed to obtain access token')\n\n# Function to get the Spotify artist ID using the artist name\ndef get_artist_id(artist_name, access_token):\n    search_url = f'https://api.spotify.com/v1/search?q={artist_name}&type=artist&limit=1'\n    headers = {'Authorization': f'Bearer {access_token}'}\n    response = requests.get(search_url, headers=headers)\n    if response.status_code == 200:\n        search_results = response.json()\n        artists = search_results['artists']['items']\n        if artists:\n            return artists[0]['id']\n        else:\n            return None\n    else:\n        return None\n\n# Function to retrieve the artist's followers and popularity\ndef get_artist_followers(artist_id, access_token):\n    artist_url = f'https://api.spotify.com/v1/artists/{artist_id}'\n    headers = {'Authorization': f'Bearer {access_token}'}\n    response = requests.get(artist_url, headers=headers)\n    if response.status_code == 200:\n        artist_data = response.json()\n        return artist_data['followers']['total'], artist_data['popularity']\n    else:\n        return None, None\n\n# Load the input CSV file \ninput_file = './Example_singer_info.csv' \ndf = pd.read_csv(input_file)\n\n# Spotify API credentials\nclient_id = '31aba57b31344fdebf98f51375d07834' \nclient_secret = '2c4f929786784910bc9a843518785cae'  \n\n# Obtain Spotify API access token\naccess_token = get_access_token(client_id, client_secret)\n\n# Initialize lists to store followers and popularity data\nfollowers_list = []\npopularity_list = []\n\n# Process each artist in the DataFrame\nfor artist_name in df['artist']:\n    artist_id = get_artist_id(artist_name, access_token)\n    if artist_id:\n        followers, popularity = get_artist_followers(artist_id, access_token)\n        followers_list.append(followers)\n        popularity_list.append(popularity)\n    else:\n        # If the artist is not found, append None\n        followers_list.append(None)\n        popularity_list.append(None)\n\n# Add followers and popularity data to the DataFrame\ndf['Followers'] = followers_list\ndf['Popularity'] = popularity_list\n\n# Save the updated DataFrame to a new CSV file\noutput_file = './Example_artist_data_with_followers_and_popularity.csv'  \ndf.to_csv(output_file, index=False)\n\nprint(f\"Processed data saved to: {output_file}\")\n\nProcessed data saved to: ./Example_artist_data_with_followers_and_popularity.csv\n\n\n\n\n\n3. Data integration and cleansing\nFirst manually merge Spotify and YouTube csv. Then using Spotify and YouTube artists as the matching key, match to verify artist match, if not, then delete the mismatched rows.\n\nimport pandas as pd\n\ninput_file = './Example_Detailed_YouTube_Video_Data.csv' \ndf = pd.read_csv(input_file, encoding='MacRoman')\n\n# Filter rows where 'Artist Name' matches 'Artist'\ndf_cleaned = df[df['Artist Name'] == df['artist']]\n\n# Save the cleaned DataFrame to a new CSV file\noutput_file = './Example_spotify_youtube.csv'  \ndf_cleaned.to_csv(output_file, index=False)\nprint(f\"Cleaned data saved to: {output_file}\")\n\nCleaned data saved to: ./Example_spotify_youtube.csv\n\n\nThen we completed all the data collection steps!"
  },
  {
    "objectID": "technical-details/data-collection/main.html#challenges",
    "href": "technical-details/data-collection/main.html#challenges",
    "title": "Data Collection",
    "section": "Challenges",
    "text": "Challenges\nI encountered several technical challenges during the data collection process. First, when selecting artists, some singers may have turned off the music video comment section on YouTube, which caused me to have to re-search for other artists. In addition, Spotify’s search engine is not always accurate, and even if I provide the song title and artist name, I still may not be able to find the target song. This is an issue for which I haven’t found a perfect solution yet, considering that I may need to further clean up the song titles to make them more accurate in order to improve the accuracy of my searches. Although the original plan was to acquire 500 pieces of data, in the end only 376 pieces of data were successfully acquired. So we may have to make real-time adjustments when it comes to data collection.\nAnother technical difficulty is that the YouTube API sometimes returns irrelevant data, so it is important to carefully review and filter out the results that meet the requirements. In addition, the use of matching key to find relevant data in the YouTube and Spotify APIs is very important to improve the matching accuracy and reliability of the data."
  },
  {
    "objectID": "technical-details/data-collection/main.html#benchmarks",
    "href": "technical-details/data-collection/main.html#benchmarks",
    "title": "Data Collection",
    "section": "Benchmarks",
    "text": "Benchmarks\nThe challenge of the project was to optimize the use of the API and reduce the interference of irrelevant data. While most projects can collect large amounts of data through APIs, accurately finding the target data is still a common difficulty. Therefore, how to improve the quality of data through data cleansing and precise matching algorithms remains a direction for further research and improvement in the future."
  },
  {
    "objectID": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "title": "Data Collection",
    "section": "Conclusion and Future Steps",
    "text": "Conclusion and Future Steps\nWe can conclude that firstly, during the data collection process, despite the convenience provided by the use of APIs, the issues of exact matching and removal of irrelevant data still need to be addressed. Second, Spotify’s search engine may need further optimization or more detailed preprocessing of the input data to improve the accuracy of the matches.\nFuture steps could focus on optimizing the process of data collection and cleansing. For example, for the Spotify search problem, attempts could be made to improve the method of standardization of song titles or validation in combination with other data sources. On the other hand, further research can be done to improve the efficiency of data filtering in the YouTube API to ensure that the data collected is more in line with expectations and less interfered by irrelevant information. These improvements will help enhance the efficiency and quality of data collection and lay a solid foundation for subsequent analysis."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html",
    "href": "technical-details/supervised-learning/main.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results.\n\n\n\n\nNormalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling.\n\n\n\n\n\nModel Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used\n\n\n\n\n\nSplit Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset.\n\n\n\n\n\nBinary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc.\n\n\n\n\n\nModel Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots).\n\n\n\n\n\nResult Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/main.html#suggested-page-structure",
    "title": "Supervised Learning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#what-to-address",
    "href": "technical-details/supervised-learning/main.html#what-to-address",
    "title": "Supervised Learning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#data-preprocessing",
    "href": "technical-details/supervised-learning/main.html#data-preprocessing",
    "title": "Supervised Learning",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-selection",
    "href": "technical-details/supervised-learning/main.html#model-selection",
    "title": "Supervised Learning",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/main.html#training-and-testing-strategy",
    "title": "Supervised Learning",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/main.html#model-evaluation-metrics",
    "title": "Supervised Learning",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#results",
    "href": "technical-details/supervised-learning/main.html#results",
    "title": "Supervised Learning",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#discussion",
    "href": "technical-details/supervised-learning/main.html#discussion",
    "title": "Supervised Learning",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/data-collection/basic process.html",
    "href": "technical-details/data-collection/basic process.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "find youtube mv related information\n\nfrom googleapiclient.discovery import build\nimport pandas as pd\nfrom datetime import datetime\nimport dateutil.parser\n\n# API Key\napi_key = \"AIzaSyDtKE-4QZj6EA-rwG7cj5gMJxdt4Fe14Nw\"\n\n# Initialize YouTube API client\nyoutube = build('youtube', 'v3', developerKey=api_key)\n\n# List to store data\nall_data = []\n\n# Read song data and fetch YouTube statistics\nwith open('find.txt', 'r') as file:\n    for line in file:\n        artist = line.strip()\n        query = f\"{artist} official music video\"\n\n        # Search request for the query\n        search_request = youtube.search().list(\n            part=\"snippet\",\n            q=query,\n            maxResults=5,\n            type=\"video\",\n            order='relevance'\n        )\n        search_response = search_request.execute()\n\n        for item in search_response['items']:\n            video_id = item['id']['videoId']\n            video_request = youtube.videos().list(\n                part=\"snippet,contentDetails,statistics\",\n                id=video_id\n            )\n            video_response = video_request.execute()\n\n            for video in video_response['items']:\n                snippet = video['snippet']\n                content_details = video['contentDetails']\n                statistics = video['statistics']\n\n                # Calculate days since video was published\n                published_at = dateutil.parser.parse(snippet['publishedAt'])\n                days_since_published = (datetime.now(published_at.tzinfo) - published_at).days\n\n                # Fetch channel details for subscriber count\n                channel_response = youtube.channels().list(\n                    part='statistics',\n                    id=snippet['channelId']\n                ).execute()\n                subscriber_count = channel_response['items'][0]['statistics'].get('subscriberCount', '0')\n\n                # Fetch comments\n                comments_request = youtube.commentThreads().list(\n                    part='snippet',\n                    videoId=video_id,\n                    order='relevance',\n                    maxResults=10\n                )\n                comments_response = comments_request.execute()\n\n                comments = [comment['snippet']['topLevelComment']['snippet']['textDisplay']\n                            for comment in comments_response.get('items', [])]\n\n                # Store all data in a dictionary\n                video_data = {\n                    'Video ID': video_id,\n                    'Title': snippet['title'],\n                    'Description': snippet['description'],\n                    'Published At': snippet['publishedAt'],\n                    'Days Since Published': days_since_published,\n                    'View Count': statistics.get('viewCount', '0'),\n                    'Like Count': statistics.get('likeCount', '0'),\n                    'Comment Count': statistics.get('commentCount', '0'),\n                    'Comments': comments,\n                    'Subscriber Count': subscriber_count,\n                    'Category ID': snippet['categoryId'],\n                    'Definition': content_details['definition'],\n                    'Duration': content_details['duration']\n                }\n\n                all_data.append(video_data)\n\n# Convert the list to a DataFrame\nfinal_df = pd.DataFrame(all_data)\n\n# Optionally save the DataFrame to a CSV file\nfinal_df.to_csv('Detailed_YouTube_Video_Data——1.csv', index=False)\n\n# Display the DataFrame\nprint(final_df.head())\n\nGenerate the csv and repeat the row five times\n\nimport pandas as pd\n\n# Input and output file paths (relative paths)\ninput_file = './artist_data_with_followers_and_popularity.csv'  \noutput_file = './singer_info.csv'  \n\n# Load the input CSV file\ndf = pd.read_csv(input_file)\n\n# Create an empty DataFrame to store repeated rows\nrepeated_df = pd.DataFrame()\n\n# Repeat each row 5 times and append it to the new DataFrame\nfor i in range(len(df)):\n    repeated_df = pd.concat([repeated_df, pd.DataFrame([df.iloc[i]] * 5)], ignore_index=True)\n\n# Save the processed DataFrame to a new CSV file\nrepeated_df.to_csv(output_file, index=False)\n\nprint(f\"Data processed successfully and saved as {output_file}\")\n\nFind the song title from YouTube title\n\nimport pandas as pd\nimport re\n\n# Load the CSV file with YouTube video data\ndf = pd.read_csv('./youTube_Video_Data_all.csv', encoding='MacRoman')\n\n# Function to extract the song name from the title\ndef extract_song_name(title):\n    # Use regular expression to find text between \" - \" and \"(\"\n    match = re.search(r' - (.*?) \\(.*\\)', title)\n    if match:\n        return match.group(1) \n    else:\n        return title  \n\n# Create a new DataFrame with extracted song names\nnew_df = pd.DataFrame({\n    'Extracted Song Name': df['Title'].apply(extract_song_name)\n})\n\n# Save the new DataFrame to a CSV file in the current directory\noutput_file = './extracted_song_names_all.csv'\nnew_df.to_csv(output_file, index=False, encoding='MacRoman')\n\nprint(f\"Song titles extracted and saved to {output_file}\")\n\nDelete the content of [] in the first column\n\nimport pandas as pd\nimport re\n\n# Load the input CSV file (relative path)\ninput_file = './extracted_song_names_all.csv'\ndf = pd.read_csv(input_file, encoding='MacRoman')\n\n# Function to remove content inside brackets (e.g., [example])\ndef remove_brackets(text):\n    return re.sub(r'\\[.*?\\]', '', text)\n\n# Apply the function to the 'Extracted Song Name' column\ndf['Extracted Song Name'] = df['Extracted Song Name'].apply(remove_brackets)\n\n\noutput_file = './extracted_song_names_cleaned.csv'\ndf.to_csv(output_file, index=False)\n\nprint(f\"Processed data saved to {output_file}\")\n\nfind csv composite code\n\nimport chardet\n\n\nfile_path = './youTube_Video_Data_all.csv'  # Relative path to the file\nwith open(file_path, 'rb') as f:\n    result = chardet.detect(f.read())  # Detect the encoding of the file\n\n\nFind information about track- using spotify api\n\nimport pandas as pd\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\n\n# Set up Spotify client credentials\nclient_credentials_manager = SpotifyClientCredentials(\n    client_id='31aba57b31344fdebf98f51375d07834',  \n    client_secret='2c4f929786784910bc9a843518785cae')\nsp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n\n\ninput_file = './extracted_song_names_cleaned.csv'  \ndf = pd.read_csv(input_file, encoding='MacRoman')\n\n# Initialize an empty list to store results\nresults = []\n\n# Process each row in the DataFrame\nfor _, row in df.iterrows():\n    song = row['Extracted Song Name'] \n    artist = row['singer'] \n    query = f\"{song} {artist}\" \n\n    try:\n        result = sp.search(q=query, limit=1, type='track')\n        track = result['tracks']['items']  # Extract track information\n\n        if track:\n            # If a track is found, extract relevant information\n            track_info = {\n                'Track Name': track[0]['name'],\n                'Artist Name': track[0]['artists'][0]['name'],\n                'Album Name': track[0]['album']['name'],\n                'Popularity': track[0]['popularity'],\n                'Duration (ms)': track[0]['duration_ms'],\n                'Track ID': track[0]['id'],\n                'Spotify URL': track[0]['external_urls']['spotify']\n            }\n            print(f\"Found: {track_info['Track Name']} by {track_info['Artist Name']}\")\n            results.append(track_info)\n        else:\n            # If no track is found, append placeholders\n            print(f\"Track not found: {query}\")\n            results.append({'Track Name': song, 'Artist Name': artist, 'Album Name': None,\n                            'Popularity': None, 'Duration (ms)': None, 'Track ID': None, 'Spotify URL': None})\n    except Exception as e:\n        # Handle exceptions during the search\n        print(f\"Search failed for: {query}, error: {e}\")\n        results.append({'Track Name': song, 'Artist Name': artist, 'Album Name': None,\n                        'Popularity': None, 'Duration (ms)': None, 'Track ID': None, 'Spotify URL': None})\n\n# Create a DataFrame from the results\noutput_df = pd.DataFrame(results)\n\n# Save the output DataFrame to a CSV file using a relative path\noutput_file = './spotify_track_info_all—v1.csv'  # Relative path to the output file\noutput_df.to_csv(output_file, index=False, encoding='utf-8')\n\nprint(f\"Processed data saved to: {output_file}\")\n\n正在搜索: Learn To Fly Foo Fighters\n找到歌曲: Learn to Fly by Foo Fighters\n正在搜索: Everlong Foo Fighters\n找到歌曲: Everlong by Foo Fighters\n正在搜索: Foo Fighters - The Pretender Foo Fighters\n找到歌曲: The Pretender by Foo Fighters\n正在搜索: My Hero Foo Fighters\n找到歌曲: My Hero by Foo Fighters\n正在搜索: Best Of You Foo Fighters\n找到歌曲: Best of You by Foo Fighters\n正在搜索: Mr. Brightside The Killers\n找到歌曲: Mr. Brightside by The Killers\n正在搜索: When You Were Young The Killers\n找到歌曲: When You Were Young by The Killers\n正在搜索: All These Things That I've Done The Killers\n找到歌曲: All These Things That I've Done by The Killers\n正在搜索: Somebody Told Me The Killers\n找到歌曲: Somebody Told Me by The Killers\n正在搜索: Human The Killers\n找到歌曲: Human by The Killers\n正在搜索: Basket Case  Green Day\n找到歌曲: Basket Case by Green Day\n正在搜索: When I Come Around  Green Day\n找到歌曲: When I Come Around by Green Day\n正在搜索: Green Day - American Idiot   Green Day\n找到歌曲: American Idiot by Green Day\n正在搜索: Green Day - Boulevard Of Broken Dreams   Green Day\n找到歌曲: Boulevard of Broken Dreams by Green Day\n正在搜索: Green Day - Wake Me Up When September Ends   Green Day\n找到歌曲: Wake Me up When September Ends by Green Day\n正在搜索: The Emptiness Machine (Official Music Video) - Linkin Park Linkin Park\n找到歌曲: H! Vltg3 (Evidence Reanimation) [feat. Pharoahe Monch and DJ Babu] by Linkin Park\n正在搜索: In The End  - Linkin Park Linkin Park\n找到歌曲: In the End by Linkin Park\n正在搜索: Two Faced (Official Music Video) - Linkin Park Linkin Park\n找到歌曲: It's Beginning to Look a Lot Like Christmas (with Mitchell Ayres & His Orchestra) by Perry Como\n正在搜索: Numb (Official Music Video)  _ Linkin Park Linkin Park\n找到歌曲: H! Vltg3 (Evidence Reanimation) [feat. Pharoahe Monch and DJ Babu] by Linkin Park\n正在搜索: Crawling  - Linkin Park Linkin Park\n找到歌曲: Crawling by Linkin Park\n正在搜索: Californication Red Hot Chili Peppers\n找到歌曲: Californication by Red Hot Chili Peppers\n正在搜索: Red Hot Chili Peppers - Can't Stop  Red Hot Chili Peppers\n找到歌曲: Can't Stop by Red Hot Chili Peppers\n正在搜索: Red Hot Chili Peppers - Under The Bridge  Red Hot Chili Peppers\n找到歌曲: Under the Bridge by Red Hot Chili Peppers\n正在搜索: Red Hot Chili Peppers - Scar Tissue   Red Hot Chili Peppers\n找到歌曲: Scar Tissue by Red Hot Chili Peppers\n正在搜索: Red Hot Chili Peppers - Tell Me Baby  Red Hot Chili Peppers\n找到歌曲: Tell Me Baby by Red Hot Chili Peppers\n正在搜索: Smells Like Teen Spirit Nirvana\n找到歌曲: Smells Like Teen Spirit by Nirvana\n正在搜索: Heart-Shaped Box Nirvana\n找到歌曲: Heart-Shaped Box by Nirvana\n正在搜索: In Bloom Nirvana\n找到歌曲: In Bloom by Nirvana\n正在搜索: Come As You Are Nirvana\n找到歌曲: Come As You Are by Nirvana\n正在搜索: Sliver Nirvana\n找到歌曲: Sliver by Nirvana\n正在搜索: Muse - Uprising  Muse\n找到歌曲: Uprising by Muse\n正在搜索: √è√ü√é√ò (Jimin) 'Who' Official MV Muse\n找到歌曲: Lucky by RuPaul\n正在搜索: Muse - Starlight  Muse\n找到歌曲: Starlight by Muse\n正在搜索: Muse - Hysteria  Muse\n找到歌曲: Hysteria by Muse\n正在搜索: Muse - Madness Muse\n找到歌曲: Madness by Muse\n正在搜索: Metallica: One (Official Music Video) Metallica\n找到歌曲: Draw Wi Out by Shatta Wale\n正在搜索: Metallica: Enter Sandman (Official Music Video) Metallica\n找到歌曲: So Much Darkness by Disturbed\n正在搜索: Metallica: Nothing Else Matters (Official Music Video) Metallica\n找到歌曲: POP by Il Cremlino\n正在搜索: Metallica: The Memory Remains (Official Music Video) Metallica\n找到歌曲: This Is My Demo by Sway\n正在搜索: Metallica: Until It Sleeps (Official Music Video) Metallica\n找到歌曲: Sarah Lawrence College Song by Sun Kil Moon\n正在搜索: Why'd You Only Call Me When You're High? Arctic Monkeys\n找到歌曲: Why'd You Only Call Me When You're High? by Arctic Monkeys\n正在搜索: Arabella Arctic Monkeys\n找到歌曲: Arabella by Arctic Monkeys\n正在搜索: Do I Wanna Know? Arctic Monkeys\n找到歌曲: Do I Wanna Know? by Arctic Monkeys\n正在搜索: Fluorescent Adolescent Arctic Monkeys\n找到歌曲: Fluorescent Adolescent by Arctic Monkeys\n正在搜索: Snap Out Of It Arctic Monkeys\n找到歌曲: Snap Out Of It by Arctic Monkeys\n正在搜索: With Or Without You U2\n找到歌曲: With Or Without You by U2\n正在搜索: Where The Streets Have No Name U2\n找到歌曲: Where The Streets Have No Name - Remastered by U2\n正在搜索: I Still Haven't Found What I'm Looking For U2\n找到歌曲: I Still Haven't Found What I'm Looking For by U2\n正在搜索: Sweetest Thing U2\n找到歌曲: Sweetest Thing by U2\n正在搜索: One U2\n找到歌曲: One by U2\n正在搜索: Angry The Rolling Stones\n找到歌曲: Angry by The Rolling Stones\n正在搜索: Sympathy For The Devil The Rolling Stones\n找到歌曲: Sympathy For The Devil by The Rolling Stones\n正在搜索: You Can__ Always Get What You Want The Rolling Stones\n找到歌曲: You Can't Always Get What You Want by The Rolling Stones\n正在搜索: Jumpin' Jack Flash The Rolling Stones\n找到歌曲: Jumpin' Jack Flash by The Rolling Stones\n正在搜索: The Rolling Stones - Waiting On A Friend - OFFICIAL PROMO The Rolling Stones\n找到歌曲: Waiting On A Friend - Remastered 2009 by The Rolling Stones\n正在搜索: Queen _ Bohemian Rhapsody (Official Video Remastered) Queen\n找到歌曲: Pink Pony Club by Chappell Roan\n正在搜索: I Want To Break Free Queen\n找到歌曲: I Want To Break Free by Queen\n正在搜索: Don't Stop Me Now Queen\n找到歌曲: Don't Stop Me Now by Queen\n正在搜索: Ava Max - Kings & Queens  Queen\n找到歌曲: Kings & Queens by Ava Max\n正在搜索: We Will Rock You Queen\n找到歌曲: We Will Rock You by Queen\n正在搜索: Whole Lotta Love Led Zeppelin\n找到歌曲: Whole Lotta Love - 1990 Remaster by Led Zeppelin\n正在搜索: Over the Hills and Far Away Led Zeppelin\n找到歌曲: Over the Hills and Far Away - Remaster by Led Zeppelin\n正在搜索: Immigrant Song Led Zeppelin\n找到歌曲: Immigrant Song - Remaster by Led Zeppelin\n正在搜索: Led Zeppelin   When the Levee Breaks (music video) Led Zeppelin\n找到歌曲: LET ME TALK by Lotus\n正在搜索: Kashmir Led Zeppelin\n找到歌曲: Kashmir - Remaster by Led Zeppelin\n正在搜索: Sweet Child O' Mine Guns N' Roses\n找到歌曲: Sweet Child O' Mine by Guns N' Roses\n正在搜索: Guns N' Roses - November Rain Guns N' Roses\n找到歌曲: November Rain by Guns N' Roses\n正在搜索: Paradise City Guns N' Roses\n找到歌曲: Paradise City by Guns N' Roses\n正在搜索: Guns N' Roses - Don't Cry Guns N' Roses\n找到歌曲: Don't Cry (Original) by Guns N' Roses\n正在搜索: Guns N' Roses - Welcome To The Jungle Guns N' Roses\n找到歌曲: Welcome To The Jungle by Guns N' Roses\n正在搜索: Cryin' Aerosmith\n找到歌曲: Cryin' by Aerosmith\n正在搜索: Aerosmith - Crazy Official Music Video Aerosmith\n找到歌曲: tv off (feat. lefty gunplay) by Kendrick Lamar\n正在搜索: Dude Aerosmith\n找到歌曲: Dude (Looks Like A Lady) by Aerosmith\n正在搜索: Rag Doll Aerosmith\n找到歌曲: Rag Doll by Aerosmith\n正在搜索: Amazing Aerosmith\n找到歌曲: Amazing by Aerosmith\n正在搜索: ALL MY LOVE Coldplay\n找到歌曲: ALL MY LOVE by Coldplay\n正在搜索: The Scientist Coldplay\n找到歌曲: The Scientist by Coldplay\n正在搜索: Yellow Coldplay\n找到歌曲: Yellow by Coldplay\n正在搜索: Fix You Coldplay\n找到歌曲: Fix You by Coldplay\n正在搜索: Adventure Of A Lifetime Coldplay\n找到歌曲: Adventure of a Lifetime by Coldplay\n正在搜索: The Beatles - Strawberry Fields Forever The Beatles\n找到歌曲: Strawberry Fields Forever - Remastered 2009 by The Beatles\n正在搜索: The Beatles - Hello, Goodbye The Beatles\n找到歌曲: Hello, Goodbye - Remastered 2009 by The Beatles\n正在搜索: Lucy In The Sky With Diamonds The Beatles\n找到歌曲: Lucy In The Sky With Diamonds - Remastered 2009 by The Beatles\n正在搜索: The Beatles - Help! The Beatles\n找到歌曲: Help! - Remastered 2009 by The Beatles\n正在搜索: And I Love Her The Beatles\n找到歌曲: And I Love Her - Remastered 2009 by The Beatles\n正在搜索: Money Pink Floyd\n找到歌曲: Money by Pink Floyd\n正在搜索: Learning To Fly Pink Floyd\n找到歌曲: Learning to Fly by Pink Floyd\n正在搜索: High Hopes Pink Floyd\n找到歌曲: High Hopes by Pink Floyd\n正在搜索: Another Brick In The Wall, Part Two Pink Floyd\n找到歌曲: PLAYBOY (feat. MV Killa, Yung Snapp, Lele Blade) by SLF\n正在搜索: Welcome to the Machine Pink Floyd\n找到歌曲: Welcome to the Machine by Pink Floyd\n正在搜索: Even Flow Pearl Jam\n找到歌曲: Even Flow by Pearl Jam\n正在搜索: Alive Pearl Jam\n找到歌曲: Alive by Pearl Jam\n正在搜索: Jeremy Pearl Jam\n找到歌曲: Jeremy by Pearl Jam\n正在搜索: Do the Evolution Pearl Jam\n找到歌曲: Do the Evolution by Pearl Jam\n正在搜索: Black Pearl Jam\n找到歌曲: Black by Pearl Jam\n正在搜索: Wonderwall Oasis\n找到歌曲: Wonderwall by Oasis\n正在搜索: Oasis - Don__ Look Back In Anger Oasis\n找到歌曲: Don't Look Back In Anger - Remastered by Oasis\n正在搜索: Supersonic Oasis\n找到歌曲: Supersonic by Oasis\n正在搜索: D'You Know What I Mean? Oasis\n找到歌曲: D'You Know What I Mean? by Oasis\n正在搜索: Champagne Supernova Oasis\n找到歌曲: Champagne Supernova by Oasis\n正在搜索: Kendrick Lamar - Not Like Us Kendrick Lamar\n找到歌曲: Not Like Us by Kendrick Lamar\n正在搜索: squabble up Kendrick Lamar\n找到歌曲: squabble up by Kendrick Lamar\n正在搜索: GNX Kendrick Lamar\n找到歌曲: gnx (feat. hitta j3, youngthreat, peysoh) by Kendrick Lamar\n正在搜索: Alright Kendrick Lamar\n找到歌曲: Alright by Kendrick Lamar\n正在搜索: Kendrick Lamar - N95 Kendrick Lamar\n找到歌曲: N95 by Kendrick Lamar\n正在搜索: She Knows J. Cole\n找到歌曲: She Knows (feat. Amber Coffman & Cults) by J. Cole\n正在搜索: a p p l y i n g . p r e s s u r e J. Cole\n找到歌曲: a p p l y i n g . p r e s s u r e by J. Cole\n正在搜索: a m a r i J. Cole\n找到歌曲: a m a r i by J. Cole\n正在搜索: J. Cole - MIDDLE CHILD J. Cole\n找到歌曲: MIDDLE CHILD by J. Cole\n正在搜索: Neighbors J. Cole\n找到歌曲: Neighbors by J. Cole\n正在搜索: Laugh Now Cry Later Drake\n找到歌曲: Laugh Now Cry Later (feat. Lil Durk) by Drake\n正在搜索: Rich Baby Daddy Drake\n找到歌曲: Rich Baby Daddy (feat. Sexyy Red & SZA) by Drake\n正在搜索: Modo Capone Drake\n找到歌曲: Modo Capone (with Drake & Fuerza Regida) by Chino Pacas\n正在搜索: Drake - Jumbotron Shit Poppin Drake\n找到歌曲: Jumbotron Shit Poppin by Drake\n正在搜索: DRAKE - FAMILY MATTERS Drake\n找到歌曲: Family Matters by Drake\n正在搜索: Kanye West - Famous Kanye West\n找到歌曲: Famous by Kanye West\n正在搜索: ¬¨‚Ä¢$, North West, Chicago West, Yuno Miles - BOMB Kanye West\n找到歌曲: squabble up by Kendrick Lamar\n正在搜索: Kanye West & Lil Pump - I Love It feat. Adele Givens  Kanye West\n找到歌曲: I Love It - 8-Bit Lil Pump & Kanye West feat. Adele Givens Emulation by 8-Bit Arcade\n正在搜索: Closed On Sunday Kanye West\n找到歌曲: Closed On Sunday by Kanye West\n正在搜索: Kanye West - All Falls Down ft. Syleena Johnson Kanye West\n找到歌曲: squabble up by Kendrick Lamar\n正在搜索: Eminem - Houdini  Eminem\n找到歌曲: Houdini by Eminem\n正在搜索: Temporary Eminem\n找到歌曲: Temporary by Eminem\n正在搜索: Without Me Eminem\n找到歌曲: Without Me by Eminem\n正在搜索: Somebody Save Me Eminem\n找到歌曲: Somebody Save Me by Eminem\n正在搜索: Just Lose It Eminem\n找到歌曲: Just Lose It by Eminem\n正在搜索: I KNOW ? Travis Scott\n找到歌曲: I KNOW ? by Travis Scott\n正在搜索: HIGHEST IN THE ROOM Travis Scott\n找到歌曲: HIGHEST IN THE ROOM by Travis Scott\n正在搜索: Travis Scott - FE!N ft. Playboi Carti Travis Scott\n找到歌曲: GALAXY by Lil Burp\n正在搜索: OUT WEST Travis Scott\n找到歌曲: OUT WEST (feat. Young Thug) by JACKBOYS\n正在搜索: Travis Scott - Drugs You Should Try It Travis Scott\n找到歌曲: Drugs You Should Try It by Travis Scott\n正在搜索: Super Freaky Girl Nicki Minaj\n找到歌曲: Super Freaky Girl by Nicki Minaj\n正在搜索: Princess Diana Nicki Minaj\n找到歌曲: Princess Diana (with Nicki Minaj) by Ice Spice\n正在搜索: 6ix9ine & Nicki Minaj  Nicki Minaj\n找到歌曲: FEFE (feat. Nicki Minaj & Murda Beatz) by 6ix9ine\n正在搜索: Nicki Minaj - Anaconda Nicki Minaj\n找到歌曲: Anaconda by Nicki Minaj\n正在搜索: Nicki Minaj & Ice Spice _ Barbie World (with Aqua)  Nicki Minaj\n找到歌曲: Barbie World (with Aqua) [From Barbie The Album] by Nicki Minaj\n正在搜索: Cardi B - WAP feat. Megan Thee Stallion  Cardi B\n找到歌曲: WAP (feat. Megan Thee Stallion) by Cardi B\n正在搜索: Like What Cardi B\n找到歌曲: Like What (Freestyle) by Cardi B\n正在搜索: Enough Cardi B\n找到歌曲: Enough (Miami) by Cardi B\n正在搜索: Cardi B - Money  Cardi B\n找到歌曲: Money by Cardi B\n正在搜索: Bongos Cardi B\n找到歌曲: Bongos (feat. Megan Thee Stallion) by Cardi B\n正在搜索: JAY-Z - Song Cry Jay-Z\n找到歌曲: Song Cry by JAY-Z\n正在搜索: JAY-Z - The Story of O.J. Jay-Z\n找到歌曲: The Story of O.J. by JAY-Z\n正在搜索: What We Do Jay-Z\n找到歌曲: What We Do by Freeway\n正在搜索: JAY-Z - Empire State Of Mind ft. Alicia Keys Jay-Z\n找到歌曲: Empire State of Mind - Piano rendition of Jay-Z ft. Alicia Keys by The Lullabeats\n正在搜索: JAY Z, Kanye West - Otis ft. Otis Redding Jay-Z\n找到歌曲: Otis (Made famous by Kanye West & Jay.z ft Otis Redding) by Karaoke Hits Band\n正在搜索: 6 Foot 7 Foot ft. Cory Gunz Lil Wayne\n找到歌曲: Rebirth by Forever Tmb Tay\n正在搜索: Lollipop Lil Wayne\n找到歌曲: Lollipop by Lil Wayne\n正在搜索: Mirror ft. Bruno Mars Lil Wayne\n找到歌曲: Mirrors (made popular by Lil' Wayne ft. Bruno Mars) [backing version] by Party Tyme\n正在搜索: John ft. Rick Ross Lil Wayne\n找到歌曲: Stay Solid by Black Jax\n正在搜索: Mama Mia Lil Wayne\n找到歌曲: Mama Mia by Lil Wayne\n正在搜索: Drop It Like It's Hot Snoop Dogg\n找到歌曲: Drop It Like It's Hot by Snoop Dogg\n正在搜索: Snoop Dogg - Gin And Juice Snoop Dogg\n找到歌曲: Gin and Juice by Snoop Dogg\n正在搜索: Dr. Dre - Still D.R.E. ft. Snoop Dogg Snoop Dogg\n找到歌曲: Still D.R.E. (Instrumental Version) by Breakbeat Kings\n正在搜索: Who Am I Snoop Dogg\n找到歌曲: Who Am I (What’s My Name)? by Snoop Dogg\n正在搜索: The Next Episode Snoop Dogg\n找到歌曲: The Next Episode by Dr. Dre\n正在搜索: In Da Club 50 Cent\n找到歌曲: In Da Club by 50 Cent\n正在搜索: Candy Shop 50 Cent\n找到歌曲: Candy Shop by 50 Cent\n正在搜索: Hate It Or Love It 50 Cent\n找到歌曲: Hate It Or Love It by The Game\n正在搜索: Wanksta 50 Cent\n找到歌曲: Wanksta by 50 Cent\n正在搜索: Definition Of Sexy by 50 Cent (Official Music Video) | 50 Cent Music 50 Cent\n找到歌曲: Disco Inferno by 50 Cent\n正在搜索: It Ain't Hard to Tell Nas\n找到歌曲: It Ain't Hard to Tell by Nas\n正在搜索: Nas Is Like Nas\n找到歌曲: Nas Is Like by Nas\n正在搜索: The World Is Yours Nas\n找到歌曲: The World Is Yours by Nas\n正在搜索: If I Ruled the World Nas\n找到歌曲: If I Ruled the World (Imagine That) (feat. Lauryn Hill) by Nas\n正在搜索: I Can Nas\n找到歌曲: I Can by Nas\n正在搜索: TOO FAST Future\n找到歌曲: TOO FAST by Future\n正在搜索: SKI Future\n找到歌曲: SKI by Future\n正在搜索: Relationship Future\n找到歌曲: Relationship (feat. Future) by Young Thug\n正在搜索: BRAZZIER Future\n找到歌曲: BRAZZIER by Future\n正在搜索: Maybach feat. Future Future\n找到歌曲: Maybach (feat. Future) by 42 Dugg\n正在搜索: Big Boy Meek Mill\n找到歌曲: Big Boy by Meek Mill\n正在搜索: Times Like This Meek Mill\n找到歌曲: Times Like This by Meek Mill\n正在搜索: Whatever I Want Meek Mill\n找到歌曲: Whatever I Want by Meek Mill\n正在搜索: Early Mornings Meek Mill\n找到歌曲: B12 by Lil Durk\n正在搜索: CYBER TRUCK Meek Mill\n找到歌曲: CYBER TRUCK by Meek Mill\n正在搜索: Hit 'Em Up Tupac\n找到歌曲: Hit 'Em Up - Single Version by 2Pac\n正在搜索: 2Pac - I Get Around Tupac\n找到歌曲: I Get Around by 2Pac\n正在搜索: 2 Of Amerikaz Most Wanted Tupac\n找到歌曲: 2 Of Amerikaz Most Wanted (ft. Snoop Doggy Dogg) by 2Pac\n正在搜索: So Many Tears Tupac\n找到歌曲: So Many Tears by 2Pac\n正在搜索: Do For Love Tupac\n找到歌曲: Do For Love by 2Pac\n正在搜索: Big Poppa The Notorious B.I.G.\n找到歌曲: Big Poppa by The Notorious B.I.G.\n正在搜索: Notorious B.I.G. The Notorious B.I.G.\n找到歌曲: Big Poppa - 2005 Remaster by The Notorious B.I.G.\n正在搜索: Hypnotize The Notorious B.I.G.\n找到歌曲: Hypnotize by The Notorious B.I.G.\n正在搜索: Juicy The Notorious B.I.G.\n找到歌曲: Juicy by The Notorious B.I.G.\n正在搜索: Big Poppa  The Notorious B.I.G.\n找到歌曲: Big Poppa - 2005 Remaster by The Notorious B.I.G.\n正在搜索: I Had Some Help Post Malone\n找到歌曲: I Had Some Help (Feat. Morgan Wallen) by Post Malone\n正在搜索: Fortnight Post Malone\n找到歌曲: Fortnight (feat. Post Malone) by Taylor Swift\n正在搜索: Guy For That Post Malone\n找到歌曲: Guy For That (Feat. Luke Combs) by Post Malone\n正在搜索: Circles Post Malone\n找到歌曲: Circles by Post Malone\n正在搜索: Chemical Post Malone\n找到歌曲: Chemical by Post Malone\n正在搜索: Tailor Swif A$AP Rocky\n找到歌曲: Tailor Swif by A$AP Rocky\n正在搜索: L$D A$AP Rocky\n找到歌曲: L$D by A$AP Rocky\n正在搜索: A$AP Forever A$AP Rocky\n找到歌曲: A$AP Forever REMIX (feat. Moby, T.I. & Kid Cudi) by A$AP Rocky\n正在搜索: HIGHJACK A$AP Rocky\n找到歌曲: HIGHJACK by A$AP Rocky\n正在搜索: Fashion Killa A$AP Rocky\n找到歌曲: Fashion Killa by A$AP Rocky\n正在搜索: THOUGHT I WAS DEAD Tyler, The Creator\n找到歌曲: Thought I Was Dead (feat. ScHoolboy Q & Santigold) by Tyler, The Creator\n正在搜索: ST. CHROMA Tyler, The Creator\n找到歌曲: VIRGINIA Boy (feat. Tyler, The Creator) - Remix by Pharrell Williams\n正在搜索: SEE YOU AGAIN featuring Kali Uchis Tyler, The Creator\n找到歌曲: Sticky (feat. GloRilla, Sexyy Red & Lil Wayne) by Tyler, The Creator\n正在搜索: Hodgy, Domo Genesis And Tyler, The Creator- Rella Tyler, The Creator\n找到歌曲: Rella (feat. Hodgy, Domo Genesis & Tyler, The Creator) by Odd Future\n正在搜索: NOID Tyler, The Creator\n找到歌曲: Noid by Tyler, The Creator\n正在搜索: I Can Do It With A Broken Heart Taylor Swift\n找到歌曲: I Can Do It With a Broken Heart by Taylor Swift\n正在搜索: Fortnight Taylor Swift\n找到歌曲: Fortnight (feat. Post Malone) by Taylor Swift\n正在搜索: Karma Taylor Swift\n找到歌曲: Karma by Taylor Swift\n正在搜索: Lavender Haze Taylor Swift\n找到歌曲: Lavender Haze by Taylor Swift\n正在搜索: The Man Taylor Swift\n找到歌曲: The Man by Taylor Swift\n正在搜索: Perfect Ed Sheeran\n找到歌曲: Perfect by Ed Sheeran\n正在搜索: Thinking Out Loud Ed Sheeran\n找到歌曲: Thinking out Loud by Ed Sheeran\n正在搜索: Ed Sheeran - Bad Habits  Ed Sheeran\n找到歌曲: Bad Habits by Ed Sheeran\n正在搜索: Shape of You Ed Sheeran\n找到歌曲: Shape of You by Ed Sheeran\n正在搜索: Ed Sheeran - Shivers  Ed Sheeran\n找到歌曲: Shivers by Ed Sheeran\n正在搜索: we can't be friends Ariana Grande\n找到歌曲: we can't be friends (wait for your love) by Ariana Grande\n正在搜索: thank u, next Ariana Grande\n找到歌曲: thank u, next by Ariana Grande\n正在搜索: the boy is mine Ariana Grande\n找到歌曲: the boy is mine by Ariana Grande\n正在搜索: yes, and? Ariana Grande\n找到歌曲: yes, and? by Ariana Grande\n正在搜索: positions Ariana Grande\n找到歌曲: positions by Ariana Grande\n正在搜索: BIRDS OF A FEATHER Billie Eilish\n找到歌曲: BIRDS OF A FEATHER by Billie Eilish\n正在搜索: CHIHIRO Billie Eilish\n找到歌曲: CHIHIRO by Billie Eilish\n正在搜索: Lost Cause Billie Eilish\n找到歌曲: Lost Cause by Billie Eilish\n正在搜索: LUNCH Billie Eilish\n找到歌曲: LUNCH by Billie Eilish\n正在搜索: when the party's over Billie Eilish\n找到歌曲: when the party's over by Billie Eilish\n正在搜索: Die With A Smile Lady Gaga\n找到歌曲: Die With A Smile by Lady Gaga\n正在搜索: Disease Lady Gaga\n找到歌曲: Disease by Lady Gaga\n正在搜索: Bad Romance Lady Gaga\n找到歌曲: Bad Romance by Lady Gaga\n正在搜索: Poker Face Lady Gaga\n找到歌曲: Poker Face by Lady Gaga\n正在搜索: Paparazzi Lady Gaga\n找到歌曲: Paparazzi by Lady Gaga\n正在搜索: Yummy Justin Bieber\n找到歌曲: Yummy by Justin Bieber\n正在搜索: Beauty And A Beat Justin Bieber\n找到歌曲: Beauty And A Beat by Justin Bieber\n正在搜索: Justin Bieber - Baby ft. Ludacris Justin Bieber\n找到歌曲: Baby (made popular by Justin Bieber ft. Ludacris) [vocal version] by Party Tyme\n正在搜索: Lonely Justin Bieber\n找到歌曲: Lonely by Justin Bieber\n正在搜索: Justin Bieber - What Do You Mean? Justin Bieber\n找到歌曲: What Do You Mean? by Justin Bieber\n正在搜索: WOMAN__ WORLD Katy Perry\n找到歌曲: WOMAN’S WORLD by Katy Perry\n正在搜索: California Gurls Katy Perry\n找到歌曲: California Gurls by Katy Perry\n正在搜索: Bon App‚àö¬©tit Katy Perry\n找到歌曲: O Mistério do Fone de Ouvido by LAMEGO\n正在搜索: Last Friday Night Katy Perry\n找到歌曲: Last Friday Night (T.G.I.F.) by Katy Perry\n正在搜索: Part Of Me Katy Perry\n找到歌曲: Part Of Me by Katy Perry\n正在搜索: Die With A Smile Bruno Mars\n找到歌曲: Die With A Smile by Lady Gaga\n正在搜索: APT. Bruno Mars\n找到歌曲: APT. by ROSÉ\n正在搜索: 24K Magic Bruno Mars\n找到歌曲: 24K Magic by Bruno Mars\n正在搜索: Uptown Funk Bruno Mars\n找到歌曲: Uptown Funk (feat. Bruno Mars) by Mark Ronson\n正在搜索: The Lazy Song Bruno Mars\n找到歌曲: The Lazy Song by Bruno Mars\n正在搜索: Easy On Me Adele\n找到歌曲: Easy On Me by Adele\n正在搜索: Rolling in the Deep Adele\n找到歌曲: Rolling in the Deep by Adele\n正在搜索: Make You Feel My Love Adele\n找到歌曲: Make You Feel My Love by Adele\n正在搜索: Someone Like You Adele\n找到歌曲: Someone Like You by Adele\n正在搜索: Top 8 Adele Songs | Adele's Best Songs Playlist | Top English songs | Popular English music playlist Adele\n找到歌曲: Someone Like You by Adele\n正在搜索: Why Why Why Shawn Mendes\n找到歌曲: Why Why Why by Shawn Mendes\n正在搜索: Stitches Shawn Mendes\n找到歌曲: Stitches by Shawn Mendes\n正在搜索: There's Nothing Holdin' Me Back Shawn Mendes\n找到歌曲: There's Nothing Holdin' Me Back by Shawn Mendes\n正在搜索: Nobody Knows Shawn Mendes\n找到歌曲: Nobody Knows by Shawn Mendes\n正在搜索: Heart of Gold Shawn Mendes\n找到歌曲: Heart of Gold by Shawn Mendes\n正在搜索: Flowers Miley Cyrus\n找到歌曲: Flowers by Miley Cyrus\n正在搜索: We Can't Stop Miley Cyrus\n找到歌曲: We Can't Stop by Miley Cyrus\n正在搜索: Party In The U.S.A. Miley Cyrus\n找到歌曲: Party In The U.S.A. by Miley Cyrus\n正在搜索: Jaded Miley Cyrus\n找到歌曲: Jaded by Miley Cyrus\n正在搜索: Wrecking Ball Miley Cyrus\n找到歌曲: Wrecking Ball by Miley Cyrus\n正在搜索: Lose You To Love Me Selena Gomez\n找到歌曲: Lose You To Love Me by Selena Gomez\n正在搜索: Calm Down Selena Gomez\n找到歌曲: Calm Down (with Selena Gomez) by Rema\n正在搜索: Love On Selena Gomez\n找到歌曲: Love On by Selena Gomez\n正在搜索: The Heart Wants What It Wants Selena Gomez\n找到歌曲: The Heart Wants What It Wants by Selena Gomez\n正在搜索: Selena Gomez - Hands To Myself Selena Gomez\n找到歌曲: Hands To Myself by Selena Gomez\n正在搜索: Sorry Not Sorry Demi Lovato\n找到歌曲: Sorry Not Sorry by Demi Lovato\n正在搜索: Dancing With The Devil Demi Lovato\n找到歌曲: Dancing With The Devil by Demi Lovato\n正在搜索: Heart Attack Demi Lovato\n找到歌曲: Heart Attack by Demi Lovato\n正在搜索: Cool for the Summer Demi Lovato\n找到歌曲: Cool for the Summer by Demi Lovato\n正在搜索: I Love Me Demi Lovato\n找到歌曲: I Love Me by Demi Lovato\n正在搜索: Dirrty Christina Aguilera\n找到歌曲: Dirrty (feat. Redman) by Christina Aguilera\n正在搜索: Genie In A Bottle Christina Aguilera\n找到歌曲: Genie In a Bottle by Christina Aguilera\n正在搜索: Candyman Christina Aguilera\n找到歌曲: Candyman by Christina Aguilera\n正在搜索: Come On Over Christina Aguilera\n找到歌曲: Come on over Baby (All I Want Is You) - Radio Version by Christina Aguilera\n正在搜索: Can't Hold Us Down Christina Aguilera\n找到歌曲: Can't Hold Us Down (feat. Lil' Kim) by Christina Aguilera\n正在搜索: Oops!...I Did It Again Britney Spears\n找到歌曲: Oops!...I Did It Again by Britney Spears\n正在搜索: Womanizer Britney Spears\n找到歌曲: Womanizer by Britney Spears\n正在搜索: ...Baby One More Time Britney Spears\n找到歌曲: ...Baby One More Time by Britney Spears\n正在搜索: Toxic Britney Spears\n找到歌曲: Toxic by Britney Spears\n正在搜索: Everytime Britney Spears\n找到歌曲: Everytime by Britney Spears\n正在搜索: All I Want for Christmas Is You Mariah Carey\n找到歌曲: All I Want for Christmas Is You by Mariah Carey\n正在搜索: We Belong Together Mariah Carey\n找到歌曲: We Belong Together by Mariah Carey\n正在搜索: Touch My Body Mariah Carey\n找到歌曲: Touch My Body by Mariah Carey\n正在搜索: Heartbreaker Mariah Carey\n找到歌曲: Heartbreaker (feat. Jay-Z) by Mariah Carey\n正在搜索: Always Be My Baby Mariah Carey\n找到歌曲: Always Be My Baby by Mariah Carey\n正在搜索: So What Pink\n找到歌曲: So What by P!nk\n正在搜索: P!nk - Just Give Me A Reason ft. Nate Ruess Pink\n找到歌曲: Just Give Me a Reason (feat. Nate Ruess) by P!nk\n正在搜索: So What Pink\n找到歌曲: So What by P!nk\n正在搜索: Beautiful Trauma Pink\n找到歌曲: Beautiful Trauma by P!nk\n正在搜索: Christina Aguilera, Lil' Kim, Mya, P!nk - Lady Marmalade Pink\n找到歌曲: Lady Marmalade - Single Edit by Christina Aguilera\n正在搜索: Broken & Beautiful Kelly Clarkson\n找到歌曲: Broken & Beautiful by Kelly Clarkson\n正在搜索: Behind These Hazel Eyes Kelly Clarkson\n找到歌曲: Behind These Hazel Eyes by Kelly Clarkson\n正在搜索: Because Of You Kelly Clarkson\n找到歌曲: Because of You by Kelly Clarkson\n正在搜索: A Moment Like This Kelly Clarkson\n找到歌曲: A Moment Like This by Kelly Clarkson\n正在搜索: Kelly Clarkson - I Do Not Hook Up Kelly Clarkson\n找到歌曲: I Do Not Hook Up by Kelly Clarkson\n正在搜索: Can't Get Enough Jennifer Lopez\n找到歌曲: Can't Get Enough by Jennifer Lopez\n正在搜索: Love Don't Cost a Thing Jennifer Lopez\n找到歌曲: Love Don't Cost a Thing by Jennifer Lopez\n正在搜索: If You Had My Love Jennifer Lopez\n找到歌曲: If You Had My Love by Jennifer Lopez\n正在搜索: Can't Get Enough Jennifer Lopez\n找到歌曲: Can't Get Enough by Jennifer Lopez\n正在搜索: Jennifer Lopez - Booty ft. Iggy Azalea Jennifer Lopez\n找到歌曲: Cardiel Tn - Jennifer López, G Lesson - Tkbxby Remix by Cardiel tn\n正在搜索: Umbrella Rihanna\n找到歌曲: Umbrella by Rihanna\n正在搜索: SOS Rihanna\n找到歌曲: SOS by Rihanna\n正在搜索: Rihanna - Love On The Brain Rihanna\n找到歌曲: Love On The Brain by Rihanna\n正在搜索: Work Rihanna\n找到歌曲: Work by Rihanna\n正在搜索: ANTIdiaRy Rihanna\n找到歌曲: James Joint by Rihanna\n正在搜索: Sun Is Shining Calvin Harris\n找到歌曲: Beach Day by Sweat-EZ\n正在搜索: I Found U Calvin Harris\n找到歌曲: I Found You / Nilda’s Story (with Calvin Harris & Miguel) by benny blanco\n正在搜索: Center Of The Universe Calvin Harris\n找到歌曲: Good Luck, Babe! by Chappell Roan\n正在搜索: Axwell /\\ Ingrosso - More Than You Know Calvin Harris\n找到歌曲: taylor swift by Angel of Discord\n正在搜索: Dreamer Calvin Harris\n找到歌曲: Prayers Up - Instrumental by Piano Dreamers\n正在搜索: I Could Be The One David Guetta\n找到歌曲: Nobody by Ay Bangz\n正在搜索: Toulouse David Guetta\n找到歌曲: La familia by Bigflo & Oli\n正在搜索: Nicky Romero - Toulouse David Guetta\n找到歌曲: Wild One Two (feat. David Guetta, Nicky Romero & Sia) - No_ID Remix by Jack Back\n正在搜索: Like Home David Guetta\n找到歌曲: One Last Time (Acoustic Version) by Chir Cataran\n正在搜索: Legacy David Guetta\n找到歌曲: Been Dope by Path P\n正在搜索: How We Party Skrillex\n找到歌曲: Hey Yea by NEFFEX\n正在搜索: R3HAB, INNA, Sash! _ Rock My Body (Official Music Video) Skrillex\n找到歌曲: 1OF1 by Kenzo!\n正在搜索: Karate Skrillex\n找到歌曲: Bangarang (feat. Sirah) by Skrillex\n正在搜索: Tiger Skrillex\n找到歌曲: RATATA by Skrillex\n正在搜索: All Around The World Skrillex\n找到歌曲: This Ain't That by Swoop Gianni\n正在搜索: On My Mind Diplo\n找到歌曲: On My Mind by Diplo\n正在搜索: Get It Right Diplo\n找到歌曲: Get It Right by Diplo\n正在搜索: Wish Diplo\n找到歌曲: Wish (feat. Trippie Redd) - Trippie Mix by Diplo\n正在搜索: Heartless feat. Morgan Wallen Diplo\n找到歌曲: Heartless (feat. Morgan Wallen) by Diplo\n正在搜索: \"Where Are _ Now\" with Justin Bieber Diplo\n找到歌曲: Where Are Ü Now (with Justin Bieber) by Jack Ü\n正在搜索: Monophobia Deadmau5\n找到歌曲: Monophobia by deadmau5\n正在搜索: Quezacotl Deadmau5\n找到歌曲: Quezacotl by deadmau5\n正在搜索: Professional Griefers Deadmau5\n找到歌曲: Professional Griefers - Vocal Mix by deadmau5\n正在搜索: When The Summer Dies Deadmau5\n找到歌曲: When The Summer Dies by deadmau5\n正在搜索: Pomegranate Deadmau5\n找到歌曲: Pomegranate by deadmau5\n正在搜索: Zedd - Clarity ft. Foxes Zedd\n找到歌曲: Clarity (made popular by Zedd ft. Foxes) [vocal version] by Party Tyme\n正在搜索: Find You ft. Matthew Koma, Miriam Bryant Zedd\n找到歌曲: Find You (made popular by Zedd ft. Matthew Koma, Miriam Bryant) [backing version] by Party Tyme\n正在搜索: The Middle Zedd\n找到歌曲: The Middle by Zedd\n正在搜索: Happy Now Zedd\n找到歌曲: Happy Now by Zedd\n正在搜索: Stay Zedd\n找到歌曲: Stay by Zedd\n正在搜索: Avicii - For A Better Day Avicii\n找到歌曲: For A Better Day by Avicii\n正在搜索: Wake Me Up Avicii\n找到歌曲: Wake Me Up by Avicii\n正在搜索: Avicii - Waiting For Love Avicii\n找到歌曲: Waiting For Love by Avicii\n正在搜索: Avicii - Hey Brother Avicii\n找到歌曲: Hey Brother by Avicii\n正在搜索: Avicii - Levels Avicii\n找到歌曲: Levels - Radio Edit by Avicii\n正在搜索: Kygo (The Album) - Live From The Troll__ Tongue Kygo\n找到歌曲: Sing About Me, I'm Dying Of Thirst by Kendrick Lamar\n正在搜索: Firestone ft. Conrad Sewell Kygo\n找到歌曲: Firestone (made popular by Kygo ft. Conrad Sewell) [backing version] by Party Tyme\n正在搜索: Stars Will Align Kygo\n找到歌曲: Stars Will Align by Kygo\n正在搜索: Kygo - Stole The Show feat. Parson James  Kygo\n找到歌曲: Stole the Snow - Instrumental MS Mix by Jack Jefferson\n正在搜索: Hold On Me Kygo\n找到歌曲: Hold On Me by Kygo\n正在搜索: Don't Be Shy Ti‚àö¬¥sto\n找到歌曲: scenecore idolZz!! - Sped Up by Proxie\n正在搜索: The Business Ti‚àö¬¥sto\n找到歌曲: X6 (feat. BIA) by Sfera Ebbasta\n正在搜索: The Motto Ti‚àö¬¥sto\n找到歌曲: Il più regolare by Oro Bianco\n正在搜索: All Nighter Ti‚àö¬¥sto\n找到歌曲: Non ce giocate by Gente de Borgata\n正在搜索: 10:35 Ti‚àö¬¥sto\n找到歌曲: Ne Bih Se Mijenja by Who See\n正在搜索: Alone Marshmello\n找到歌曲: Alone by Marshmello\n正在搜索: Happier Marshmello\n找到歌曲: Happier by Marshmello\n正在搜索: Stars Marshmello\n找到歌曲: Stars by Marshmello\n正在搜索: Lights On Marshmello\n找到歌曲: Lights On by Marshmello\n正在搜索: Together Marshmello\n找到歌曲: Together by Marshmello\n正在搜索: No Beef Steve Aoki\n找到歌曲: No Beef - Steve Aoki's 11 Years Later Remix by AFROJACK\n正在搜索: REMIX RUMBLE ft. Steve Aoki (Official Music Video) | Teamfight Tactics Steve Aoki\n找到歌曲: REMIX RUMBLE - Steve Aoki Remix by League of Legends\n正在搜索: BTS (√é‚àû¬©√å_____) 'MIC Drop (Steve Aoki Remix)' Official MV Steve Aoki\n找到歌曲: MIC Drop (Steve Aoki Remix) (Full Length Edition) by BTS\n正在搜索: Boneless Steve Aoki\n找到歌曲: Boneless by Steve Aoki\n正在搜索: Waste It On Me feat. BTS Steve Aoki\n找到歌曲: Waste It On Me (feat. BTS) by Steve Aoki\n正在搜索: Don't Let Me Down The Chainsmokers\n找到歌曲: Don't Let Me Down by The Chainsmokers\n正在搜索: Closer The Chainsmokers\n找到歌曲: Closer by The Chainsmokers\n正在搜索: Closer The Chainsmokers\n找到歌曲: Closer by The Chainsmokers\n正在搜索: The Chainsmokers - Let You Go ft. Great Good Fine Ok The Chainsmokers\n找到歌曲: Let You Go - Radio Edit by The Chainsmokers\n正在搜索: Addicted ft. Ink The Chainsmokers\n找到歌曲: squabble up by Kendrick Lamar\n正在搜索: Turn Up The Speakers Afrojack\n找到歌曲: Turn up the Speakers - Radio Edit by AFROJACK\n正在搜索: Hey ft. Afrojack Afrojack\n找到歌曲: Hey Mama (made popular by David Guetta ft. Nicki Minaj & Afrojack) [vocal version] by Party Tyme\n正在搜索: Rock The House Afrojack\n找到歌曲: Rock The House by AFROJACK\n正在搜索: Ten Feet Tall Afrojack\n找到歌曲: Ten Feet Tall by AFROJACK\n正在搜索: No Beef Afrojack\n找到歌曲: No Beef by AFROJACK\n正在搜索: Major Lazer _ Light it Up (feat. Nyla & Fuse ODG) (Remix)  Major Lazer\n找到歌曲: Light It Up (Remix) by Major Lazer\n正在搜索: Lean On Major Lazer\n找到歌曲: Lean On by Major Lazer\n正在搜索: Pon De Floor Major Lazer\n找到歌曲: Pon De Floor - Instrumental by Major Lazer\n正在搜索: Lean On Major Lazer\n找到歌曲: Lean On by Major Lazer\n正在搜索: Run Up Major Lazer\n找到歌曲: Run Up by Major Lazer\n正在搜索: I Like It Alesso\n找到歌曲: I Like It (with Nate Smith) by Alesso\n正在搜索: REMEDY Alesso\n找到歌曲: REMEDY by Alesso\n正在搜索: Words Alesso\n找到歌曲: Words (feat. Zara Larsson) by Alesso\n正在搜索: Never Going Home Tonight Alesso\n找到歌曲: Never Going Home Tonight (feat. Madison Love) by David Guetta\n正在搜索: Heroes Alesso\n找到歌曲: Heroes (we could be) by Alesso\n正在搜索: Disco Maghreb DJ Snake\n找到歌曲: Disco Maghreb by DJ Snake\n正在搜索: DJ Snake, Lil Jon - Turn Down for What DJ Snake\n找到歌曲: Turn Down for What by DJ Snake\n正在搜索: DJ Snake, AlunaGeorge - You Know You Like It DJ Snake\n找到歌曲: You Know You Like It by DJ Snake\n正在搜索: Lean On DJ Snake\n找到歌曲: Lean On by Major Lazer\n正在搜索: Diana DJ Snake\n找到歌曲: Diana (with Hamza) by DJ Snake\n正在搜索: Falling In Love Hardwell\n找到歌曲: Falling In Love by Hardwell\n正在搜索: Call Me A Spaceman Hardwell\n找到歌曲: Call Me A Spaceman by Hardwell\n正在搜索: HARDWELL - TOMORROWLAND BRASIL 2024 Hardwell\n找到歌曲: Not Like Us by Kendrick Lamar\n正在搜索: Young Again Hardwell\n找到歌曲: Young Again (feat. Chris Jones) by Hardwell\n正在搜索: Sally Hardwell\n找到歌曲: Sally (feat. Harrison) by Hardwell\n正在搜索: Animals Martin Garrix\n找到歌曲: Animals by Martin Garrix\n正在搜索: High On Life Martin Garrix\n找到歌曲: High On Life (feat. Bonn) by Martin Garrix\n正在搜索: In The Name Of Love Martin Garrix\n找到歌曲: In the Name of Love by Martin Garrix\n正在搜索: Told You So Martin Garrix\n找到歌曲: Told You So by Martin Garrix\n正在搜索: Tremor Martin Garrix\n找到歌曲: Tremor - Sensation 2014 Anthem by Dimitri Vegas & Like Mike\n正在搜索: Don't You Worry Child Swedish House Mafia\n找到歌曲: Don't You Worry Child - Radio Edit by Swedish House Mafia\n正在搜索:  Lioness Swedish House Mafia\n找到歌曲: Lioness by Swedish House Mafia\n正在搜索: Save The World Swedish House Mafia\n找到歌曲: Save The World by Swedish House Mafia\n正在搜索: One Swedish House Mafia\n找到歌曲: One by Swedish House Mafia\n正在搜索: Swedish House Mafia - Greyhound - Extended Video Remix HD Swedish House Mafia\n找到歌曲: Greyhound by Swedish House Mafia\n正在搜索: In And Out Of Love Armin van Buuren\n找到歌曲: In And Out Of Love (Mixed) by Rivo\n正在搜索: This Is What It Feels Like Armin van Buuren\n找到歌曲: This Is What It Feels Like - Armin van Buuren 2023 Remix by Armin van Buuren\n正在搜索: Waiting For The Night Armin van Buuren\n找到歌曲: Waiting For The Night by Armin van Buuren\n正在搜索: I Need You Armin van Buuren\n找到歌曲: I Need You by Armin van Buuren\n正在搜索: Feels So Good Armin van Buuren\n找到歌曲: Feels So Good - Armin van Buuren Club Mix by Armin van Buuren\n正在搜索: Misty Sarah Vaughan\n找到歌曲: Misty by Sarah Vaughan\n正在搜索: *Sarah Vaughan* - Broken hearted Melody Sarah Vaughan\n找到歌曲: Broken Hearted Melody by Sarah Vaughan\n正在搜索: Somewhere Over The Rainbow Sarah Vaughan\n找到歌曲: Over the Rainbow by Sarah Vaughan\n正在搜索: Sarah Vaughan & Cleo Laine __edley_ (1977) Sarah Vaughan\n找到歌曲: October Song by Amy Winehouse\n正在搜索: live Sweden '58, Holland '58, & Sweden '64 Sarah Vaughan\n找到歌曲: reincarnated by Kendrick Lamar\n正在搜索: Pash 1997 Kate Ceberano \n找到歌曲: Courage by Kate Ceberano\n正在搜索: Kate Ceberano - Brave Official Music Video Kate Ceberano \n找到歌曲: Helen by Kate Ceberano\n正在搜索: Kate Ceberano - Bedroom Eyes Kate Ceberano \n找到歌曲: Bedroom Eyes by Kate Ceberano\n正在搜索: Brave Kate Ceberano \n找到歌曲: Bedroom Eyes by Kate Ceberano\n正在搜索: Kate Ceberano & Ronan Keating - It's Only Christmas - 2009 Kate Ceberano \n找到歌曲: Last Christmas by Wham!\n正在搜索: Don't Explain Cassandra Wilson\n找到歌曲: Don't Explain by Cassandra Wilson\n正在搜索: Cassandra Wilson - Redemption Song.MP4 Cassandra Wilson\n找到歌曲: Jackie and Wilson by Hozier\n正在搜索: Cassandra Wilson \"You Go To My Head\" Cassandra Wilson\n找到歌曲: You Go to My Head by Cassandra Wilson\n正在搜索: Cassandra Wilson - Harvest Moon Cassandra Wilson\n找到歌曲: Harvest Moon by Cassandra Wilson\n正在搜索: Cassandra Wilson \"Red Guitar\" Cassandra Wilson\n找到歌曲: Red Guitar by Cassandra Wilson\n正在搜索: Nancy Wilson \"Face It, Girl It's Over\" on The Ed Sullivan Show Nancy Wilson \n找到歌曲: Face It Girl, It’s Over - Live On The Ed Sullivan Show, November 24, 1968 by Nancy Wilson\n正在搜索: (YOU DON'T KNOW) HOW GLAD I AM Nancy Wilson \n找到歌曲: (You Don't Know) How Glad I Am by Nancy Wilson\n正在搜索: For Once in My Life Nancy Wilson \n找到歌曲: For Once In My Life - Remastered/1994 by Nancy Wilson\n正在搜索: Heart - These Dreams Nancy Wilson \n找到歌曲: My Northern Heart in Pieces by Berka 1337\n正在搜索: Nancy Wilson -- \"Don't Ask My Neighbors\" Live (1990) Nancy Wilson \n找到歌曲: Peace of Mind by Riccardo LoDolce\n正在搜索: 'Round Midnight Carmen McRae\n找到歌曲: 'Round Midnight - Remastered 2001 by Carmen McRae\n正在搜索: Sammy Davis & Carmen McRae - Funny Girl Medley Carmen McRae\n找到歌曲: Happy To Make Your Acquaintance by Sammy Davis Jr.\n正在搜索: CARMEN MCRAE - No More Blues Carmen McRae\n找到歌曲: No More Blues (Chega De Saudade) - Live At The Great American Music Hall/1976 by Carmen McRae\n正在搜索: Get It Straight - 8/14/1988 - Newport Jazz Festival Carmen McRae\n找到歌曲: Get It Straight - Remastered 2001 by Carmen McRae\n正在搜索: Carmen McRae - Music Carmen McRae\n找到歌曲: Stolen Moments - Live At The Great American Music Hall, San Francisco / 1987 by Carmen McRae\n正在搜索: BETTY CARTER Betty Carter\n找到歌曲: This Is Always by Betty Carter\n正在搜索: The Good Life Betty Carter\n找到歌曲: The Good Life by Betty Carter\n正在搜索: Betty Carter - But Beautiful Betty Carter\n找到歌曲: But Beautiful - Live At The Great American Music Hall, San Francisco / 1987 by Carmen McRae\n正在搜索: Betty Carter - New All The Time - FILM Betty Carter\n找到歌曲: Disco Man by Idris Muhammad\n正在搜索: In concert Betty Carter 1980 part 1 Betty Carter\n找到歌曲: wacced out murals by Kendrick Lamar\n正在搜索: JULIE LONDON - CRY ME A RIVER Julie London\n找到歌曲: Cry Me A River by Julie London\n正在搜索: Cry Me a River Julie London\n找到歌曲: Cry Me A River by Julie London\n正在搜索: Julie London - Why don't you do right Julie London\n找到歌曲: Why Don't You Do Right by Julie London\n正在搜索: Cry Me A River Julie London\n找到歌曲: Cry Me A River by Julie London\n正在搜索: Fly Me To The Moon Julie London\n找到歌曲: Fly Me To The Moon (In Other Words) by Julie London\n正在搜索: The Christmas Song Nat King Cole\n找到歌曲: The Christmas Song by Nat King Cole\n正在搜索: Nat King Cole - Unforgettable Nat King Cole\n找到歌曲: Unforgettable by Nat King Cole\n正在搜索: Nat King Cole: \"An Evening With Nat King Cole\" - LIVE! Nat King Cole\n找到歌曲: A Nightingale Sang In Berkeley Square (duet with Gloria Estefan) by Nat King Cole\n正在搜索: Natalie Cole & Nat King Cole \"Unforgettable\" 1991 (audio remastered) Nat King Cole\n找到歌曲: Unforgettable by Nat King Cole\n正在搜索: Nat King Cole Greatest Hits - Best Songs Of Nat King Cole - The Very Best of Nat King Cole Nat King Cole\n找到歌曲: When I Fall In Love by Nat King Cole\n正在搜索: Jamie Cullum - Mind Trick Jamie Cullum\n找到歌曲: Mind Trick by Jamie Cullum\n正在搜索: Drink Jamie Cullum\n找到歌曲: Drink by Jamie Cullum\n正在搜索: Jamie Cullum - Don't Stop the Music Jamie Cullum\n找到歌曲: Don't Stop The Music by Jamie Cullum\n正在搜索: Jamie Cullum - Hang Your Lights Jamie Cullum\n找到歌曲: Hang Your Lights by Jamie Cullum\n正在搜索: In The Bleak Midwinter Jamie Cullum\n找到歌曲: In The Bleak Midwinter by Jamie Cullum\n正在搜索: Diana Krall - Let's Face The Music And Dance Diana Krall\n找到歌曲: Let's Face The Music And Dance by Diana Krall\n正在搜索: Diana Krall - Just The Way You Are Diana Krall\n找到歌曲: Just The Way You Are by Diana Krall\n正在搜索: Diana Krall Live in Paris Diana Krall\n找到歌曲: Just The Way You Are by Diana Krall\n正在搜索: Cry Me A River Diana Krall\n找到歌曲: Cry Me A River by Diana Krall\n正在搜索: Diana Krall - The Look Of Love Diana Krall\n找到歌曲: The Look Of Love by Diana Krall\n正在搜索: Until C‚àö¬©cile McLorin Salvant\n找到歌曲: Condone It by Juice WRLD\n正在搜索: C‚àö¬©cile McLorin Salvant: NPR Music Tiny Desk Concert C‚àö¬©cile McLorin Salvant\n找到歌曲: En El Olvido - Live At NPR's Tiny Desk by Omar Apollo\n正在搜索: Ghost Song C‚àö¬©cile McLorin Salvant\n找到歌曲: What Makes You Beautiful by One Direction\n正在搜索: Over the Rainbow C‚àö¬©cile McLorin Salvant\n找到歌曲: Goin' Down by Ol' Dirty Bastard\n正在搜索: C‚àö¬©cile McLorin Salvant at The Met Cloisters: M‚àö¬©lusine | MetLiveArts C‚àö¬©cile McLorin Salvant\n找到歌曲: Est-ce ainsi que les hommes vivent ? by Cécile McLorin Salvant\n正在搜索: Gregory Porter performs It's Probably Me at the Polar Music Prize Ceremony 2017 Gregory Porter\n找到歌曲: It's Probably Me - Live at Polar Music Prize, Stockholm / 2017 by Gregory Porter\n正在搜索: Gregory Porter - Holding On ft. Kem Gregory Porter\n找到歌曲: All I Want for Christmas Is You by Mariah Carey\n正在搜索: Hey Laura Gregory Porter\n找到歌曲: Hey Laura by Gregory Porter\n正在搜索: \"Be Good Gregory Porter\n找到歌曲: Be Good (Lion's Song) by Gregory Porter\n正在搜索: Insanity Gregory Porter\n找到歌曲: Insanity by Gregory Porter\n正在搜索: Baby I'm A Fool Melody Gardot\n找到歌曲: Baby I'm A Fool by Melody Gardot\n正在搜索: Preacherman Melody Gardot\n找到歌曲: Preacherman by Melody Gardot\n正在搜索: Little Something Melody Gardot\n找到歌曲: Little Something by Melody Gardot\n正在搜索: C__st Magnifique Melody Gardot\n找到歌曲: C'est Magnifique - Live In Namouche Studios by Melody Gardot\n正在搜索: From Paris With Love Melody Gardot\n找到歌曲: From Paris With Love - Single Version by Melody Gardot\n正在搜索: Formwela 1 Esperanza Spalding\n找到歌曲: Formwela 10 by Esperanza Spalding\n正在搜索: Esperanza Spalding Crowned & Kissed Official Esperanza Spalding\n找到歌曲: Cancioncitas de Amor by Romeo Santos\n正在搜索: Esperanza Spalding - \"One\" Esperanza Spalding\n找到歌曲: One by Esperanza Spalding\n正在搜索: Black Gold by Esperanza Spalding  Esperanza Spalding\n找到歌曲: Black Gold (As Made Famous By Esperanza Spalding) [Karaoke Version] by Off The Record\n正在搜索: Esperanza Spalding - Unconditional Love Esperanza Spalding\n找到歌曲: Unconditional Love by Esperanza Spalding\n正在搜索: Norah Jones - Come Away With Me Norah Jones\n找到歌曲: Come Away With Me by Norah Jones\n正在搜索: Norah Jones - Don't Know Why Norah Jones\n找到歌曲: Don't Know Why by Norah Jones\n正在搜索: Norah Jones - Sunrise Norah Jones\n找到歌曲: Sunrise by Norah Jones\n正在搜索: Carry On Norah Jones\n找到歌曲: Carry On by Norah Jones\n正在搜索: Happy Pills Norah Jones\n找到歌曲: Happy Pills by Norah Jones\n正在搜索: Louis Armstrong - What A Wonderful World Louis Armstrong\n找到歌曲: What A Wonderful World by Louis Armstrong\n正在搜索: What A Wonderful World Louis Armstrong\n找到歌曲: What A Wonderful World by Louis Armstrong\n正在搜索: Louis Armstrong \"When The Saints Go Marching In\" on The Ed Sullivan Show Louis Armstrong\n找到歌曲: When The Saints Go Marching In - Live On The Ed Sullivan Show, September 20, 1959 by Louis Armstrong\n正在搜索: What A Wonderful World Louis Armstrong\n找到歌曲: What A Wonderful World by Louis Armstrong\n正在搜索: Louis Armstrong - La vie en rose Louis Armstrong\n找到歌曲: La vie en rose - Single Version by Louis Armstrong\n正在搜索: Feeling Good Nina Simone\n找到歌曲: Feeling Good by Nina Simone\n正在搜索: Nina Simone Greatest Hits Full Album - Best Of Nina Simone 2021 - Nina Simone Jazz Songs Nina Simone\n找到歌曲: Feeling Good by Nina Simone\n正在搜索: I Put A Spell On You Nina Simone\n找到歌曲: I Put A Spell On You by Nina Simone\n正在搜索: Sinnerman Nina Simone\n找到歌曲: Sinnerman by Nina Simone\n正在搜索: Ain't Got No, I Got Life Nina Simone\n找到歌曲: Ain't Got No - I Got Life (From the musical production \"Hair\") by Nina Simone\n正在搜索: Have Yourself A Merry Little Christmas Frank Sinatra\n找到歌曲: Have Yourself A Merry Little Christmas - Remastered 1999 by Frank Sinatra\n正在搜索: Let It Snow! Let It Snow! Let It Snow! Frank Sinatra\n找到歌曲: Let It Snow! Let It Snow! Let It Snow! (with The B. Swanson Quartet) by Frank Sinatra\n正在搜索: Frank Sinatra - Luck Be A Lady Frank Sinatra\n找到歌曲: Luck Be A Lady by Frank Sinatra\n正在搜索: My Way Frank Sinatra\n找到歌曲: My Way by Frank Sinatra\n正在搜索: Frank Sinatra - Strangers In The Night Frank Sinatra\n找到歌曲: Strangers In The Night by Frank Sinatra\n正在搜索: Billie Holiday - \"Strange Fruit\" Live 1959  Billie Holiday\n找到歌曲: Strange Fruit - Live at Philharmonic Hall, Los Angeles, 1945 by Billie Holiday\n正在搜索: Fine And Mellow Billie Holiday\n找到歌曲: Fine And Mellow by Billie Holiday\n正在搜索: Billie Holiday & Louis Armstrong - New Orleans Billie Holiday\n找到歌曲: Jazz Thing - Video Mix by Gang Starr\n正在搜索: Billie Holiday - The Blues Are Brewin' Billie Holiday\n找到歌曲: Baby, I Don't Cry Over You by Billie Holiday\n正在搜索: Don't Explain Billie Holiday\n找到歌曲: Don't Explain by Billie Holiday\n正在搜索: Kurt Elling - Nature Boy - Jazz and Orchestra Kurt Elling\n找到歌曲: It's the Most Wonderful Time of the Year by Andy Williams\n正在搜索: Kenny Banks Jr. & Kurt Elling - \"Georgia on My Mind\" Kurt Elling\n找到歌曲: Georgia on My Mind by Ray Charles\n正在搜索: Where the Streets Have No Name Kurt Elling\n找到歌曲: Where The Streets Have No Name by Kurt Elling\n正在搜索: American Tune Kurt Elling\n找到歌曲: American Tune by Kurt Elling\n正在搜索: My Foolish Heart by Kurt Elling Kurt Elling\n找到歌曲: Santa Tell Me by Ariana Grande\n搜索结果已保存到 spotify_track_info_all—v1.csv\n\n\nfind singer’s information- using spotify api\n\nimport pandas as pd\nimport requests\n\n# Function to obtain an access token for Spotify API\ndef get_access_token(client_id, client_secret):\n    auth_url = 'https://accounts.spotify.com/api/token'\n    auth_data = {\n        'grant_type': 'client_credentials',\n        'client_id': client_id,\n        'client_secret': client_secret\n    }\n    response = requests.post(auth_url, data=auth_data)\n    if response.status_code == 200:\n        return response.json()['access_token']\n    else:\n        raise Exception('Failed to obtain access token')\n\n# Function to get the Spotify artist ID using the artist name\ndef get_artist_id(artist_name, access_token):\n    search_url = f'https://api.spotify.com/v1/search?q={artist_name}&type=artist&limit=1'\n    headers = {'Authorization': f'Bearer {access_token}'}\n    response = requests.get(search_url, headers=headers)\n    if response.status_code == 200:\n        search_results = response.json()\n        artists = search_results['artists']['items']\n        if artists:\n            return artists[0]['id']\n        else:\n            return None\n    else:\n        return None\n\n# Function to retrieve the artist's followers and popularity\ndef get_artist_followers(artist_id, access_token):\n    artist_url = f'https://api.spotify.com/v1/artists/{artist_id}'\n    headers = {'Authorization': f'Bearer {access_token}'}\n    response = requests.get(artist_url, headers=headers)\n    if response.status_code == 200:\n        artist_data = response.json()\n        return artist_data['followers']['total'], artist_data['popularity']\n    else:\n        return None, None\n\n# Load the input CSV file with artist names and genres\ninput_file = './list.csv' \ndf = pd.read_csv(input_file, header=None)\ndf.columns = ['Artist', 'Genre'] \n\n# Spotify API credentials\nclient_id = '31aba57b31344fdebf98f51375d07834'  # Replace with your client ID\nclient_secret = '2c4f929786784910bc9a843518785cae'  # Replace with your client secret\n\n# Obtain Spotify API access token\naccess_token = get_access_token(client_id, client_secret)\n\n# Initialize lists to store followers and popularity data\nfollowers_list = []\npopularity_list = []\n\n# Process each artist in the DataFrame\nfor artist_name in df['Artist']:\n    artist_id = get_artist_id(artist_name, access_token)\n    if artist_id:\n        followers, popularity = get_artist_followers(artist_id, access_token)\n        followers_list.append(followers)\n        popularity_list.append(popularity)\n    else:\n        # If the artist is not found, append None\n        followers_list.append(None)\n        popularity_list.append(None)\n\n# Add followers and popularity data to the DataFrame\ndf['Followers'] = followers_list\ndf['Popularity'] = popularity_list\n\n# Save the updated DataFrame to a new CSV file\noutput_file = './artist_data_with_followers_and_popularity.csv'  \ndf.to_csv(output_file, index=False)\n\nprint(f\"Processed data saved to: {output_file}\")\n\nDelete Spotify and YouTube mismatches\n\nimport pandas as pd\n\ninput_file = './spotify_track_info_all—v1.csv' \ndf = pd.read_csv(input_file, encoding='MacRoman')\n\n# Filter rows where 'Artist Name' matches 'Artist'\ndf_cleaned = df[df['Artist Name'] == df['Artist']]\n\n# Save the cleaned DataFrame to a new CSV file\noutput_file = './spotify_youtube.csv'  # Relative path to the output file\ndf_cleaned.to_csv(output_file, index=False)\nprint(f\"Cleaned data saved to: {output_file}\")"
  },
  {
    "objectID": "technical-details/data-collection/Untitled-1.html",
    "href": "technical-details/data-collection/Untitled-1.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "from googleapiclient.discovery import build\nimport pandas as pd\nfrom datetime import datetime\nimport dateutil.parser\n\n# API Key\napi_key = \"AIzaSyDtKE-4QZj6EA-rwG7cj5gMJxdt4Fe14Nw\"\n\n# Initialize YouTube API client\nyoutube = build('youtube', 'v3', developerKey=api_key)\n\n# List to store data\nall_data = []\n\n# Read song data and fetch YouTube statistics\nwith open('song_data.txt', 'r') as file:\n    for line in file:\n        artist = line.strip()\n        query = f\"{artist} official music video\"\n\n        # Search request for the query\n        search_request = youtube.search().list(\n            part=\"snippet\",\n            q=query,\n            maxResults=5,\n            type=\"video\",\n            order='relevance'\n        )\n        search_response = search_request.execute()\n\n        for item in search_response['items']:\n            video_id = item['id']['videoId']\n            video_request = youtube.videos().list(\n                part=\"snippet,contentDetails,statistics\",\n                id=video_id\n            )\n            video_response = video_request.execute()\n\n            for video in video_response['items']:\n                snippet = video['snippet']\n                content_details = video['contentDetails']\n                statistics = video['statistics']\n\n                # Calculate days since video was published\n                published_at = dateutil.parser.parse(snippet['publishedAt'])\n                days_since_published = (datetime.now(published_at.tzinfo) - published_at).days\n\n                # Fetch channel details for subscriber count\n                channel_response = youtube.channels().list(\n                    part='statistics',\n                    id=snippet['channelId']\n                ).execute()\n                subscriber_count = channel_response['items'][0]['statistics'].get('subscriberCount', '0')\n\n                # Fetch comments\n                comments_request = youtube.commentThreads().list(\n                    part='snippet',\n                    videoId=video_id,\n                    order='relevance',\n                    maxResults=10\n                )\n                comments_response = comments_request.execute()\n\n                comments = [comment['snippet']['topLevelComment']['snippet']['textDisplay']\n                            for comment in comments_response.get('items', [])]\n\n                # Store all data in a dictionary\n                video_data = {\n                    'Video ID': video_id,\n                    'Title': snippet['title'],\n                    'Description': snippet['description'],\n                    'Published At': snippet['publishedAt'],\n                    'Days Since Published': days_since_published,\n                    'View Count': statistics.get('viewCount', '0'),\n                    'Like Count': statistics.get('likeCount', '0'),\n                    'Comment Count': statistics.get('commentCount', '0'),\n                    'Comments': comments,\n                    'Subscriber Count': subscriber_count,\n                    'Category ID': snippet['categoryId'],\n                    'Definition': content_details['definition'],\n                    'Duration': content_details['duration']\n                }\n\n                all_data.append(video_data)\n\n# Convert the list to a DataFrame\nfinal_df = pd.DataFrame(all_data)\n\n# Optionally save the DataFrame to a CSV file\nfinal_df.to_csv('Detailed_YouTube_Video_Data.csv', index=False)\n\n# Display the DataFrame\nprint(final_df.head())\n\n      Video ID                                              Title  \\\n0  1VQ_3sBZEm0    Foo Fighters - Learn To Fly (Official HD Video)   \n1  eBG7P-K-r1Y        Foo Fighters - Everlong (Official HD Video)   \n2  SBjQ9tuuTJQ                       Foo Fighters - The Pretender   \n3  EqWRaAF6_WY         Foo Fighters - My Hero (Official HD Video)   \n4  h_L4Rixya64  Foo Fighters - Best Of You (Official Music Video)   \n\n                                         Description          Published At  \\\n0  Foo Fighters' official music video for 'Learn ...  2009-10-03T04:46:13Z   \n1  \"Everlong\" by Foo Fighters \\nListen to Foo Fig...  2009-10-03T04:49:58Z   \n2  Watch the official music video for \"The Preten...  2009-10-03T04:46:14Z   \n3  \"My Hero\" by Foo Fighters \\nListen to Foo Figh...  2011-03-18T19:35:42Z   \n4  Watch the official music video for \"Best Of Yo...  2009-10-03T20:49:33Z   \n\n   Days Since Published View Count Like Count Comment Count  \\\n0                  5549  183888273     808087         33852   \n1                  5549  324339281    1820753         53186   \n2                  5549  588029134    2785440         92233   \n3                  5017   87504683     564313         26089   \n4                  5548  265538783    1281111         34995   \n\n                                            Comments Subscriber Count  \\\n0  [Can we bring this back? This feeling? This mu...          1290000   \n1  [Stop asking who&#39;s still listening, we nev...          1290000   \n2  [How can you not get chills listening to this ...          1290000   \n3  [My son and I was supposed to spend the summer...          1290000   \n4  [What a talent Dave is. Drummer, singer, guita...          1290000   \n\n  Category ID Definition Duration  \n0          10         hd  PT4M37S  \n1          10         hd  PT4M52S  \n2          10         hd  PT4M31S  \n3          10         hd   PT4M3S  \n4          10         hd  PT4M16S"
  },
  {
    "objectID": "technical-details/data-collection/overview.html#goals",
    "href": "technical-details/data-collection/overview.html#goals",
    "title": "Overview",
    "section": "",
    "text": "Identify the key factors affecting the popularity official music video( view count) on Youtube by analyzing various features of official mv on Youtube and characteristics of corresponding songs on Spotify."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#motivation",
    "href": "technical-details/data-collection/overview.html#motivation",
    "title": "Overview",
    "section": "",
    "text": "View count of music video on YouTube, the world’s largest video- sharing platform is crucial for measuring the popularity and success of mv and song.\nUnderstanding the key factors that drive and impact the mv popularity can help music producers, singers improve mv content and promotion strategies.\nNowdays, it seems that few people have quantified the key factors needed to achieve a successful mv based on mv data, and most people trust their own industry experience, intuition, and artistic aesthetics, etc.\nCombing the data of Youtube and Spotify provides us with a more comprehensive view, since the user behavior on these two platforms are likely to influence each other."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#objectives",
    "href": "technical-details/data-collection/overview.html#objectives",
    "title": "Overview",
    "section": "",
    "text": "Determine the primary factors that impact the mv view count, including: - YouTube features: days since published, like count, comment count, mean comment sentiment score, etc. - Spotify features: singer_popularity, duration , song_popularity, etc.\nExplore the correlation between Youtube and Spotify features and mv view count to identify the key traits.\nBased on the analysis, summarize and propose mv optimization suggestions to increase mv view count for singers, music producers and record labels.\nThis overview should give technical staff a clear understanding of the context and importance of the work, while guiding them on what to focus on in the details that follow."
  },
  {
    "objectID": "technical-details/data-collection/main.html#goals",
    "href": "technical-details/data-collection/main.html#goals",
    "title": "Data Collection",
    "section": "Goals",
    "text": "Goals\nIdentify the key factors affecting the popularity official music video( view count) on Youtube by analyzing various features of official mv on Youtube and characteristics of corresponding songs on Spotify."
  },
  {
    "objectID": "technical-details/data-collection/main.html#motivation",
    "href": "technical-details/data-collection/main.html#motivation",
    "title": "Data Collection",
    "section": "Motivation",
    "text": "Motivation\nView count of music video on YouTube, the world’s largest video- sharing platform is crucial for measuring the popularity and success of mv and song.\nUnderstanding the key factors that drive and impact the mv popularity can help music producers, singers improve mv content and promotion strategies.\nNowdays, it seems that few people have quantified the key factors needed to achieve a successful mv based on mv data, and most people trust their own industry experience, intuition, and artistic aesthetics, etc.\nCombing the data of Youtube and Spotify provides us with a more comprehensive view, since the user behavior on these two platforms are likely to influence each other."
  },
  {
    "objectID": "technical-details/data-collection/main.html#objectives",
    "href": "technical-details/data-collection/main.html#objectives",
    "title": "Data Collection",
    "section": "Objectives",
    "text": "Objectives\nDetermine the primary factors that impact the mv view count, including: - YouTube features: days since published, like count, comment count, mean comment sentiment score, etc. - Spotify features: singer_popularity, duration , song_popularity, etc.\nExplore the correlation between Youtube and Spotify features and mv view count to identify the key traits.\nBased on the analysis, summarize and propose mv optimization suggestions to increase mv view count for singers, music producers and record labels.\nThis overview should give technical staff a clear understanding of the context and importance of the work, while guiding them on what to focus on in the details that follow."
  },
  {
    "objectID": "technical-details/data-collection/main.html#youtube-data-collection",
    "href": "technical-details/data-collection/main.html#youtube-data-collection",
    "title": "Data Collection",
    "section": "1. YouTube Data Collection",
    "text": "1. YouTube Data Collection"
  },
  {
    "objectID": "technical-details/data-collection/main.html#acquiring-official-music-video-data",
    "href": "technical-details/data-collection/main.html#acquiring-official-music-video-data",
    "title": "Data Collection",
    "section": "1.1 Acquiring Official Music Video Data",
    "text": "1.1 Acquiring Official Music Video Data\n\nfrom googleapiclient.discovery import build\nimport pandas as pd\n\n# API Key\napi_key = \"AIzaSyDtKE-4QZj6EA-rwG7cj5gMJxdt4Fe14Nw\"\n\n# Initialize YouTube API client\nyoutube = build('youtube', 'v3', developerKey=api_key)\n\n# List to store data\nall_data = []\n\n# Read song data and fetch YouTube statistics\nwith open('song_data.txt', 'r') as file:\n    for line in file:\n        artist = line.strip()\n        query = f\"{artist} official music video\"\n\n        # Search request for the query\n        search_request = youtube.search().list(\n            part=\"snippet\",\n            q=query,\n            maxResults=5,\n            type=\"video\",\n            order='relevance'\n        )\n        search_response = search_request.execute()\n\n        for item in search_response['items']:\n            video_id = item['id']['videoId']\n            video_request = youtube.videos().list(\n                part=\"snippet,contentDetails,statistics\",\n                id=video_id\n            )\n            video_response = video_request.execute()\n\n            for video in video_response['items']:\n                snippet = video['snippet']\n                content_details = video['contentDetails']\n                statistics = video['statistics']\n\n                # Calculate days since video was published\n                published_at = dateutil.parser.parse(snippet['publishedAt'])\n                days_since_published = (datetime.now(published_at.tzinfo) - published_at).days\n\n                # Fetch channel details for subscriber count\n                channel_response = youtube.channels().list(\n                    part='statistics',\n                    id=snippet['channelId']\n                ).execute()\n                subscriber_count = channel_response['items'][0]['statistics'].get('subscriberCount', '0')\n\n                # Fetch comments\n                comments_request = youtube.commentThreads().list(\n                    part='snippet',\n                    videoId=video_id,\n                    order='relevance',\n                    maxResults=10\n                )\n                comments_response = comments_request.execute()\n\n                comments = [comment['snippet']['topLevelComment']['snippet']['textDisplay']\n                            for comment in comments_response.get('items', [])]\n\n                # Store all data in a dictionary\n                video_data = {\n                    'Video ID': video_id,\n                    'Title': snippet['title'],\n                    'Description': snippet['description'],\n                    'Published At': snippet['publishedAt'],\n                    'Days Since Published': days_since_published,\n                    'View Count': statistics.get('viewCount', '0'),\n                    'Like Count': statistics.get('likeCount', '0'),\n                    'Comment Count': statistics.get('commentCount', '0'),\n                    'Comments': comments,\n                    'Subscriber Count': subscriber_count,\n                    'Category ID': snippet['categoryId'],\n                    'Definition': content_details['definition'],\n                    'Duration': content_details['duration']\n                }\n\n                all_data.append(video_data)\n\n# Convert the list to a DataFrame\nfinal_df = pd.DataFrame(all_data)\n\n# Optionally save the DataFrame to a CSV file\nfinal_df.to_csv('Detailed_YouTube_Video_Data.csv', index=False)\n\n# Display the DataFrame\nprint(final_df.head())\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[7], line 14\n     11 all_data = []\n     13 # Read song data and fetch YouTube statistics\n---&gt; 14 with open('song_data.txt', 'r') as file:\n     15     for line in file:\n     16         artist = line.strip()\n\nFile ~/Library/Python/3.12/lib/python/site-packages/IPython/core/interactiveshell.py:324, in _modified_open(file, *args, **kwargs)\n    317 if file in {0, 1, 2}:\n    318     raise ValueError(\n    319         f\"IPython won't let you open fd={file} by default \"\n    320         \"as it is likely to crash IPython. If you know what you are doing, \"\n    321         \"you can use builtins' open.\"\n    322     )\n--&gt; 324 return io_open(file, *args, **kwargs)\n\nFileNotFoundError: [Errno 2] No such file or directory: 'song_data.txt'\n\n\n\n\nfrom googleapiclient.discovery import build\nimport pandas as pd\nfrom datetime import datetime\nimport dateutil.parser\n\n# API Key\napi_key = \"AIzaSyDtKE-4QZj6EA-rwG7cj5gMJxdt4Fe14Nw\"\n\n# Initialize YouTube API client\nyoutube = build('youtube', 'v3', developerKey=api_key)\n\n# List to store data\nall_data = []\n\n# Read song data and fetch YouTube statistics\n# We should have put 20*5 names of singers, but for the sake of presentation, we choose three singers as a demonstration here \nwith open('find.txt', 'r') as file:\n    for line in file:\n        artist = line.strip()\n        query = f\"{artist} official music video\"\n\n        # Search request for the query\n        search_request = youtube.search().list(\n            part=\"snippet\",\n            q=query,\n            maxResults=5,\n            type=\"video\",\n            order='relevance'\n        )\n        search_response = search_request.execute()\n\n        for item in search_response['items']:\n            video_id = item['id']['videoId']\n            video_request = youtube.videos().list(\n                part=\"snippet,contentDetails,statistics\",\n                id=video_id\n            )\n            video_response = video_request.execute()\n\n            for video in video_response['items']:\n                snippet = video['snippet']\n                content_details = video['contentDetails']\n                statistics = video['statistics']\n\n                # Calculate days since video was published\n                published_at = dateutil.parser.parse(snippet['publishedAt'])\n                days_since_published = (datetime.now(published_at.tzinfo) - published_at).days\n\n                # Fetch channel details for subscriber count\n                channel_response = youtube.channels().list(\n                    part='statistics',\n                    id=snippet['channelId']\n                ).execute()\n                subscriber_count = channel_response['items'][0]['statistics'].get('subscriberCount', '0')\n\n                # Fetch comments\n                comments_request = youtube.commentThreads().list(\n                    part='snippet',\n                    videoId=video_id,\n                    order='relevance',\n                    maxResults=10\n                )\n                comments_response = comments_request.execute()\n\n                comments = [comment['snippet']['topLevelComment']['snippet']['textDisplay']\n                            for comment in comments_response.get('items', [])]\n\n                # Store all data in a dictionary\n                video_data = {\n                    'Video ID': video_id,\n                    'Title': snippet['title'],\n                    'Description': snippet['description'],\n                    'Published At': snippet['publishedAt'],\n                    'Days Since Published': days_since_published,\n                    'View Count': statistics.get('viewCount', '0'),\n                    'Like Count': statistics.get('likeCount', '0'),\n                    'Comment Count': statistics.get('commentCount', '0'),\n                    'Comments': comments,\n                    'Subscriber Count': subscriber_count,\n                    'Category ID': snippet['categoryId'],\n                    'Definition': content_details['definition'],\n                    'Duration': content_details['duration']\n                }\n\n                all_data.append(video_data)\n\n# Convert the list to a DataFrame\nfinal_df = pd.DataFrame(all_data)\n\n# Optionally save the DataFrame to a CSV file\nfinal_df.to_csv('Example_Detailed_YouTube_Video_Data.csv', index=False)\n\n# Display the DataFrame\nprint(final_df.head())\n\n      Video ID                                              Title  \\\n0  1VQ_3sBZEm0    Foo Fighters - Learn To Fly (Official HD Video)   \n1  eBG7P-K-r1Y        Foo Fighters - Everlong (Official HD Video)   \n2  SBjQ9tuuTJQ                       Foo Fighters - The Pretender   \n3  EqWRaAF6_WY         Foo Fighters - My Hero (Official HD Video)   \n4  h_L4Rixya64  Foo Fighters - Best Of You (Official Music Video)   \n\n                                         Description          Published At  \\\n0  Foo Fighters' official music video for 'Learn ...  2009-10-03T04:46:13Z   \n1  \"Everlong\" by Foo Fighters \\nListen to Foo Fig...  2009-10-03T04:49:58Z   \n2  Watch the official music video for \"The Preten...  2009-10-03T04:46:14Z   \n3  \"My Hero\" by Foo Fighters \\nListen to Foo Figh...  2011-03-18T19:35:42Z   \n4  Watch the official music video for \"Best Of Yo...  2009-10-03T20:49:33Z   \n\n   Days Since Published View Count Like Count Comment Count  \\\n0                  5550  183921366     808172         33856   \n1                  5550  324414087    1821270         53201   \n2                  5550  588092620    2785700         92245   \n3                  5018   87531478     564448         26099   \n4                  5549  265573360    1281212         34999   \n\n                                            Comments Subscriber Count  \\\n0  [I’m just realising how great Dave grohls acti...          1290000   \n1  [Dad died today. \\r&lt;br&gt;1:20 am.\\r&lt;br&gt;A five da...          1290000   \n2  [So thankful for this awesome song. I&#39;ll b...          1290000   \n3  [My son and I was supposed to spend the summer...          1290000   \n4  [The emotion in his face and his voice transce...          1290000   \n\n  Category ID Definition Duration  \n0          10         hd  PT4M37S  \n1          10         hd  PT4M52S  \n2          10         hd  PT4M31S  \n3          10         hd   PT4M3S  \n4          10         hd  PT4M16S"
  },
  {
    "objectID": "technical-details/data-collection/main.html#merge-artist-name-and-music-genre-into-csv",
    "href": "technical-details/data-collection/main.html#merge-artist-name-and-music-genre-into-csv",
    "title": "Data Collection",
    "section": "1.2 Merge artist name and music genre into csv",
    "text": "1.2 Merge artist name and music genre into csv\nThe row of the initial find.csv(include artist name and genre) is repeated five times per row to correspond to the five mv chosen by each artist (python) Then merge these two columns into the csv (copy manually)\n\nimport pandas as pd\n\ninput_file = './find.csv'  \noutput_file = './Example_singer_info.csv'  \n\n# Load the input CSV file\ndf = pd.read_csv(input_file)\n\n# Create an empty DataFrame to store repeated rows\nrepeated_df = pd.DataFrame()\n\n# Repeat each row 5 times and append it to the new DataFrame\nfor i in range(len(df)):\n    repeated_df = pd.concat([repeated_df, pd.DataFrame([df.iloc[i]] * 5)], ignore_index=True)\n\n# Save the processed DataFrame to a new CSV file\nrepeated_df.to_csv(output_file, index=False)\n\nprint(f\"Data processed successfully and saved as {output_file}\")\n\nData processed successfully and saved as ./Example_singer_info.csv"
  },
  {
    "objectID": "technical-details/data-collection/main.html#data-preprocessing",
    "href": "technical-details/data-collection/main.html#data-preprocessing",
    "title": "Data Collection",
    "section": "1.3 Data preprocessing",
    "text": "1.3 Data preprocessing\nExtract the song name from the csv’s title and generate a new CSV file containing the singer’s and song’s name.\n\nimport pandas as pd\nimport re\n\n# Load the CSV file with YouTube video data\ndf = pd.read_csv('./Example_Detailed_YouTube_Video_Data.csv', encoding='MacRoman')\n\n# Function to extract the song name from the title\ndef extract_song_name(title):\n    # Use regular expression to find text between \" - \" and \"(\"\n    match = re.search(r' - (.*?) \\(.*\\)', title)\n    if match:\n        return match.group(1) \n    else:\n        return title  \n\n# Create a new DataFrame with extracted song names\nnew_df = pd.DataFrame({\n    'Extracted Song Name': df['Title'].apply(extract_song_name)\n})\n\n# Save the new DataFrame to a CSV file in the current directory\noutput_file = './Example_extracted_song_names.csv'\nnew_df.to_csv(output_file, index=False, encoding='MacRoman')\n\nprint(f\"Song titles extracted and saved to {output_file}\")\n\nSong titles extracted and saved to ./Example_extracted_song_names.csv\n\n\n\n\ninput_file = './Example_extracted_song_names.csv'\ndf = pd.read_csv(input_file, encoding='MacRoman')\n\n# Function to remove content inside brackets (e.g., [example])\ndef remove_brackets(text):\n    return re.sub(r'\\[.*?\\]', '', text)\n\n# Apply the function to the 'Extracted Song Name' column\ndf['Extracted Song Name'] = df['Extracted Song Name'].apply(remove_brackets)\n\n\noutput_file = './Example_extracted_song_names_cleaned.csv'\ndf.to_csv(output_file, index=False)\n\nprint(f\"Processed data saved to {output_file}\")\n\nProcessed data saved to ./Example_extracted_song_names_cleaned.csv\n\n\nManually merge example_extracted_song_name_cleaned.csv with example_artist_info.csv"
  },
  {
    "objectID": "technical-details/data-collection/main.html#spotify-collection",
    "href": "technical-details/data-collection/main.html#spotify-collection",
    "title": "Data Collection",
    "section": "2. Spotify Collection",
    "text": "2. Spotify Collection"
  },
  {
    "objectID": "technical-details/data-collection/main.html#acquiring-track-information",
    "href": "technical-details/data-collection/main.html#acquiring-track-information",
    "title": "Data Collection",
    "section": "2.1 Acquiring Track Information",
    "text": "2.1 Acquiring Track Information\n\nimport pandas as pd\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\n\n# Set up Spotify client credentials\nclient_credentials_manager = SpotifyClientCredentials(\n    client_id='3e1596de002340b898f5d10c9aeae4ea',\n    client_secret='526fa44678974475b0f6ba5d8efd16c4'\n)\nsp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n\n# Input and output file paths\ninput_file = './Example_extracted_song_names_cleaned.csv'\noutput_file = './Example_spotify_track_info.csv'\n\n# Load the input CSV file\ndf = pd.read_csv(input_file, encoding='MacRoman')\n\n# Initialize a list to store results\nresults = []\n\n# Process each row in the DataFrame\nfor _, row in df.iterrows():\n    song = row['Extracted Song Name']\n    artist = row['singer']\n    query = f'{song} {artist}'  # Concatenate song and artist for the search query\n\n    try:\n        # Search for the track on Spotify\n        result = sp.search(q=query, limit=1, type='track')\n        tracks = result.get('tracks', {}).get('items', [])\n\n        if tracks:\n            # If a track is found, extract its details\n            track = tracks[0]\n            track_info = {\n                'Track Name': track['name'],\n                'Artist Name': track['artists'][0]['name'],\n                'Album Name': track['album']['name'],\n                'Popularity': track['popularity'],\n                'Duration (ms)': track['duration_ms'],\n                'Track ID': track['id'],\n                'Spotify URL': track['external_urls']['spotify']\n            }\n            print(f\"Found: {track_info['Track Name']} by {track_info['Artist Name']}\")\n        else:\n            # If no track is found, append placeholders\n            print(f\"Track not found: {query}\")\n            track_info = {\n                'Track Name': song,\n                'Artist Name': artist,\n                'Album Name': None,\n                'Popularity': None,\n                'Duration (ms)': None,\n                'Track ID': None,\n                'Spotify URL': None\n            }\n\n        # Append the result to the list\n        results.append(track_info)\n\n    except Exception as e:\n        # Handle exceptions during the search\n        print(f\"Error processing query '{query}': {e}\")\n        results.append({\n            'Track Name': song,\n            'Artist Name': artist,\n            'Album Name': None,\n            'Popularity': None,\n            'Duration (ms)': None,\n            'Track ID': None,\n            'Spotify URL': None\n        })\n\n# Create a DataFrame from the results\noutput_df = pd.DataFrame(results)\n\n# Save the output DataFrame to a CSV file\noutput_df.to_csv(output_file, index=False, encoding='utf-8')\n\nprint(f\"Processed data saved to: {output_file}\")\n\nFound: Learn to Fly by Foo Fighters\nFound: Everlong by Foo Fighters\nFound: The Pretender by Foo Fighters\nFound: My Hero by Foo Fighters\nFound: Best of You by Foo Fighters\nFound: Mr. Brightside by The Killers\nFound: When You Were Young by The Killers\nFound: Mr. Brightside by The Killers\nFound: Somebody Told Me by The Killers\nFound: One Empty Grave by A Sound of Thunder\nFound: Basket Case by Green Day\nFound: When I Come Around by Green Day\nFound: American Idiot by Green Day\nFound: Boulevard of Broken Dreams by Green Day\nFound: Wake Me up When September Ends by Green Day\nProcessed data saved to: ./Example_spotify_track_info.csv"
  },
  {
    "objectID": "technical-details/data-collection/main.html#obtaining-artist-information",
    "href": "technical-details/data-collection/main.html#obtaining-artist-information",
    "title": "Data Collection",
    "section": "2.2 Obtaining Artist Information",
    "text": "2.2 Obtaining Artist Information\n\nimport pandas as pd\nimport requests\n\n# Function to obtain an access token for Spotify API\ndef get_access_token(client_id, client_secret):\n    auth_url = 'https://accounts.spotify.com/api/token'\n    auth_data = {\n        'grant_type': 'client_credentials',\n        'client_id': client_id,\n        'client_secret': client_secret\n    }\n    response = requests.post(auth_url, data=auth_data)\n    if response.status_code == 200:\n        return response.json()['access_token']\n    else:\n        raise Exception('Failed to obtain access token')\n\n# Function to get the Spotify artist ID using the artist name\ndef get_artist_id(artist_name, access_token):\n    search_url = f'https://api.spotify.com/v1/search?q={artist_name}&type=artist&limit=1'\n    headers = {'Authorization': f'Bearer {access_token}'}\n    response = requests.get(search_url, headers=headers)\n    if response.status_code == 200:\n        search_results = response.json()\n        artists = search_results['artists']['items']\n        if artists:\n            return artists[0]['id']\n        else:\n            return None\n    else:\n        return None\n\n# Function to retrieve the artist's followers and popularity\ndef get_artist_followers(artist_id, access_token):\n    artist_url = f'https://api.spotify.com/v1/artists/{artist_id}'\n    headers = {'Authorization': f'Bearer {access_token}'}\n    response = requests.get(artist_url, headers=headers)\n    if response.status_code == 200:\n        artist_data = response.json()\n        return artist_data['followers']['total'], artist_data['popularity']\n    else:\n        return None, None\n\n# Load the input CSV file \ninput_file = './Example_singer_info.csv' \ndf = pd.read_csv(input_file)\n\n# Spotify API credentials\nclient_id = '31aba57b31344fdebf98f51375d07834' \nclient_secret = '2c4f929786784910bc9a843518785cae'  \n\n# Obtain Spotify API access token\naccess_token = get_access_token(client_id, client_secret)\n\n# Initialize lists to store followers and popularity data\nfollowers_list = []\npopularity_list = []\n\n# Process each artist in the DataFrame\nfor artist_name in df['artist']:\n    artist_id = get_artist_id(artist_name, access_token)\n    if artist_id:\n        followers, popularity = get_artist_followers(artist_id, access_token)\n        followers_list.append(followers)\n        popularity_list.append(popularity)\n    else:\n        # If the artist is not found, append None\n        followers_list.append(None)\n        popularity_list.append(None)\n\n# Add followers and popularity data to the DataFrame\ndf['Followers'] = followers_list\ndf['Popularity'] = popularity_list\n\n# Save the updated DataFrame to a new CSV file\noutput_file = './Example_artist_data_with_followers_and_popularity.csv'  \ndf.to_csv(output_file, index=False)\n\nprint(f\"Processed data saved to: {output_file}\")\n\nProcessed data saved to: ./Example_artist_data_with_followers_and_popularity.csv"
  },
  {
    "objectID": "technical-details/data-collection/main.html#data-integration-and-cleansing",
    "href": "technical-details/data-collection/main.html#data-integration-and-cleansing",
    "title": "Data Collection",
    "section": "3. Data integration and cleansing",
    "text": "3. Data integration and cleansing\nFirst manually merge Spotify and YouTube csv. Then using Spotify and YouTube artists as the matching key, match to verify artist match, if not, then delete the mismatched rows.\n\nimport pandas as pd\n\ninput_file = './Example_Detailed_YouTube_Video_Data.csv' \ndf = pd.read_csv(input_file, encoding='MacRoman')\n\n# Filter rows where 'Artist Name' matches 'Artist'\ndf_cleaned = df[df['Artist Name'] == df['artist']]\n\n# Save the cleaned DataFrame to a new CSV file\noutput_file = './Example_spotify_youtube.csv'  \ndf_cleaned.to_csv(output_file, index=False)\nprint(f\"Cleaned data saved to: {output_file}\")\n\nCleaned data saved to: ./Example_spotify_youtube.csv\n\n\nThen we completed all the data collection steps!"
  },
  {
    "objectID": "technical-details/eda/eda_test.html",
    "href": "technical-details/eda/eda_test.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nfile_path = \"/Users/zhouyiqin/Desktop/final_project/Updated_Data_with_Sentiments.csv\"\ndf = pd.read_csv(file_path)\n\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\n\n\nnumeric_cols = ['View Count', 'Like Count', 'Comment Count', \n                'Duration_seconds', 'Mean Sentiment Score', 'singer_popularity']\n                \nprint(\"\\nNumerical Variables Summary Statistics:\")\nprint(df[numeric_cols].describe())\n\n\nNumerical Variables Summary Statistics:\n         View Count    Like Count  Comment Count  Duration_seconds  \\\ncount  3.750000e+02  3.750000e+02   3.750000e+02        375.000000   \nmean   3.702933e+08  2.498181e+06   9.775152e+04        292.744000   \nstd    7.043627e+08  4.194654e+06   1.783573e+05        397.971143   \nmin    2.760000e+02  0.000000e+00   0.000000e+00         21.000000   \n25%    1.374476e+07  1.327810e+05   4.136500e+03        200.500000   \n50%    9.101476e+07  7.850650e+05   2.807200e+04        239.000000   \n75%    3.711109e+08  2.878034e+06   1.032220e+05        284.500000   \nmax    6.379786e+09  3.378610e+07   1.184861e+06       4835.000000   \n\n       Mean Sentiment Score  singer_popularity  \ncount            375.000000         375.000000  \nmean               0.325594          80.120000  \nstd                0.225798          11.509773  \nmin               -0.336767          39.000000  \n25%                0.169655          77.000000  \n50%                0.319200          81.000000  \n75%                0.484267          87.000000  \nmax                0.817367         100.000000  \n\n\n\ndf['Log10_View_Count'] = np.log10(df['View Count'].replace(0, np.nan))\n# calcultate Like Ratio\ndf['Like_Ratio'] = df['Like Count'] / df['View Count'] * 100\nfig, axes = plt.subplots(2, 2, figsize=(10, 8)) \n\n# subplot1: Distribution of Log10(View Count)\nsns.histplot(df['Log10_View_Count'], kde=True, ax=axes[0, 0])\naxes[0, 0].set_title('Distribution of Log10(View Count)')\naxes[0, 0].set_xlabel('Log10(View Count)')\n\n# subplot2: Like Ratio distribution\nsns.histplot(df['Like_Ratio'], kde=True, ax=axes[0, 1])\naxes[0, 1].set_title('Distribution of Like Ratio (%)')\naxes[0, 1].set_xlabel('Like Ratio (%)')\naxes[0, 1].set_xlim(-1, 7)\n\n# subplot3: Distribution of Video Duration\nsns.histplot(df['Duration (ms)'], kde=True, ax=axes[1, 0])\naxes[1, 0].set_title('Distribution of Video Duration')\naxes[1, 0].set_xlabel('Duration (ms)')\n\n\n\n# subplot4: Distribution of Mean Sentiment Score\nsns.histplot(df['Mean Sentiment Score'], kde=True, ax=axes[1, 1])\naxes[1, 1].set_title('Distribution of Mean Sentiment Score')\naxes[1, 1].set_xlabel('Sentiment Score')\n\n# 调整布局\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n这四张图分别展现了四个不同指标的数据分布情况:\n\nLog10(View Count)的分布: View Count取对数后呈现出明显的正态分布特征,主要集中在6-8之间。\nLike Ratio(点赞率)的分布: 点赞率主要分布在0-3%之间,1%以下的数据占比最大。整体呈现出偏右的长尾分布。\nVideo Duration(视频时长)的分布: 视频时长主要集中在0-600秒,即10分钟以内。600秒以上的长视频占比较小。\nMean Sentiment Score(平均情感得分)的分布: 平均情感得分主要分布在0-0.6之间,得分偏低的视频占比较大,整体呈现出偏左的分布。\n\n总的来说,这四张图从不同角度展现了该数据集的基本特征: - View Count高度集中,呈正态分布 - 点赞率和平均情感得分普遍较低,分布偏右偏左 - 大部分视频时长较短,10分钟以内\n这些分布特点为后续的数据分析提供了重要参考。\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\n\n\n# 2. Categorical Variables Analysis\n# Genre distribution\nplt.figure(figsize=(10, 6))\ngenre_counts = df['genre'].value_counts()\nsns.barplot(x=genre_counts.index, y=genre_counts.values)\nplt.title('Distribution of Music Genres')\nplt.xticks(rotation=45)\n\n([0, 1, 2, 3, 4],\n [Text(0, 0, 'rock'),\n  Text(1, 0, 'pop'),\n  Text(2, 0, 'jazz'),\n  Text(3, 0, 'hip-pop'),\n  Text(4, 0, 'electronic')])\n\n\n\n\n\n\n\n\n\n\n# Print category counts\nprint(\"\\nGenre Distribution:\")\nprint(df['genre'].value_counts())\n\n\nGenre Distribution:\ngenre\nrock          88\npop           88\njazz          72\nhip-pop       70\nelectronic    57\nName: count, dtype: int64\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\n\nCorrelation Analysis\n\nnumerical_columns = ['View Count', 'Like Count', 'Comment Count','singer_followers','singer_popularity', 'Duration (ms)','Duration_seconds', 'Mean Sentiment Score']\n# Calculate correlation matrix\ncorrelation_matrix = df[numerical_columns].corr()\n\n# Create correlation heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, \n            annot=True,  # Show correlation values\n            cmap='coolwarm', \n            center=0,  \n            fmt='.2f') \nplt.title('Correlation Matrix of Numerical Variables')\nplt.tight_layout()\n\n\n\n\n\n\n\n\nView Count, Like Count, Comment Count之间的相关性非常高,都在0.8以上,说明视频的播放量、点赞量、评论数有很强的正相关。 singer_followers与singer_popularity的相关性达到0.65,代表歌手的粉丝数和歌手本身的知名度有较强的正相关。 Duration与其他变量的相关性都比较弱,说明视频时长与播放量等指标关系不大。 Mean Sentiment Score与其他变量的相关性系数为负值,尤其与歌手知名度的负相关性更强,说明知名度高的歌手其视频评论的平均情感得分反而更低。\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\n\n# 2. Feature Pairing Analysis\n# Create scatter plots for key relationships\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# View Count vs Like Count (with log transformation)\nsns.scatterplot(data=df, \n                x=np.log10(df['View Count']), \n                y=np.log10(df['Like Count']),\n                hue='genre',\n                ax=axes[0,0])\naxes[0,0].set_title('Log10(View Count) vs Log10(Like Count) by Genre')\naxes[0,0].set_xlabel('Log10(View Count)')\naxes[0,0].set_ylabel('Log10(Like Count)')\n\n# Views vs Sentiment Score\nsns.scatterplot(data=df,\n                x='Mean Sentiment Score',\n                y=np.log10(df['View Count']),\n                hue='genre',\n                ax=axes[0,1])\naxes[0,1].set_title('Sentiment Score vs Log10(View Count) by Genre')\n\n# Duration vs Engagement\ndf['Engagement_Rate'] = (df['Like Count'] + df['Comment Count']) / df['View Count'] * 100\nsns.scatterplot(data=df,\n                x='Duration_seconds',\n                y='Engagement_Rate',\n                hue='genre',\n                ax=axes[1,0])\naxes[1,0].set_title('Duration vs Engagement Rate by Genre')\n\n# Popularity Analysis\nsns.boxplot(data=df,\n            x='genre',\n            y='singer_popularity',\n            ax=axes[1,1])\naxes[1,1].set_title('Singer Popularity Distribution by Genre')\naxes[1,1].set_xticklabels(axes[1,1].get_xticklabels(), rotation=45)\n\nplt.tight_layout()\n\n/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log10\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n/var/folders/qx/qwpm5zm52fzd9w3sfybpybzh0000gn/T/ipykernel_33101/819984757.py:38: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  axes[1,1].set_xticklabels(axes[1,1].get_xticklabels(), rotation=45)\n\n\n\n\n\n\n\n\n\n第一张图显示了不同流派在Log10(View Count)和Log10(Like Count)两个指标上的分布。可以看出,除个别极端值外,大部分流派在两个指标上呈现出强正相关,且pop音乐的播放量和点赞量普遍更高。 第二张图揭示了不同流派的平均情感得分(Sentiment Score)与Log10(View Count)之间的关系。总体来看情感得分都集中在0.4-0.6之间,且与播放量关联性不强。 第三张图对比了不同流派视频的平均时长(Duration)和互动率(Engagement Rate)。可以看出,rock和electronic音乐的平均时长更长,但互动率相对较低。而pop音乐视频虽然时长较短,但互动率普遍更高。 第四张图则展现了各流派歌手粉丝数量(Singer Popularity)的分布情况。可见rock和pop歌手的粉丝数普遍高于hip-hop、electronic和jazz歌手。\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\n\n\n# 3. Cross-tabulation Analysis\n# Create a cross-tab of genre and popularity categories\ndf['popularity_category'] = pd.qcut(df['singer_popularity'], \n                                  q=3, \n                                  labels=['Low', 'Medium', 'High'])\ngenre_popularity_crosstab = pd.crosstab(df['genre'], \n                                       df['popularity_category'], \n                                       normalize='index') * 100\n\nprint(\"\\nGenre-Popularity Cross-tabulation (%):\")\nprint(genre_popularity_crosstab)\n\n# Calculate summary statistics by genre\ngenre_summary = df.groupby('genre').agg({\n    'View Count': 'mean',\n    'Like Count': 'mean',\n    'Comment Count': 'mean',\n    'Mean Sentiment Score': 'mean',\n    'singer_followers': 'mean'\n}).round(2)\n\nprint(\"\\nSummary Statistics by Genre:\")\nprint(genre_summary)\n\n\nGenre-Popularity Cross-tabulation (%):\npopularity_category        Low     Medium       High\ngenre                                               \nelectronic           57.894737  42.105263   0.000000\nhip-pop              12.857143  24.285714  62.857143\njazz                 86.111111   6.944444   6.944444\npop                   4.545455  38.636364  56.818182\nrock                 19.318182  55.681818  25.000000\n\nSummary Statistics by Genre:\n              View Count  Like Count  Comment Count  Mean Sentiment Score  \\\ngenre                                                                       \nelectronic  4.812136e+08  3444539.65      113885.77                  0.27   \nhip-pop     2.120770e+08  1923365.79       80653.13                  0.18   \njazz        1.062045e+07    91090.62        2455.99                  0.47   \npop         6.973413e+08  4625445.59      204115.83                  0.32   \nrock        3.915309e+08  2184612.99       72506.67                  0.37   \n\n            singer_followers  \ngenre                         \nelectronic       11088701.67  \nhip-pop          30818759.69  \njazz              1741634.22  \npop              52305955.89  \nrock             21119041.56  \n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\n\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n# 1. Calculate Skewness and Kurtosis\nprint(\"Skewness and Kurtosis Analysis:\")\nfor col in numerical_columns:\n    skew = stats.skew(df[col].dropna())\n    kurt = stats.kurtosis(df[col].dropna())\n    print(f\"\\n{col}:\")\n    print(f\"Skewness: {skew:.2f}\")\n    print(f\"Kurtosis: {kurt:.2f}\")\n\nSkewness and Kurtosis Analysis:\n\nView Count:\nSkewness: 3.68\nKurtosis: 18.98\n\nLike Count:\nSkewness: 3.03\nKurtosis: 12.53\n\nComment Count:\nSkewness: 3.12\nKurtosis: 11.26\n\nsinger_followers:\nSkewness: 1.87\nKurtosis: 2.95\n\nsinger_popularity:\nSkewness: -1.24\nKurtosis: 2.09\n\nDuration (ms):\nSkewness: 1.70\nKurtosis: 5.24\n\nMean Sentiment Score:\nSkewness: -0.09\nKurtosis: -0.36\n\n\n\n#2. Visualization of distributions before and after transformations\ndef plot_distributions(data, column, transformations):\n    \"\"\"\n    Plot original and transformed distributions\n    Args:\n        data: DataFrame\n        column: column name to analyze\n        transformations: list of transformation functions\n    \"\"\"\n    n_plots = len(transformations) + 1\n    plt.figure(figsize=(15, 5))\n    \n    # Original distribution\n    plt.subplot(1, n_plots, 1)\n    sns.histplot(data[column], kde=True)\n    plt.title(f'Original {column}')\n    \n    # Transformed distributions\n    for i, (name, func) in enumerate(transformations.items(), 1):\n        plt.subplot(1, n_plots, i+1)\n        transformed_data = func(data[column])\n        sns.histplot(transformed_data, kde=True)\n        plt.title(f'{name} Transformed')\n\n# Define transformations\ntransformations = {\n    'Log': lambda x: np.log1p(x),\n    'Square Root': lambda x: np.sqrt(x),\n    'Box-Cox': lambda x: stats.boxcox(x - x.min() + 1)[0] if (x - x.min() + 1 &gt; 0).all() else x\n}\n\n\n# 3. Apply and visualize transformations for highly skewed variables\nhighly_skewed = ['View Count', 'Like Count', 'Comment Count']\nfor col in highly_skewed:\n    plot_distributions(df, col, transformations)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# 4. Normalization\ndef normalize_and_plot(data, column):\n    \"\"\"\n    Apply different normalization techniques and plot results\n    \"\"\"\n    # Create scalers\n    standard_scaler = StandardScaler()\n    minmax_scaler = MinMaxScaler()\n    \n    # Reshape data for scalers\n    data_reshaped = data[column].values.reshape(-1, 1)\n    \n    # Apply scalers\n    data_standard = standard_scaler.fit_transform(data_reshaped)\n    data_minmax = minmax_scaler.fit_transform(data_reshaped)\n    \n    # Plot comparisons\n    plt.figure(figsize=(15, 5))\n    \n    plt.subplot(131)\n    sns.histplot(data[column], kde=True)\n    plt.title(f'Original {column}')\n    \n    plt.subplot(132)\n    sns.histplot(data_standard.flatten(), kde=True)\n    plt.title('Standard Scaled')\n    \n    plt.subplot(133)\n    sns.histplot(data_minmax.flatten(), kde=True)\n    plt.title('MinMax Scaled')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return pd.DataFrame({\n        'original': data[column],\n        'standard_scaled': data_standard.flatten(),\n        'minmax_scaled': data_minmax.flatten()\n    })\n\n# 5. Apply normalization and generate summary statistics\nnormalized_results = {}\nfor col in numerical_columns:\n    print(f\"\\nNormalization results for {col}:\")\n    normalized_data = normalize_and_plot(df, col)\n    print(\"\\nSummary statistics:\")\n    print(normalized_data.describe())\n    normalized_results[col] = normalized_data\n\n# 6. Calculate correlation between original and transformed data\nfor col in numerical_columns:\n    if col in normalized_results:\n        corr = normalized_results[col].corr()\n        print(f\"\\nCorrelation matrix for {col}:\")\n        print(corr)\n\n\nNormalization results for View Count:\n\n\n\n\n\n\n\n\n\n\nSummary statistics:\n           original  standard_scaled  minmax_scaled\ncount  3.750000e+02     3.750000e+02     375.000000\nmean   3.702933e+08     7.105427e-18       0.058042\nstd    7.043627e+08     1.001336e+00       0.110405\nmin    2.760000e+02    -5.264160e-01       0.000000\n25%    1.374476e+07    -5.068765e-01       0.002154\n50%    9.101476e+07    -3.970280e-01       0.014266\n75%    3.711109e+08     1.162287e-03       0.058170\nmax    6.379786e+09     8.543214e+00       1.000000\n\nNormalization results for Like Count:\n\n\n\n\n\n\n\n\n\n\nSummary statistics:\n           original  standard_scaled  minmax_scaled\ncount  3.750000e+02     3.750000e+02     375.000000\nmean   2.498181e+06     3.789561e-17       0.073941\nstd    4.194654e+06     1.001336e+00       0.124153\nmin    0.000000e+00    -5.963588e-01       0.000000\n25%    1.327810e+05    -5.646617e-01       0.003930\n50%    7.850650e+05    -4.089503e-01       0.023236\n75%    2.878034e+06     9.067732e-02       0.085184\nmax    3.378610e+07     7.468964e+00       1.000000\n\nNormalization results for Comment Count:\n\n\n\n\n\n\n\n\n\n\nSummary statistics:\n           original  standard_scaled  minmax_scaled\ncount  3.750000e+02     3.750000e+02     375.000000\nmean   9.775152e+04    -1.421085e-17       0.082500\nstd    1.783573e+05     1.001336e+00       0.150530\nmin    0.000000e+00    -5.487980e-01       0.000000\n25%    4.136500e+03    -5.255748e-01       0.003491\n50%    2.807200e+04    -3.911958e-01       0.023692\n75%    1.032220e+05     3.071246e-02       0.087117\nmax    1.184861e+06     6.103266e+00       1.000000\n\nNormalization results for singer_followers:\n\n\n\n\n\n\n\n\n\n\nSummary statistics:\n           original  standard_scaled  minmax_scaled\ncount  3.750000e+02     3.750000e+02     375.000000\nmean   2.500311e+07    -1.515825e-16       0.194858\nstd    2.918020e+07     1.001336e+00       0.228103\nmin    7.581900e+04    -8.553950e-01       0.000000\n25%    4.449456e+06    -7.053110e-01       0.034189\n50%    1.493754e+07    -3.454062e-01       0.116175\n75%    3.250965e+07     2.575915e-01       0.253537\nmax    1.280014e+08     3.534450e+00       1.000000\n\nNormalization results for singer_popularity:\n\n\n\n\n\n\n\n\n\n\nSummary statistics:\n         original  standard_scaled  minmax_scaled\ncount  375.000000     3.750000e+02     375.000000\nmean    80.120000    -3.789561e-16       0.674098\nstd     11.509773     1.001336e+00       0.188685\nmin     39.000000    -3.577389e+00       0.000000\n25%     77.000000    -2.714361e-01       0.622951\n50%     81.000000     7.655891e-02       0.688525\n75%     87.000000     5.985515e-01       0.786885\nmax    100.000000     1.729535e+00       1.000000\n\nNormalization results for Duration (ms):\n\n\n\n\n\n\n\n\n\n\nSummary statistics:\n            original  standard_scaled  minmax_scaled\ncount     375.000000     3.750000e+02     375.000000\nmean   233642.093333     1.515825e-16       0.293792\nstd     68067.837578     1.001336e+00       0.123778\nmin     72080.000000    -2.376716e+00       0.000000\n25%    191973.000000    -6.129879e-01       0.218019\n50%    218948.000000    -2.161627e-01       0.267072\n75%    266033.000000     4.764979e-01       0.352693\nmax    622000.000000     5.713076e+00       1.000000\n\nNormalization results for Mean Sentiment Score:\n\n\n\n\n\n\n\n\n\n\nSummary statistics:\n         original  standard_scaled  minmax_scaled\ncount  375.000000     3.750000e+02     375.000000\nmean     0.325594     9.473903e-18       0.573903\nstd      0.225798     1.001336e+00       0.195643\nmin     -0.336767    -2.937344e+00       0.000000\n25%      0.169655    -6.915375e-01       0.438790\n50%      0.319200    -2.835663e-02       0.568363\n75%      0.484267     7.036575e-01       0.711385\nmax      0.817367     2.180842e+00       1.000000\n\nCorrelation matrix for View Count:\n                 original  standard_scaled  minmax_scaled\noriginal              1.0              1.0            1.0\nstandard_scaled       1.0              1.0            1.0\nminmax_scaled         1.0              1.0            1.0\n\nCorrelation matrix for Like Count:\n                 original  standard_scaled  minmax_scaled\noriginal              1.0              1.0            1.0\nstandard_scaled       1.0              1.0            1.0\nminmax_scaled         1.0              1.0            1.0\n\nCorrelation matrix for Comment Count:\n                 original  standard_scaled  minmax_scaled\noriginal              1.0              1.0            1.0\nstandard_scaled       1.0              1.0            1.0\nminmax_scaled         1.0              1.0            1.0\n\nCorrelation matrix for singer_followers:\n                 original  standard_scaled  minmax_scaled\noriginal              1.0              1.0            1.0\nstandard_scaled       1.0              1.0            1.0\nminmax_scaled         1.0              1.0            1.0\n\nCorrelation matrix for singer_popularity:\n                 original  standard_scaled  minmax_scaled\noriginal              1.0              1.0            1.0\nstandard_scaled       1.0              1.0            1.0\nminmax_scaled         1.0              1.0            1.0\n\nCorrelation matrix for Duration (ms):\n                 original  standard_scaled  minmax_scaled\noriginal              1.0              1.0            1.0\nstandard_scaled       1.0              1.0            1.0\nminmax_scaled         1.0              1.0            1.0\n\nCorrelation matrix for Mean Sentiment Score:\n                 original  standard_scaled  minmax_scaled\noriginal              1.0              1.0            1.0\nstandard_scaled       1.0              1.0            1.0\nminmax_scaled         1.0              1.0            1.0\n\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\n\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats import f_oneway\n# 2. ANOVA test for view counts across genres\ngenres = df['Genre'].unique()\ngenre_views = [df[df['Genre'] == genre]['View Count'] for genre in genres]\nf_stat, p_value = f_oneway(*genre_views)\nprint(\"\\n=== One-way ANOVA Test for View Counts Across Genres ===\")\nprint(f\"F-statistic: {f_stat:.4f}\")\nprint(f\"p-value: {p_value}\")\n\n\n=== One-way ANOVA Test for View Counts Across Genres ===\nF-statistic: 11.9445\np-value: 3.871891640564041e-09\n\n\n\n#Chi-square test\n# Create contingency table\ncontingency_table = pd.crosstab(df['genre'], df['singer'])\nchi2, p_value, dof, expected = chi2_contingency(contingency_table)\nprint(\"\\n=== Chi-square Test for Independence ===\")\nprint(f\"Chi-square statistic: {chi2:.4f}\")\nprint(f\"p-value: {p_value}\")\n\n\n=== Chi-square Test for Independence ===\nChi-square statistic: 1500.0000\np-value: 3.013591313202591e-141\n\n\n\n#t-test\nprint(\"\\n=== T-tests for Sentiment Scores Between Genres ===\")\nfor genre1 in genres:\n    for genre2 in genres:\n        if genre1 &lt; genre2:  # avoid duplicate comparisons\n            scores1 = df[df['genre'] == genre1]['Mean Sentiment Score']\n            scores2 = df[df['genre'] == genre2]['Mean Sentiment Score']\n            t_stat, p_value = stats.ttest_ind(scores1, scores2)\n            print(f\"{genre1} vs {genre2}:\")\n            print(f\"t-statistic: {t_stat}\")\n            print(f\"p-value: {p_value}\\n\")\n\n\n=== T-tests for Sentiment Scores Between Genres ===\nhip-pop vs rock:\nt-statistic: -5.945093374465196\np-value: 1.7465267629304015e-08\n\nhip-pop vs pop:\nt-statistic: -4.579262357938638\np-value: 9.483338020106626e-06\n\nhip-pop vs jazz:\nt-statistic: -9.773706288584009\np-value: 1.606995348433881e-17\n\npop vs rock:\nt-statistic: -1.609084046893032\np-value: 0.10941079187324153\n\nelectronic vs rock:\nt-statistic: -2.804854647333346\np-value: 0.005734763289028276\n\nelectronic vs hip-pop:\nt-statistic: 2.7419653704568594\np-value: 0.007003998170075622\n\nelectronic vs pop:\nt-statistic: -1.4555040194036937\np-value: 0.14772159941408194\n\nelectronic vs jazz:\nt-statistic: -5.829205742148866\np-value: 4.3325958156418246e-08\n\njazz vs rock:\nt-statistic: 2.908976476089868\np-value: 0.004149095428953946\n\njazz vs pop:\nt-statistic: 4.733587350391504\np-value: 4.868298963617719e-06\n\n\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\n\n\n# 1. Statistical Tests and Visualizations for Engagement Metrics\ndef analyze_engagement_metrics():\n    # Calculate engagement rate\n    df['engagement_rate'] = (df['Like Count'] + df['Comment Count']) / df['View Count'] * 100\n    \n    # Create visualization for engagement metrics\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # Scatter plot: Views vs Likes with regression line\n    sns.regplot(data=df, x='View Count', y='Like Count', ax=axes[0,0])\n    axes[0,0].set_title('Correlation: Views vs Likes')\n    \n    # Calculate correlation\n    correlation = df['View Count'].corr(df['Like Count'])\n    print(f\"\\nCorrelation between Views and Likes: {correlation:.3f}\")\n    \n    # Box plot: Engagement rate by genre\n    sns.boxplot(data=df, x='genre', y='engagement_rate', ax=axes[0,1])\n    axes[0,1].set_title('Engagement Rate by Genre')\n    axes[0,1].tick_labels = rotation=45\n    \n    # Violin plot: Sentiment distribution by genre\n    sns.violinplot(data=df, x='genre', y='Mean Sentiment Score', ax=axes[1,0])\n    axes[1,0].set_title('Sentiment Distribution by Genre')\n    axes[1,0].tick_labels = rotation=45\n    \n    # Scatter plot: Duration vs Views\n    sns.scatterplot(data=df, x='Duration_seconds', y='View Count', hue='genre', ax=axes[1,1])\n    axes[1,1].set_title('Duration vs Views by Genre')\n    \n    plt.tight_layout()\n    \n    return fig\n\nfig = analyze_engagement_metrics()\n\n\nCorrelation between Views and Likes: 0.931\n\n\n\n\n\n\n\n\n\n\n# 2. Statistical Tests\ndef perform_statistical_tests():\n    # T-test for views between different genres\n    genres = df['genre'].unique()\n    print(\"\\nT-test results for View Count between genres:\")\n    for i in range(len(genres)):\n        for j in range(i+1, len(genres)):\n            genre1_views = df[df['genre'] == genres[i]]['View Count']\n            genre2_views = df[df['genre'] == genres[j]]['View Count']\n            t_stat, p_value = stats.ttest_ind(genre1_views, genre2_views)\n            print(f\"{genres[i]} vs {genres[j]}: p-value = {p_value:.4f}\")\n    \n    # ANOVA test for sentiment scores across genres\n    genre_groups = [group['Mean Sentiment Score'].values for name, group in df.groupby('genre')]\n    f_stat, p_value = stats.f_oneway(*genre_groups)\n    print(f\"\\nANOVA test for sentiment scores across genres:\")\n    print(f\"F-statistic: {f_stat}\")\n    print(f\"p-value: {p_value}\")\n\nperform_statistical_tests()\n\n\nT-test results for View Count between genres:\nrock vs hip-pop: p-value = 0.0222\nrock vs pop: p-value = 0.0129\nrock vs electronic: p-value = 0.4317\nrock vs jazz: p-value = 0.0000\nhip-pop vs pop: p-value = 0.0002\nhip-pop vs electronic: p-value = 0.0196\nhip-pop vs jazz: p-value = 0.0001\npop vs electronic: p-value = 0.1805\npop vs jazz: p-value = 0.0000\nelectronic vs jazz: p-value = 0.0000\n\nANOVA test for sentiment scores across genres:\nF-statistic: 20.791343730596818\np-value: 1.791803341936434e-15\n\n\n\n# 3. Summary Statistics by Genre\ndef generate_summary_statistics():\n    summary_stats = df.groupby('genre').agg({\n        'View Count': ['mean', 'std', 'min', 'max'],\n        'Like Count': ['mean', 'std'],\n        'Comment Count': ['mean', 'std'],\n        'Mean Sentiment Score': ['mean', 'std']\n    }).round(2)\n    \n    print(\"\\nSummary Statistics by Genre:\")\n    print(summary_stats)\n\ngenerate_summary_statistics()\n\n\nSummary Statistics by Genre:\n              View Count                                    Like Count  \\\n                    mean           std     min         max        mean   \ngenre                                                                    \nelectronic  4.812136e+08  8.352102e+08   33947  3701858405  3444539.65   \nhip-pop     2.120770e+08  4.132461e+08   55780  2234394474  1923365.79   \njazz        1.062045e+07  1.864018e+07     276    94229284    91090.62   \npop         6.973413e+08  1.008843e+09  150497  6379785926  4625445.59   \nrock        3.915309e+08  5.351308e+08  455841  2389301239  2184612.99   \n\n                       Comment Count            Mean Sentiment Score        \n                   std          mean        std                 mean   std  \ngenre                                                                       \nelectronic  5500857.02     113885.77  208795.65                 0.27  0.20  \nhip-pop     2866792.20      80653.13  134561.74                 0.18  0.16  \njazz         167996.85       2455.99    4287.11                 0.47  0.20  \npop         5504375.03     204115.83  250109.54                 0.32  0.21  \nrock        2999623.30      72506.67  108401.21                 0.37  0.23"
  },
  {
    "objectID": "technical-details/data-collection/main.html#data-collection-process",
    "href": "technical-details/data-collection/main.html#data-collection-process",
    "title": "Data Collection",
    "section": "Data Collection Process",
    "text": "Data Collection Process\nThis project collects music data through YouTube and Spotify APIs, covering information on the works of 20 representative artists in five genres: Electronic, Jazz, Hip-Hop, Pop and Rock. The data processing flow is as follows:\n\n1. YouTube Data Collection\n\n1.1 Acquiring Official Music Video Data\n\nfrom googleapiclient.discovery import build\nimport pandas as pd\nfrom datetime import datetime\nimport dateutil.parser\n\n# API Key\napi_key = \"AIzaSyDtKE-4QZj6EA-rwG7cj5gMJxdt4Fe14Nw\"\n\n# Initialize YouTube API client\nyoutube = build('youtube', 'v3', developerKey=api_key)\n\n# List to store data\nall_data = []\n\n# Read song data and fetch YouTube statistics\n# We should have put 20*5 names of singers, but for the sake of presentation, we choose three singers as a demonstration here \nwith open('find.txt', 'r') as file:\n    for line in file:\n        artist = line.strip()\n        query = f\"{artist} official music video\"\n\n        # Search request for the query\n        search_request = youtube.search().list(\n            part=\"snippet\",\n            q=query,\n            maxResults=5,\n            type=\"video\",\n            order='relevance'\n        )\n        search_response = search_request.execute()\n\n        for item in search_response['items']:\n            video_id = item['id']['videoId']\n            video_request = youtube.videos().list(\n                part=\"snippet,contentDetails,statistics\",\n                id=video_id\n            )\n            video_response = video_request.execute()\n\n            for video in video_response['items']:\n                snippet = video['snippet']\n                content_details = video['contentDetails']\n                statistics = video['statistics']\n\n                # Calculate days since video was published\n                published_at = dateutil.parser.parse(snippet['publishedAt'])\n                days_since_published = (datetime.now(published_at.tzinfo) - published_at).days\n\n                # Fetch channel details for subscriber count\n                channel_response = youtube.channels().list(\n                    part='statistics',\n                    id=snippet['channelId']\n                ).execute()\n                subscriber_count = channel_response['items'][0]['statistics'].get('subscriberCount', '0')\n\n                # Fetch comments\n                comments_request = youtube.commentThreads().list(\n                    part='snippet',\n                    videoId=video_id,\n                    order='relevance',\n                    maxResults=10\n                )\n                comments_response = comments_request.execute()\n\n                comments = [comment['snippet']['topLevelComment']['snippet']['textDisplay']\n                            for comment in comments_response.get('items', [])]\n\n                # Store all data in a dictionary\n                video_data = {\n                    'Video ID': video_id,\n                    'Title': snippet['title'],\n                    'Description': snippet['description'],\n                    'Published At': snippet['publishedAt'],\n                    'Days Since Published': days_since_published,\n                    'View Count': statistics.get('viewCount', '0'),\n                    'Like Count': statistics.get('likeCount', '0'),\n                    'Comment Count': statistics.get('commentCount', '0'),\n                    'Comments': comments,\n                    'Subscriber Count': subscriber_count,\n                    'Category ID': snippet['categoryId'],\n                    'Definition': content_details['definition'],\n                    'Duration': content_details['duration']\n                }\n\n                all_data.append(video_data)\n\n# Convert the list to a DataFrame\nfinal_df = pd.DataFrame(all_data)\n\n# Optionally save the DataFrame to a CSV file\nfinal_df.to_csv('Example_Detailed_YouTube_Video_Data.csv', index=False)\n\n# Display the DataFrame\nprint(final_df.head())\n\n      Video ID                                              Title  \\\n0  1VQ_3sBZEm0    Foo Fighters - Learn To Fly (Official HD Video)   \n1  eBG7P-K-r1Y        Foo Fighters - Everlong (Official HD Video)   \n2  SBjQ9tuuTJQ                       Foo Fighters - The Pretender   \n3  EqWRaAF6_WY         Foo Fighters - My Hero (Official HD Video)   \n4  h_L4Rixya64  Foo Fighters - Best Of You (Official Music Video)   \n\n                                         Description          Published At  \\\n0  Foo Fighters' official music video for 'Learn ...  2009-10-03T04:46:13Z   \n1  \"Everlong\" by Foo Fighters \\nListen to Foo Fig...  2009-10-03T04:49:58Z   \n2  Watch the official music video for \"The Preten...  2009-10-03T04:46:14Z   \n3  \"My Hero\" by Foo Fighters \\nListen to Foo Figh...  2011-03-18T19:35:42Z   \n4  Watch the official music video for \"Best Of Yo...  2009-10-03T20:49:33Z   \n\n   Days Since Published View Count Like Count Comment Count  \\\n0                  5550  183921366     808172         33856   \n1                  5550  324414087    1821270         53201   \n2                  5550  588092620    2785700         92245   \n3                  5018   87531478     564448         26099   \n4                  5549  265573360    1281212         34999   \n\n                                            Comments Subscriber Count  \\\n0  [I’m just realising how great Dave grohls acti...          1290000   \n1  [Dad died today. \\r&lt;br&gt;1:20 am.\\r&lt;br&gt;A five da...          1290000   \n2  [So thankful for this awesome song. I&#39;ll b...          1290000   \n3  [My son and I was supposed to spend the summer...          1290000   \n4  [The emotion in his face and his voice transce...          1290000   \n\n  Category ID Definition Duration  \n0          10         hd  PT4M37S  \n1          10         hd  PT4M52S  \n2          10         hd  PT4M31S  \n3          10         hd   PT4M3S  \n4          10         hd  PT4M16S  \n\n\n\n\n1.2 Merge artist name and music genre into csv\nThe row of the initial find.csv(include artist name and genre) is repeated five times per row to correspond to the five mv chosen by each artist (python) Then merge these two columns into the csv (copy manually)\n\nimport pandas as pd\n\ninput_file = './find.csv'  \noutput_file = './Example_singer_info.csv'  \n\n# Load the input CSV file\ndf = pd.read_csv(input_file)\n\n# Create an empty DataFrame to store repeated rows\nrepeated_df = pd.DataFrame()\n\n# Repeat each row 5 times and append it to the new DataFrame\nfor i in range(len(df)):\n    repeated_df = pd.concat([repeated_df, pd.DataFrame([df.iloc[i]] * 5)], ignore_index=True)\n\n# Save the processed DataFrame to a new CSV file\nrepeated_df.to_csv(output_file, index=False)\n\nprint(f\"Data processed successfully and saved as {output_file}\")\n\nData processed successfully and saved as ./Example_singer_info.csv\n\n\n\n\n1.3 Data preprocessing\nExtract the song name from the csv’s title and generate a new CSV file containing the singer’s and song’s name.\n\nimport pandas as pd\nimport re\n\n# Load the CSV file with YouTube video data\ndf = pd.read_csv('./Example_Detailed_YouTube_Video_Data.csv', encoding='MacRoman')\n\n# Function to extract the song name from the title\ndef extract_song_name(title):\n    # Use regular expression to find text between \" - \" and \"(\"\n    match = re.search(r' - (.*?) \\(.*\\)', title)\n    if match:\n        return match.group(1) \n    else:\n        return title  \n\n# Create a new DataFrame with extracted song names\nnew_df = pd.DataFrame({\n    'Extracted Song Name': df['Title'].apply(extract_song_name)\n})\n\n# Save the new DataFrame to a CSV file in the current directory\noutput_file = './Example_extracted_song_names.csv'\nnew_df.to_csv(output_file, index=False, encoding='MacRoman')\n\nprint(f\"Song titles extracted and saved to {output_file}\")\n\nSong titles extracted and saved to ./Example_extracted_song_names.csv\n\n\n\n\ninput_file = './Example_extracted_song_names.csv'\ndf = pd.read_csv(input_file, encoding='MacRoman')\n\n# Function to remove content inside brackets (e.g., [example])\ndef remove_brackets(text):\n    return re.sub(r'\\[.*?\\]', '', text)\n\n# Apply the function to the 'Extracted Song Name' column\ndf['Extracted Song Name'] = df['Extracted Song Name'].apply(remove_brackets)\n\n\noutput_file = './Example_extracted_song_names_cleaned.csv'\ndf.to_csv(output_file, index=False)\n\nprint(f\"Processed data saved to {output_file}\")\n\nProcessed data saved to ./Example_extracted_song_names_cleaned.csv\n\n\nManually merge example_extracted_song_name_cleaned.csv with example_artist_info.csv\n\n\n\n2. Spotify Collection\n\n2.1 Acquiring Track Information\n\nimport pandas as pd\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\n\n# Set up Spotify client credentials\nclient_credentials_manager = SpotifyClientCredentials(\n    client_id='3e1596de002340b898f5d10c9aeae4ea',\n    client_secret='526fa44678974475b0f6ba5d8efd16c4'\n)\nsp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n\n# Input and output file paths\ninput_file = './Example_extracted_song_names_cleaned.csv'\noutput_file = './Example_spotify_track_info.csv'\n\n# Load the input CSV file\ndf = pd.read_csv(input_file, encoding='MacRoman')\n\n# Initialize a list to store results\nresults = []\n\n# Process each row in the DataFrame\nfor _, row in df.iterrows():\n    song = row['Extracted Song Name']\n    artist = row['singer']\n    query = f'{song} {artist}'  # Concatenate song and artist for the search query\n\n    try:\n        # Search for the track on Spotify\n        result = sp.search(q=query, limit=1, type='track')\n        tracks = result.get('tracks', {}).get('items', [])\n\n        if tracks:\n            # If a track is found, extract its details\n            track = tracks[0]\n            track_info = {\n                'Track Name': track['name'],\n                'Artist Name': track['artists'][0]['name'],\n                'Album Name': track['album']['name'],\n                'Popularity': track['popularity'],\n                'Duration (ms)': track['duration_ms'],\n                'Track ID': track['id'],\n                'Spotify URL': track['external_urls']['spotify']\n            }\n            print(f\"Found: {track_info['Track Name']} by {track_info['Artist Name']}\")\n        else:\n            # If no track is found, append placeholders\n            print(f\"Track not found: {query}\")\n            track_info = {\n                'Track Name': song,\n                'Artist Name': artist,\n                'Album Name': None,\n                'Popularity': None,\n                'Duration (ms)': None,\n                'Track ID': None,\n                'Spotify URL': None\n            }\n\n        # Append the result to the list\n        results.append(track_info)\n\n    except Exception as e:\n        # Handle exceptions during the search\n        print(f\"Error processing query '{query}': {e}\")\n        results.append({\n            'Track Name': song,\n            'Artist Name': artist,\n            'Album Name': None,\n            'Popularity': None,\n            'Duration (ms)': None,\n            'Track ID': None,\n            'Spotify URL': None\n        })\n\n# Create a DataFrame from the results\noutput_df = pd.DataFrame(results)\n\n# Save the output DataFrame to a CSV file\noutput_df.to_csv(output_file, index=False, encoding='utf-8')\n\nprint(f\"Processed data saved to: {output_file}\")\n\nFound: Learn to Fly by Foo Fighters\nFound: Everlong by Foo Fighters\nFound: The Pretender by Foo Fighters\nFound: My Hero by Foo Fighters\nFound: Best of You by Foo Fighters\nFound: Mr. Brightside by The Killers\nFound: When You Were Young by The Killers\nFound: Mr. Brightside by The Killers\nFound: Somebody Told Me by The Killers\nFound: One Empty Grave by A Sound of Thunder\nFound: Basket Case by Green Day\nFound: When I Come Around by Green Day\nFound: American Idiot by Green Day\nFound: Boulevard of Broken Dreams by Green Day\nFound: Wake Me up When September Ends by Green Day\nProcessed data saved to: ./Example_spotify_track_info.csv\n\n\n\n\n2.2 Obtaining Artist Information\n\nimport pandas as pd\nimport requests\n\n# Function to obtain an access token for Spotify API\ndef get_access_token(client_id, client_secret):\n    auth_url = 'https://accounts.spotify.com/api/token'\n    auth_data = {\n        'grant_type': 'client_credentials',\n        'client_id': client_id,\n        'client_secret': client_secret\n    }\n    response = requests.post(auth_url, data=auth_data)\n    if response.status_code == 200:\n        return response.json()['access_token']\n    else:\n        raise Exception('Failed to obtain access token')\n\n# Function to get the Spotify artist ID using the artist name\ndef get_artist_id(artist_name, access_token):\n    search_url = f'https://api.spotify.com/v1/search?q={artist_name}&type=artist&limit=1'\n    headers = {'Authorization': f'Bearer {access_token}'}\n    response = requests.get(search_url, headers=headers)\n    if response.status_code == 200:\n        search_results = response.json()\n        artists = search_results['artists']['items']\n        if artists:\n            return artists[0]['id']\n        else:\n            return None\n    else:\n        return None\n\n# Function to retrieve the artist's followers and popularity\ndef get_artist_followers(artist_id, access_token):\n    artist_url = f'https://api.spotify.com/v1/artists/{artist_id}'\n    headers = {'Authorization': f'Bearer {access_token}'}\n    response = requests.get(artist_url, headers=headers)\n    if response.status_code == 200:\n        artist_data = response.json()\n        return artist_data['followers']['total'], artist_data['popularity']\n    else:\n        return None, None\n\n# Load the input CSV file \ninput_file = './Example_singer_info.csv' \ndf = pd.read_csv(input_file)\n\n# Spotify API credentials\nclient_id = '31aba57b31344fdebf98f51375d07834' \nclient_secret = '2c4f929786784910bc9a843518785cae'  \n\n# Obtain Spotify API access token\naccess_token = get_access_token(client_id, client_secret)\n\n# Initialize lists to store followers and popularity data\nfollowers_list = []\npopularity_list = []\n\n# Process each artist in the DataFrame\nfor artist_name in df['artist']:\n    artist_id = get_artist_id(artist_name, access_token)\n    if artist_id:\n        followers, popularity = get_artist_followers(artist_id, access_token)\n        followers_list.append(followers)\n        popularity_list.append(popularity)\n    else:\n        # If the artist is not found, append None\n        followers_list.append(None)\n        popularity_list.append(None)\n\n# Add followers and popularity data to the DataFrame\ndf['Followers'] = followers_list\ndf['Popularity'] = popularity_list\n\n# Save the updated DataFrame to a new CSV file\noutput_file = './Example_artist_data_with_followers_and_popularity.csv'  \ndf.to_csv(output_file, index=False)\n\nprint(f\"Processed data saved to: {output_file}\")\n\nProcessed data saved to: ./Example_artist_data_with_followers_and_popularity.csv\n\n\n\n\n\n3. Data integration and cleaning\nFirst manually merge Spotify and YouTube csv. Then using Spotify and YouTube artists as the matching key, match to verify artist match, if not, then delete the mismatched rows.\n\nimport pandas as pd\n\ninput_file = './Example_Detailed_YouTube_Video_Data.csv' \ndf = pd.read_csv(input_file, encoding='MacRoman')\n\n# Filter rows where 'Artist Name' matches 'Artist'\ndf_cleaned = df[df['Artist Name'] == df['artist']]\n\n# Save the cleaned DataFrame to a new CSV file\noutput_file = './Example_spotify_youtube.csv'  \ndf_cleaned.to_csv(output_file, index=False)\nprint(f\"Cleaned data saved to: {output_file}\")\n\nCleaned data saved to: ./Example_spotify_youtube.csv\n\n\nThen we completed all the data collection steps!"
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#introduction-and-motivation",
    "href": "technical-details/data-cleaning/instructions.html#introduction-and-motivation",
    "title": "Instructions",
    "section": "Introduction and Motivation",
    "text": "Introduction and Motivation\nThis document outlines the steps taken to clean and preprocess the data from a dataset containing view/like/comment counts, relevant comments and populariy value from Spotify and YouTube. The primary goal is to prepare the data for sentiment analysis and further statistical modeling, focusing on extracting meaningful insights about song popularity and engagement."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#overview-of-methods",
    "href": "technical-details/data-cleaning/instructions.html#overview-of-methods",
    "title": "Instructions",
    "section": "Overview of Methods",
    "text": "Overview of Methods\nThe methods used include text cleaning, language detection, sentiment analysis, data transformation, and normalization. These techniques ensure that the data is in an appropriate format for analysis, removing any inconsistencies and ensuring quality and accuracy in the results."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#code-implementation-and-description",
    "href": "technical-details/data-cleaning/instructions.html#code-implementation-and-description",
    "title": "Instructions",
    "section": "Code Implementation and Description",
    "text": "Code Implementation and Description\n\nData Reading\nThe raw data is loaded from a CSV file using Pandas, which provides a convenient framework for data manipulation in Python.\n\nimport pandas as pd\ndf = pd.read_csv('../../data/raw-data/spotify_youtube.csv', encoding='iso-8859-1')\ndf.head()\n\n\n\n\n\n\n\n\nsinger\ngenre\nVideo ID\nTitle\nDescription\nPublished At\nDays Since Published\nView Count\nLike Count\nComment Count\n...\nArtist Name\nArtist\nGenre\nsinger_followers\nsinger_popularity\nAlbum Name\nPopularity\nDuration (ms)\nTrack ID\nSpotify URL\n\n\n\n\n0\nFoo Fighters\nrock\n1VQ_3sBZEm0\nFoo Fighters - Learn To Fly (Official HD Video)\nFoo Fighters' official music video for 'Learn ...\n2009-10-03T04:46:13Z\n5549\n183888273\n808087\n33852\n...\nFoo Fighters\nFoo Fighters\nrock\n12214832\n78\nThere Is Nothing Left To Lose\n73\n235293\n5OQsiBsky2k2kDKy2bX2eT\nhttps://open.spotify.com/track/5OQsiBsky2k2kDK...\n\n\n1\nFoo Fighters\nrock\neBG7P-K-r1Y\nFoo Fighters - Everlong (Official HD Video)\n\"Everlong\" by Foo Fighters \\nListen to Foo Fig...\n2009-10-03T04:49:58Z\n5549\n324339281\n1820753\n53186\n...\nFoo Fighters\nFoo Fighters\nrock\n12214832\n78\nThe Colour And The Shape\n80\n250546\n5UWwZ5lm5PKu6eKsHAGxOk\nhttps://open.spotify.com/track/5UWwZ5lm5PKu6eK...\n\n\n2\nFoo Fighters\nrock\nSBjQ9tuuTJQ\nFoo Fighters - The Pretender\nWatch the official music video for \"The Preten...\n2009-10-03T04:46:14Z\n5549\n588029134\n2785440\n92233\n...\nFoo Fighters\nFoo Fighters\nrock\n12214832\n78\nEchoes, Silence, Patience & Grace\n75\n269373\n7x8dCjCr0x6x2lXKujYD34\nhttps://open.spotify.com/track/7x8dCjCr0x6x2lX...\n\n\n3\nFoo Fighters\nrock\nEqWRaAF6_WY\nFoo Fighters - My Hero (Official HD Video)\n\"My Hero\" by Foo Fighters \\nListen to Foo Figh...\n2011-03-18T19:35:42Z\n5017\n87504683\n564313\n26089\n...\nFoo Fighters\nFoo Fighters\nrock\n12214832\n78\nThe Colour And The Shape\n72\n260026\n4dVbhS6OiYvFikshyaQaCN\nhttps://open.spotify.com/track/4dVbhS6OiYvFiks...\n\n\n4\nFoo Fighters\nrock\nh_L4Rixya64\nFoo Fighters - Best Of You (Official Music Video)\nWatch the official music video for \"Best Of Yo...\n2009-10-03T20:49:33Z\n5548\n265538783\n1281111\n34995\n...\nFoo Fighters\nFoo Fighters\nrock\n12214832\n78\nIn Your Honor\n72\n255626\n5FZxsHWIvUsmSK1IAvm2pp\nhttps://open.spotify.com/track/5FZxsHWIvUsmSK1...\n\n\n\n\n5 rows × 26 columns\n\n\n\n\n\nComment Cleaning and Language Filtering\nComments are cleaned by removing HTML tags, punctuation, numbers, and ensuring they are in English. This is crucial for the accuracy of the sentiment analysis.\ndef clean_comments(comments):\n    # Cleaning code here...\n    return english_comments\n\n\nSentiment Analysis\nUsing NLTK’s VADER, we perform sentiment analysis on the cleaned English comments. This provides a mean sentiment score for each record, indicating the overall sentiment of the comments associated with each song.\ndef average_sentiment_score(comments):\n    # Sentiment analysis code here...\n    return score\n\n\nData Transformations\nSeveral transformations are applied to the dataset: - Converting video duration from ISO 8601 format to seconds. - Binary encoding of video definition (HD or SD). - Factorizing the ‘genre’ column to prepare for modeling.\ndf['Duration_seconds'] = df['Duration'].apply(duration_to_seconds)\ndf['Definition'] = df['Definition'].apply(convert_definition)\ndf['genre_label'] = pd.factorize(df['genre'])[0]\nFor future analysis, some categorical variables are converted into numerical style.\n\n\nData Normalization\nUsing StandardScaler, numerical columns are scaled to have zero mean and unit variance. This step is important for models that are sensitive to the magnitude of input features.\nscaler = StandardScaler()\ndf[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n\n\nBefore-and-After Visualizations\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the normalized data\ndf_normalized = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\n\ndef plot_data_comparisons(df_original, df_normalized, column_name):\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # Original Data Histogram\n    sns.histplot(df_original[column_name], ax=axes[0], kde=True, color='skyblue')\n    axes[0].set_title(f'Original {column_name}')\n    \n    # Normalized Data Histogram\n    sns.histplot(df_normalized[column_name], ax=axes[1], kde=True, color='olive')\n    axes[1].set_title(f'Normalized {column_name}')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Assuming the original data is still available and named df\ndf_original = pd.read_csv('../../data/raw-data/spotify_youtube.csv', encoding='iso-8859-1')\n\n# Example variables to visualize\nvariables_to_visualize = ['View Count', 'Like Count', 'Subscriber Count']\n\nfor variable in variables_to_visualize:\n    plot_data_comparisons(df_original, df_normalized, variable)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe standardization has transformed the data into a scale where it is centered around zero, greatly reducing the range of values and making the distribution more compact. The normalization process highlights the underlying data structure more clearly by smoothing out extreme variations and focusing on the distribution’s shape, facilitating more effective data analysis and model training.\n\n\nData Storage\nThe processed data is saved back to a CSV file, ensuring that all modifications are preserved for subsequent analysis.\ndf.to_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv', index=False)\n\ndf = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\ndf.head()\n\n\n\n\n\n\n\n\nsinger\ngenre\nVideo ID\nTitle\nDescription\nPublished At\nDays Since Published\nView Count\nLike Count\nComment Count\n...\nsinger_popularity\nAlbum Name\nPopularity\nDuration (ms)\nTrack ID\nSpotify URL\nMean Sentiment Score\nProcessed_Comments\nDuration_seconds\ngenre_label\n\n\n\n\n0\nFoo Fighters\nrock\n1VQ_3sBZEm0\nFoo Fighters - Learn To Fly (Official HD Video)\nFoo Fighters' official music video for 'Learn ...\n2009-10-03T04:46:13Z\n1.168381\n-0.264997\n-0.403455\n-0.358746\n...\n-0.184437\nThere Is Nothing Left To Lose\n73\n0.024286\n5OQsiBsky2k2kDKy2bX2eT\nhttps://open.spotify.com/track/5OQsiBsky2k2kDK...\n0.366239\n['can we bring this back this feeling this mus...\n-0.039614\n-1.319198\n\n\n1\nFoo Fighters\nrock\neBG7P-K-r1Y\nFoo Fighters - Everlong (Official HD Video)\n\"Everlong\" by Foo Fighters \\nListen to Foo Fig...\n2009-10-03T04:49:58Z\n1.168381\n-0.065329\n-0.161714\n-0.250200\n...\n-0.184437\nThe Colour And The Shape\n80\n0.248671\n5UWwZ5lm5PKu6eKsHAGxOk\nhttps://open.spotify.com/track/5UWwZ5lm5PKu6eK...\n-0.984291\n['stop asking whos still listening we never st...\n-0.001872\n-1.319198\n\n\n2\nFoo Fighters\nrock\nSBjQ9tuuTJQ\nFoo Fighters - The Pretender\nWatch the official music video for \"The Preten...\n2009-10-03T04:46:14Z\n1.168381\n0.309538\n0.068574\n-0.030982\n...\n-0.184437\nEchoes, Silence, Patience & Grace\n75\n0.525632\n7x8dCjCr0x6x2lXKujYD34\nhttps://open.spotify.com/track/7x8dCjCr0x6x2lX...\n1.334540\n['how can you not get chills listening to this...\n-0.054710\n-1.319198\n\n\n3\nFoo Fighters\nrock\nEqWRaAF6_WY\nFoo Fighters - My Hero (Official HD Video)\n\"My Hero\" by Foo Fighters \\nListen to Foo Figh...\n2011-03-18T19:35:42Z\n0.902404\n-0.402018\n-0.461648\n-0.402329\n...\n-0.184437\nThe Colour And The Shape\n72\n0.388130\n4dVbhS6OiYvFikshyaQaCN\nhttps://open.spotify.com/track/4dVbhS6OiYvFiks...\n1.577078\n['my son and i was supposed to spend the summe...\n-0.125161\n-1.319198\n\n\n4\nFoo Fighters\nrock\nh_L4Rixya64\nFoo Fighters - Best Of You (Official Music Video)\nWatch the official music video for \"Best Of Yo...\n2009-10-03T20:49:33Z\n1.167881\n-0.148921\n-0.290536\n-0.352329\n...\n-0.184437\nIn Your Honor\n72\n0.323402\n5FZxsHWIvUsmSK1IAvm2pp\nhttps://open.spotify.com/track/5FZxsHWIvUsmSK1...\n0.468128\n['what a talent dave is drummer singer guitari...\n-0.092452\n-1.319198\n\n\n\n\n5 rows × 30 columns\n\n\n\nReview Data Types\n\n\n\n\n\n\n\n\nVariable Name\nData Type\nDescription\n\n\n\n\nsinger\nCategorical\nArtist or band name\n\n\ngenre\nCategorical\nGenre of the music\n\n\nVideo ID\nCategorical\nUnique identifier for the video\n\n\nTitle\nCategorical\nTitle of the video\n\n\nDescription\nCategorical\nDescription of the video\n\n\nPublished At\nDate-time\nDate and time the video was published\n\n\nDays Since Published\nNumerical\nNumber of days since the video was published\n\n\nView Count\nNumerical\nTotal number of views on the video\n\n\nLike Count\nNumerical\nNumber of likes on the video\n\n\nComment Count\nNumerical\nNumber of comments on the video\n\n\nComments\nCategorical\nList of 10 comments on the video\n\n\nSubscriber Count\nNumerical\nNumber of subscribers to the channel\n\n\nCategory ID\nCategorical\nYouTube category ID for the video\n\n\nDefinition\nCategorical\nQuality definition of the video\n\n\nDuration\nCategorical\nDuration of the video in a human-readable format\n\n\nTrack Name\nCategorical\nName of the track\n\n\nArtist Name\nCategorical\nName of the artist\n\n\nArtist\nCategorical\nName of the artist\n\n\nsinger_followers\nNumerical\nNumber of followers the singer has on Spotify\n\n\nsinger_popularity\nNumerical\nPopularity rating of the singer\n\n\nAlbum Name\nCategorical\nName of the album\n\n\nPopularity\nNumerical\nPopularity rating of the track\n\n\nDuration (ms)\nNumerical\nDuration of the track in milliseconds\n\n\nTrack ID\nCategorical\nUnique identifier for the track\n\n\nSpotify URL\nCategorical\nURL to the track on Spotify\n\n\nMean Sentiment Score\nNumerical\nAverage sentiment score of comments\n\n\nProcessed_Comments\nCategorical\nProcessed list of comments\n\n\nDuration_seconds\nNumerical\nDuration of the video in seconds\n\n\ngenre_label\nCategorical\nCategorical label for the genre"
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#summary-and-interpretation-of-results",
    "href": "technical-details/data-cleaning/instructions.html#summary-and-interpretation-of-results",
    "title": "Instructions",
    "section": "Summary and Interpretation of Results",
    "text": "Summary and Interpretation of Results\nThe preprocessing steps significantly cleaned and transformed the raw data, making it suitable for accurate and insightful analysis. The sentiment scores provide a quantitative measure of public perception, which, combined with other song metrics, can be used to gauge popularity and engagement."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#introduction-and-motivation",
    "href": "technical-details/data-cleaning/main.html#introduction-and-motivation",
    "title": "Data Cleaning",
    "section": "Introduction and Motivation",
    "text": "Introduction and Motivation\nThis document outlines the steps taken to clean and preprocess the data from a dataset containing view/like/comment counts, relevant comments and populariy value from Spotify and YouTube. The primary goal is to prepare the data for sentiment analysis and further statistical modeling, focusing on extracting meaningful insights about song popularity and engagement."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#overview-of-methods",
    "href": "technical-details/data-cleaning/main.html#overview-of-methods",
    "title": "Data Cleaning",
    "section": "Overview of Methods",
    "text": "Overview of Methods\nThe methods used include text cleaning, language detection, sentiment analysis, data transformation, and normalization. These techniques ensure that the data is in an appropriate format for analysis, removing any inconsistencies and ensuring quality and accuracy in the results."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#code-implementation-and-description",
    "href": "technical-details/data-cleaning/main.html#code-implementation-and-description",
    "title": "Data Cleaning",
    "section": "Code Implementation and Description",
    "text": "Code Implementation and Description\n\nData Reading\nThe raw data is loaded from a CSV file using Pandas, which provides a convenient framework for data manipulation in Python.\nimport pandas as pd\ndf = pd.read_csv('../../data/raw-data/spotify_youtube.csv', encoding='iso-8859-1')\ndf.head()\n\n\nComment Cleaning and Language Filtering\nComments are cleaned by removing HTML tags, punctuation, numbers, and ensuring they are in English. This is crucial for the accuracy of the sentiment analysis.\ndef clean_comments(comments):\n    # Cleaning code here...\n    return english_comments\n\n\nSentiment Analysis\nUsing NLTK’s VADER, we perform sentiment analysis on the cleaned English comments. This provides a mean sentiment score for each record, indicating the overall sentiment of the comments associated with each song.\ndef average_sentiment_score(comments):\n    # Sentiment analysis code here...\n    return score\n\n\nData Transformations\nSeveral transformations are applied to the dataset: - Converting video duration from ISO 8601 format to seconds. - Binary encoding of video definition (HD or SD). - Factorizing the ‘genre’ column to prepare for modeling.\ndf['Duration_seconds'] = df['Duration'].apply(duration_to_seconds)\ndf['Definition'] = df['Definition'].apply(convert_definition)\ndf['genre_label'] = pd.factorize(df['genre'])[0]\nFor future analysis, some categorical variables are converted into numerical style.\n\n\nData Normalization\nUsing StandardScaler, numerical columns are scaled to have zero mean and unit variance. This step is important for models that are sensitive to the magnitude of input features.\nscaler = StandardScaler()\ndf[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n\n\nBefore-and-After Visualizations\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the normalized data\ndf_normalized = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\n\ndef plot_data_comparisons(df_original, df_normalized, column_name):\n    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n    \n    # Original Data Histogram\n    sns.histplot(df_original[column_name], ax=axes[0], kde=True, color='skyblue')\n    axes[0].set_title(f'Original {column_name}')\n    \n    # Normalized Data Histogram\n    sns.histplot(df_normalized[column_name], ax=axes[1], kde=True, color='olive')\n    axes[1].set_title(f'Normalized {column_name}')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Assuming the original data is still available and named df\ndf_original = pd.read_csv('../../data/raw-data/spotify_youtube.csv', encoding='iso-8859-1')\n\n# Example variables to visualize\nvariables_to_visualize = ['View Count', 'Like Count', 'Subscriber Count']\n\nfor variable in variables_to_visualize:\n    plot_data_comparisons(df_original, df_normalized, variable)\nThe standardization has transformed the data into a scale where it is centered around zero, greatly reducing the range of values and making the distribution more compact. The normalization process highlights the underlying data structure more clearly by smoothing out extreme variations and focusing on the distribution’s shape, facilitating more effective data analysis and model training.\n\n\nData Storage\nThe processed data is saved back to a CSV file, ensuring that all modifications are preserved for subsequent analysis.\ndf.to_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv', index=False)\ndf = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\ndf.head()\nReview Data Types\n\n\n\n\n\n\n\n\nVariable Name\nData Type\nDescription\n\n\n\n\nsinger\nCategorical\nArtist or band name\n\n\ngenre\nCategorical\nGenre of the music\n\n\nVideo ID\nCategorical\nUnique identifier for the video\n\n\nTitle\nCategorical\nTitle of the video\n\n\nDescription\nCategorical\nDescription of the video\n\n\nPublished At\nDate-time\nDate and time the video was published\n\n\nDays Since Published\nNumerical\nNumber of days since the video was published\n\n\nView Count\nNumerical\nTotal number of views on the video\n\n\nLike Count\nNumerical\nNumber of likes on the video\n\n\nComment Count\nNumerical\nNumber of comments on the video\n\n\nComments\nCategorical\nList of 10 comments on the video\n\n\nSubscriber Count\nNumerical\nNumber of subscribers to the channel\n\n\nCategory ID\nCategorical\nYouTube category ID for the video\n\n\nDefinition\nCategorical\nQuality definition of the video\n\n\nDuration\nCategorical\nDuration of the video in a human-readable format\n\n\nTrack Name\nCategorical\nName of the track\n\n\nArtist Name\nCategorical\nName of the artist\n\n\nArtist\nCategorical\nName of the artist\n\n\nsinger_followers\nNumerical\nNumber of followers the singer has on Spotify\n\n\nsinger_popularity\nNumerical\nPopularity rating of the singer\n\n\nAlbum Name\nCategorical\nName of the album\n\n\nPopularity\nNumerical\nPopularity rating of the track\n\n\nDuration (ms)\nNumerical\nDuration of the track in milliseconds\n\n\nTrack ID\nCategorical\nUnique identifier for the track\n\n\nSpotify URL\nCategorical\nURL to the track on Spotify\n\n\nMean Sentiment Score\nNumerical\nAverage sentiment score of comments\n\n\nProcessed_Comments\nCategorical\nProcessed list of comments\n\n\nDuration_seconds\nNumerical\nDuration of the video in seconds\n\n\ngenre_label\nCategorical\nCategorical label for the genre"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#summary-and-interpretation-of-results",
    "href": "technical-details/data-cleaning/main.html#summary-and-interpretation-of-results",
    "title": "Data Cleaning",
    "section": "Summary and Interpretation of Results",
    "text": "Summary and Interpretation of Results\nThe preprocessing steps significantly cleaned and transformed the raw data, making it suitable for accurate and insightful analysis. The sentiment scores provide a quantitative measure of public perception, which, combined with other song metrics, can be used to gauge popularity and engagement."
  },
  {
    "objectID": "technical-details/supervised-learning/multi_v2.html",
    "href": "technical-details/supervised-learning/multi_v2.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "like ratio & comment ratio"
  },
  {
    "objectID": "technical-details/supervised-learning/multi_v2.html#data-preprocessing",
    "href": "technical-details/supervised-learning/multi_v2.html#data-preprocessing",
    "title": "DSAN-5000: Project",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n合并为新的csv\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# 加载数据\nfile_path = '../../data/processed-data//Normalized_Data_with _like_comment_ratio.csv'\ndf = pd.read_csv(file_path)\nprint('-------like ratio---------')\nprint(df['like_ratio'].describe())\nprint()\nprint('-------comment ratio---------')\nprint(df['comment_ratio'].describe())\n\n-------like ratio---------\ncount    375.000000\nmean       1.187740\nstd        1.201180\nmin        0.000000\n25%        0.557778\n50%        0.787268\n75%        1.402276\nmax       12.401685\nName: like_ratio, dtype: float64\n\n-------comment ratio---------\ncount    375.000000\nmean       0.053684\nstd        0.083883\nmin        0.000000\n25%        0.016598\n50%        0.026350\n75%        0.059631\nmax        0.963266\nName: comment_ratio, dtype: float64\n\n\nlike_ratio Segmentation based on quartiles\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# histplot build\nplt.figure(figsize=(10, 6))\nsns.histplot(df['like_ratio'], kde=True)\nplt.title('Distribution of like_ratio')\nplt.xlabel('Popularity')\nplt.ylabel('Frequency')\nplt.show()\n\n# boxplot build\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=df['like_ratio'])\nplt.title('Box Plot of like_ratio')\nplt.xlabel('like_ratio')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlow_threshold = 0.7\nhigh_threshold = 1.6\ndef classify_like_ratio(like_ratio):\n    if like_ratio &lt;= low_threshold:\n        return 'Low'\n    elif like_ratio &lt;= high_threshold:\n        return 'Medium'\n    else:\n        return 'High'\n\n\ndf['Like Ratio Category'] = df['like_ratio'].apply(classify_like_ratio)\n# check like ratio category count\nprint(df['Like Ratio Category'].value_counts())\n\nLike Ratio Category\nLow       154\nMedium    146\nHigh       75\nName: count, dtype: int64\n\n\nFeature Selection or Extraction\n\n# Specify the feature columns\nfeatures = [\n    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',\n    'Subscriber Count', 'Mean Sentiment Score',\n    'Duration_seconds', 'singer_followers', 'singer_popularity'\n]\n# Ensure there are no missing values in the dataset\ndf = df.dropna(subset=features + ['Like Ratio Category'])"
  },
  {
    "objectID": "technical-details/supervised-learning/multi_v2.html#model-selection",
    "href": "technical-details/supervised-learning/multi_v2.html#model-selection",
    "title": "DSAN-5000: Project",
    "section": "Model Selection",
    "text": "Model Selection\n\nModel Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used\n\n我这里选择简单回归，决策树以及随机森林\n\n# 查看特征重要性\nimportances = rf_model.feature_importances_\nfor feature, importance in zip(features, importances):\n    print(f\"Feature: {feature}, Importance: {importance:.4f}\")\n\nFeature: Days Since Published, Importance: 0.2341\nFeature: View Count, Importance: 0.1502\nFeature: Like Count, Importance: 0.0826\nFeature: Comment Count, Importance: 0.0884\nFeature: Subscriber Count, Importance: 0.0725\nFeature: Definition, Importance: 0.0049\nFeature: Mean Sentiment Score, Importance: 0.0711\nFeature: Duration_seconds, Importance: 0.0910\nFeature: genre_label, Importance: 0.0486\nFeature: singer_followers, Importance: 0.0795\nFeature: singer_popularity, Importance: 0.0769\n\n\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Standardize the data (PCA works best on standardized data)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Apply PCA\npca = PCA(n_components=5)  # Choose the number of components to keep\nX_train_pca = pca.fit_transform(X_train_scaled)\nX_test_pca = pca.transform(X_test_scaled)\n\n# Explained variance ratio (how much variance each component explains)\nprint(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n\n# You can now use X_train_pca and X_test_pca for training your model\n\nExplained Variance Ratio: [0.41826507 0.1782449  0.11526605 0.10813635 0.07119372]\n\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom imblearn.over_sampling import SMOTE\n\nX = df[features]\ny = df['Like Ratio Category']\n\n# Data set splitting\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Feature standardization\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n\n\n# Initialize the Logistic Regression model (One-vs-Rest for multi-class)\nmodel = LogisticRegression(multi_class='ovr', max_iter=1000, random_state=42)\n\n# Train the model\nmodel.fit(X_train_scaled, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test_scaled)\n\n# Evaluate the model\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n\n# Cross-validation to get a better estimate of the model performance\ncross_val_scores = cross_val_score(model, X_scaled, y, cv=5)\nprint(\"\\nCross-validation scores:\", cross_val_scores)\nprint(\"\\nMean cross-validation score:\", cross_val_scores.mean())\n\nAccuracy: 0.7733333333333333\n\nClassification Report:\n               precision    recall  f1-score   support\n\n        High       0.64      0.82      0.72        11\n         Low       0.86      0.86      0.86        35\n      Medium       0.73      0.66      0.69        29\n\n    accuracy                           0.77        75\n   macro avg       0.74      0.78      0.76        75\nweighted avg       0.78      0.77      0.77        75\n\n\nCross-validation scores: [0.65333333 0.61333333 0.81333333 0.72       0.46666667]\n\nMean cross-validation score: 0.6533333333333333\n\n\n/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n  warnings.warn(\n/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n  warnings.warn(\n/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n  warnings.warn(\n\n\n\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# 创建一个包含过采样和欠采样的管道\nsampling_pipeline = Pipeline([\n    ('smote', SMOTE(random_state=42)),\n    ('decision_tree', DecisionTreeClassifier(random_state=42))\n])\n\n\n#decision tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.model_selection import cross_val_score\n\n# Initialize the Decision Tree model\ndt_model = DecisionTreeClassifier(random_state=42)\n\n# Train the model\ndt_model.fit(X_train_scaled, y_train)\n\n# Predict on the test set\ny_pred_dt = dt_model.predict(X_test_scaled)\n\n# Evaluate the model\nprint(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\nprint(\"\\nDecision Tree Classification Report:\\n\", classification_report(y_test, y_pred_dt))\n\n# Cross-validation to get a better estimate of the model performance\ncross_val_scores_dt = cross_val_score(dt_model, X_train_scaled, y_train, cv=5)\nprint(\"\\nDecision Tree Cross-validation scores:\", cross_val_scores_dt)\nprint(\"\\nMean cross-validation score (Decision Tree):\", cross_val_scores_dt.mean())\n\n\nDecision Tree Accuracy: 0.6933333333333334\n\nDecision Tree Classification Report:\n               precision    recall  f1-score   support\n\n        High       0.47      0.82      0.60        11\n         Low       0.82      0.77      0.79        35\n      Medium       0.70      0.55      0.62        29\n\n    accuracy                           0.69        75\n   macro avg       0.66      0.71      0.67        75\nweighted avg       0.72      0.69      0.70        75\n\n\nDecision Tree Cross-validation scores: [0.56666667 0.55       0.61666667 0.63333333 0.63333333]\n\nMean cross-validation score (Decision Tree): 0.6\n\n\n\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, accuracy_score\n\n#param_grid\nparam_grid_refined = {\n    'max_depth': [3,4,5],  \n    'min_samples_split': [8,10,15],\n    'min_samples_leaf': [3,4,5,6],   \n    'criterion': ['entropy'],        \n    'class_weight': ['balanced']  \n}\n\ngrid_search_refined = GridSearchCV(\n    DecisionTreeClassifier(random_state=42),\n    param_grid_refined,\n    cv=5,\n    scoring='f1_macro', \n    n_jobs=-1\n)\n\n# train mode\ngrid_search_refined.fit(X_train_scaled, y_train)\n\nprint(\"Refined Best parameters:\", grid_search_refined.best_params_)\nprint(\"Refined Best cross-validation score:\", grid_search_refined.best_score_)\n\nbest_model_refined = DecisionTreeClassifier(\n    **grid_search_refined.best_params_\n)\n\nbest_model_refined.fit(X_train_scaled, y_train)\n\ny_pred_refined = best_model_refined.predict(X_test_scaled)\nprint(\"\\nRefined Test set accuracy:\", accuracy_score(y_test, y_pred_refined))\nprint(\"\\nRefined Classification Report:\\n\", classification_report(y_test, y_pred_refined))\n\nRefined Best parameters: {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 8}\nRefined Best cross-validation score: 0.6208158730661306\n\nRefined Test set accuracy: 0.7466666666666667\n\nRefined Classification Report:\n               precision    recall  f1-score   support\n\n        High       0.56      0.91      0.69        11\n         Low       0.85      0.83      0.84        35\n      Medium       0.74      0.59      0.65        29\n\n    accuracy                           0.75        75\n   macro avg       0.72      0.77      0.73        75\nweighted avg       0.77      0.75      0.75        75\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn import tree\n\n# Assuming 'best_model_refined' is your trained DecisionTreeClassifier model\n\n# Set the size of the plot and DPI for higher resolution\nplt.figure(figsize=(30, 20), dpi=100)\n\n# Plot the decision tree with custom font size and node size\ntree.plot_tree(\n    best_model_refined,  # Here you can use your trained model\n    feature_names=features,  # Replace 'features' with the list of feature names\n    class_names=best_model_refined.classes_,  # Class names for your target\n    filled=True,\n    rounded=True,\n    fontsize=10,  # Adjust font size here for better readability\n    node_ids=True,\n    proportion=False,\n    precision=2\n)\n\n# Adjust the layout and make more space available\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Initialize the Random Forest model\nrf_model = RandomForestClassifier(n_estimators=100)\n\n# Train the model\nrf_model.fit(X_train_scaled, y_train)\n\n# Predict on the test set\ny_pred_rf = rf_model.predict(X_test_scaled)\n\n# Evaluate the model\nprint(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\nprint(\"\\nRandom Forest Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n\n# Cross-validation to get a better estimate of the model performance\ncross_val_scores_rf = cross_val_score(rf_model, X_train_scaled, y_train, cv=5)\nprint(\"\\nRandom Forest Cross-validation scores:\", cross_val_scores_rf)\nprint(\"\\nMean cross-validation score (Random Forest):\", cross_val_scores_rf.mean())\n\nRandom Forest Accuracy: 0.8\n\nRandom Forest Classification Report:\n               precision    recall  f1-score   support\n\n        High       0.75      0.82      0.78        11\n         Low       0.88      0.80      0.84        35\n      Medium       0.74      0.79      0.77        29\n\n    accuracy                           0.80        75\n   macro avg       0.79      0.80      0.80        75\nweighted avg       0.81      0.80      0.80        75\n\n\nRandom Forest Cross-validation scores: [0.73333333 0.63333333 0.65       0.71666667 0.71666667]\n\nMean cross-validation score (Random Forest): 0.6900000000000001"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#introduction-and-motivation",
    "href": "technical-details/supervised-learning/main.html#introduction-and-motivation",
    "title": "Supervised Learning",
    "section": "Introduction and Motivation",
    "text": "Introduction and Motivation\nThis analysis aims to explore the factors influencing the popularity of content on a digital platform. We will use various regression models to predict popularity based on multiple features extracted from our dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#overview-of-methods",
    "href": "technical-details/supervised-learning/main.html#overview-of-methods",
    "title": "Supervised Learning",
    "section": "Overview of Methods",
    "text": "Overview of Methods\nWe will use Support Vector Regression (SVR) and linear models like Ridge and Lasso regression.\nimport pandas as pd\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\ndf = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\n\n# Specify the feature columns\nfeatures = [\n    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',\n    'Subscriber Count', 'Definition', 'Mean Sentiment Score',\n    'Duration_seconds', 'genre_label', 'singer_followers', 'singer_popularity'\n]\n\n# Ensure the target column 'popularity' exists in DataFrame\nif 'Popularity' not in df.columns:\n    raise ValueError(\"The 'popularity' column is missing from the DataFrame.\")\n\n# Split into input (X) and target (y)\nX = df[features]  # Inputs\ny = df['Popularity']  # Target\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Assuming the SVR model is already trained as svr_model\nsvr_model = SVR(kernel='rbf', C=100, gamma='auto')\nsvr_model.fit(X_train, y_train)\n\n# Predict on the testing data\ny_pred = svr_model.predict(X_test)\n# Calculate mean squared error and R^2 score\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Mean Squared Error: {mse}, R^2 Score: {r2}\")\n\n# Perform permutation importance\nperm_importance = permutation_importance(svr_model, X_test, y_test, n_repeats=30, random_state=42)\n\n# Get importance scores\nimportance_scores = perm_importance.importances_mean\n\n# Print feature importance\nprint(\"Feature importances:\")\nfor i, feature in enumerate(features):\n    print(f\"{feature}: {importance_scores[i]}\")"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#conclusion",
    "href": "technical-details/supervised-learning/main.html#conclusion",
    "title": "Supervised Learning",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n\n\n\n\n\n\nModel\nMSE\nR² Score\n\n\n\n\nSVR\n99.74699515216331\n0.568546463925952\n\n\nLinear Regression\n123.52107678768438\n0.46571217229730644\n\n\nRidge Regression\n125.61840469866303\n0.4566402243943334\n\n\nLasso Regression\n124.26131518891665\n0.46251028661381044\n\n\n\nFeature importances from SVR: - Days Since Published: 0.14590051030422282 - View Count: 0.0689113148502238 - Like Count: 0.06300639647560934 - Comment Count: 0.004829835437846231 - Subscriber Count: 0.15936065653760564 - Definition: 0.05992188839233011 - Mean Sentiment Score: 0.037774091955604584 - Duration_seconds: 0.00110286673615743 - genre_label: 0.1581978297916415 - singer_followers: 0.0360773423716881 - singer_popularity: 0.6712934520249385\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.barh(features, importance_scores, color='skyblue')\nplt.xlabel('Importance Score')\nplt.title('Feature Importance Scores')\nplt.gca().invert_yaxis()  # Invert y-axis to have the highest score on top\nplt.show()\n\nResult Interpretation\nThe Support Vector Regression (SVR) model outperformed the linear models in terms of Mean Squared Error (MSE) and R² score. The lower MSE and higher R² of the SVR indicate better performance in fitting the data compared to the Linear, Ridge, and Lasso regressions. The R² scores suggest that the SVR model was able to explain approximately 56.85% of the variance in the dataset, which is more than the approximately 46.57% by the Linear Regression, 45.66% by Ridge, and 46.25% by Lasso Regression.\n\n\nInsights\nFrom the SVR model’s permutation importance, ‘singer_popularity’ emerged as the most influential feature, significantly impacting the prediction of a song’s popularity. This suggests that more popular singers tend to have more popular songs, highlighting the influence of an artist’s existing reputation on new releases."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#introduction-and-motivation-1",
    "href": "technical-details/supervised-learning/main.html#introduction-and-motivation-1",
    "title": "Supervised Learning",
    "section": "Introduction and Motivation",
    "text": "Introduction and Motivation\nThe goal of this section is to predict whether a song is considered “popular” using binary classification methods. We aim to understand the features that significantly influence song popularity on digital platforms."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#overview-of-methods-1",
    "href": "technical-details/supervised-learning/main.html#overview-of-methods-1",
    "title": "Supervised Learning",
    "section": "Overview of Methods",
    "text": "Overview of Methods\nIn this section, we focus on Logistic Regression for binary classification. Logistic Regression is chosen for its ability to provide probabilities for outcomes and its interpretability.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import train_test_split\n\n# Assume df is your DataFrame and the preprocessing has been done to define 'is_popular'\nfeatures = [\n    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',\n    'Subscriber Count', 'Definition', 'Mean Sentiment Score',\n    'Duration_seconds', 'genre_label', 'singer_followers', 'singer_popularity'\n]\nX = df[features]\ny = df['is_popular']\n\n# Splitting the dataset and under-sampling\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n\n# Logistic Regression model\nlr_model = LogisticRegression(random_state=42, max_iter=1000)\nlr_model.fit(X_resampled, y_resampled)\n\n# Making predictions and evaluating the model\ny_pred = lr_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclass_report = classification_report(y_test, y_pred)\n\nprint(\"Logistic Regression Model Evaluation\")\nprint(\"Accuracy:\", accuracy)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\nprint(\"Classification Report:\\n\", class_report)"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#results-1",
    "href": "technical-details/supervised-learning/main.html#results-1",
    "title": "Supervised Learning",
    "section": "Results",
    "text": "Results\nThe Logistic Regression model showed an accuracy of 77.33%, with a detailed classification report indicating precision, recall, and F1-score for both classes.\n\nModel Performance Summary\nComparison of model performance metrics for different classification models used in predicting song popularity:\n\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nPrecision\nRecall\nF1-Score\n\n\n\n\nLogistic Regression\n77.33%\n0.96 (False), 0.35 (True)\n0.77 (False), 0.80 (True)\n0.85 (False), 0.48 (True)\n\n\nSVM (Best Kernel)\n75.00%\n0.91 (False), 0.26 (True)\n0.78 (False), 0.50 (True)\n0.84 (False), 0.34 (True)\n\n\nRandom Forest\n71.00%\n0.92 (False), 0.25 (True)\n0.72 (False), 0.60 (True)\n0.81 (False), 0.35 (True)"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#conclusion-1",
    "href": "technical-details/supervised-learning/main.html#conclusion-1",
    "title": "Supervised Learning",
    "section": "Conclusion",
    "text": "Conclusion\nThe Logistic Regression model, adjusted for class imbalance via under-sampling, provided satisfactory classification results, proving effective for identifying popular songs."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#result-interpretation-1",
    "href": "technical-details/supervised-learning/main.html#result-interpretation-1",
    "title": "Supervised Learning",
    "section": "Result Interpretation",
    "text": "Result Interpretation\nThe model was particularly strong in identifying non-popular songs (class ‘False’) with high precision and recall. And it has a higher recall score on popular songs compared to other models."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-performance-comparison",
    "href": "technical-details/supervised-learning/main.html#model-performance-comparison",
    "title": "Supervised Learning",
    "section": "Model Performance Comparison",
    "text": "Model Performance Comparison\nThis model was compared to other binary classification models such as SVM and Random Forest. Logistic Regression was chosen for its balance between performance and interpretability in this specific context. And it has a higher recall score on popular songs compared to other models."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#introduction-and-motivation",
    "href": "technical-details/supervised-learning/instructions.html#introduction-and-motivation",
    "title": "Instructions",
    "section": "Introduction and Motivation",
    "text": "Introduction and Motivation\nThis analysis aims to explore the factors influencing the popularity of content on a digital platform. We will use various regression models to predict popularity based on multiple features extracted from our dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#overview-of-methods",
    "href": "technical-details/supervised-learning/instructions.html#overview-of-methods",
    "title": "Instructions",
    "section": "Overview of Methods",
    "text": "Overview of Methods\nWe will use Support Vector Regression (SVR) and linear models like Ridge and Lasso regression.\nimport pandas as pd\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\ndf = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\n\n# Specify the feature columns\nfeatures = [\n    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',\n    'Subscriber Count', 'Definition', 'Mean Sentiment Score',\n    'Duration_seconds', 'genre_label', 'singer_followers', 'singer_popularity'\n]\n\n# Ensure the target column 'popularity' exists in DataFrame\nif 'Popularity' not in df.columns:\n    raise ValueError(\"The 'popularity' column is missing from the DataFrame.\")\n\n# Split into input (X) and target (y)\nX = df[features]  # Inputs\ny = df['Popularity']  # Target\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Assuming the SVR model is already trained as svr_model\nsvr_model = SVR(kernel='rbf', C=100, gamma='auto')\nsvr_model.fit(X_train, y_train)\n\n# Predict on the testing data\ny_pred = svr_model.predict(X_test)\n# Calculate mean squared error and R^2 score\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Mean Squared Error: {mse}, R^2 Score: {r2}\")\n\n# Perform permutation importance\nperm_importance = permutation_importance(svr_model, X_test, y_test, n_repeats=30, random_state=42)\n\n# Get importance scores\nimportance_scores = perm_importance.importances_mean\n\n# Print feature importance\nprint(\"Feature importances:\")\nfor i, feature in enumerate(features):\n    print(f\"{feature}: {importance_scores[i]}\")"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#conclusion",
    "href": "technical-details/supervised-learning/instructions.html#conclusion",
    "title": "Instructions",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n\n\n\n\n\n\nModel\nMSE\nR² Score\n\n\n\n\nSVR\n99.74699515216331\n0.568546463925952\n\n\nLinear Regression\n123.52107678768438\n0.46571217229730644\n\n\nRidge Regression\n125.61840469866303\n0.4566402243943334\n\n\nLasso Regression\n124.26131518891665\n0.46251028661381044\n\n\n\nFeature importances from SVR: - Days Since Published: 0.14590051030422282 - View Count: 0.0689113148502238 - Like Count: 0.06300639647560934 - Comment Count: 0.004829835437846231 - Subscriber Count: 0.15936065653760564 - Definition: 0.05992188839233011 - Mean Sentiment Score: 0.037774091955604584 - Duration_seconds: 0.00110286673615743 - genre_label: 0.1581978297916415 - singer_followers: 0.0360773423716881 - singer_popularity: 0.6712934520249385\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.barh(features, importance_scores, color='skyblue')\nplt.xlabel('Importance Score')\nplt.title('Feature Importance Scores')\nplt.gca().invert_yaxis()  # Invert y-axis to have the highest score on top\nplt.show()\n\nResult Interpretation\nThe Support Vector Regression (SVR) model outperformed the linear models in terms of Mean Squared Error (MSE) and R² score. The lower MSE and higher R² of the SVR indicate better performance in fitting the data compared to the Linear, Ridge, and Lasso regressions. The R² scores suggest that the SVR model was able to explain approximately 56.85% of the variance in the dataset, which is more than the approximately 46.57% by the Linear Regression, 45.66% by Ridge, and 46.25% by Lasso Regression.\n\n\nInsights\nFrom the SVR model’s permutation importance, ‘singer_popularity’ emerged as the most influential feature, significantly impacting the prediction of a song’s popularity. This suggests that more popular singers tend to have more popular songs, highlighting the influence of an artist’s existing reputation on new releases."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#introduction-and-motivation-1",
    "href": "technical-details/supervised-learning/instructions.html#introduction-and-motivation-1",
    "title": "Instructions",
    "section": "Introduction and Motivation",
    "text": "Introduction and Motivation\nThe goal of this section is to predict whether a song is considered “popular” using binary classification methods. We aim to understand the features that significantly influence song popularity on digital platforms."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#overview-of-methods-1",
    "href": "technical-details/supervised-learning/instructions.html#overview-of-methods-1",
    "title": "Instructions",
    "section": "Overview of Methods",
    "text": "Overview of Methods\nIn this section, we focus on Logistic Regression for binary classification. Logistic Regression is chosen for its ability to provide probabilities for outcomes and its interpretability.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import train_test_split\n\n# Assume df is your DataFrame and the preprocessing has been done to define 'is_popular'\nfeatures = [\n    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',\n    'Subscriber Count', 'Definition', 'Mean Sentiment Score',\n    'Duration_seconds', 'genre_label', 'singer_followers', 'singer_popularity'\n]\nX = df[features]\ny = df['is_popular']\n\n# Splitting the dataset and under-sampling\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n\n# Logistic Regression model\nlr_model = LogisticRegression(random_state=42, max_iter=1000)\nlr_model.fit(X_resampled, y_resampled)\n\n# Making predictions and evaluating the model\ny_pred = lr_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclass_report = classification_report(y_test, y_pred)\n\nprint(\"Logistic Regression Model Evaluation\")\nprint(\"Accuracy:\", accuracy)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\nprint(\"Classification Report:\\n\", class_report)"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#results-1",
    "href": "technical-details/supervised-learning/instructions.html#results-1",
    "title": "Instructions",
    "section": "Results",
    "text": "Results\nThe Logistic Regression model showed an accuracy of 77.33%, with a detailed classification report indicating precision, recall, and F1-score for both classes.\n\nModel Performance Summary\nComparison of model performance metrics for different classification models used in predicting song popularity:\n\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nPrecision\nRecall\nF1-Score\n\n\n\n\nLogistic Regression\n77.33%\n0.96 (False), 0.35 (True)\n0.77 (False), 0.80 (True)\n0.85 (False), 0.48 (True)\n\n\nSVM (Best Kernel)\n75.00%\n0.91 (False), 0.26 (True)\n0.78 (False), 0.50 (True)\n0.84 (False), 0.34 (True)\n\n\nRandom Forest\n71.00%\n0.92 (False), 0.25 (True)\n0.72 (False), 0.60 (True)\n0.81 (False), 0.35 (True)"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#conclusion-1",
    "href": "technical-details/supervised-learning/instructions.html#conclusion-1",
    "title": "Instructions",
    "section": "Conclusion",
    "text": "Conclusion\nThe Logistic Regression model, adjusted for class imbalance via under-sampling, provided satisfactory classification results, proving effective for identifying popular songs."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#result-interpretation-1",
    "href": "technical-details/supervised-learning/instructions.html#result-interpretation-1",
    "title": "Instructions",
    "section": "Result Interpretation",
    "text": "Result Interpretation\nThe model was particularly strong in identifying non-popular songs (class ‘False’) with high precision and recall. And it has a higher recall score on popular songs compared to other models."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-performance-comparison",
    "href": "technical-details/supervised-learning/instructions.html#model-performance-comparison",
    "title": "Instructions",
    "section": "Model Performance Comparison",
    "text": "Model Performance Comparison\nThis model was compared to other binary classification models such as SVM and Random Forest. Logistic Regression was chosen for its balance between performance and interpretability in this specific context. And it has a higher recall score on popular songs compared to other models."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#pca",
    "href": "technical-details/unsupervised-learning/main.html#pca",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the dataset\ndf = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\n\n# Specify the feature columns\nfeatures = [\n    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',\n    'Subscriber Count', 'Definition', 'Mean Sentiment Score',\n    'Duration_seconds', 'genre_label', 'singer_followers', 'singer_popularity'\n]\n\n# Ensure the target column 'popularity' exists in DataFrame\nif 'Popularity' not in df.columns:\n    raise ValueError(\"The 'popularity' column is missing from the DataFrame.\")\n\n# Split into input (X) and target (y)\nX = df[features]  # Inputs\ny = df['Popularity']  # Target\n\n# Applying PCA\npca = PCA(n_components=3)  # Reduce to 4 dimensions for visualization\nX_pca = pca.fit_transform(X)\n# How much variance was retained?\nprint(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\nprint(\"Total Variance Explained:\", sum(pca.explained_variance_ratio_))"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#t-sne",
    "href": "technical-details/unsupervised-learning/main.html#t-sne",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "perplexities = [5, 30, 50, 100]\nfig, ax = plt.subplots(2, 2, figsize=(15, 10))\nfor i, perplexity in enumerate(perplexities):\n    # Applying t-SNE\n    tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=3000, random_state=42)\n    X_tsne = tsne.fit_transform(X)\n\n    # Plotting\n    ax[i//2, i%2].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', marker='o', alpha=0.5)\n    ax[i//2, i%2].set_title(f't-SNE with Perplexity={perplexity}')\n    ax[i//2, i%2].set_xlabel('t-SNE Feature 1')\n    ax[i//2, i%2].set_ylabel('t-SNE Feature 2')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#evaluation-and-comparison-pca-vs.-t-sne",
    "href": "technical-details/unsupervised-learning/main.html#evaluation-and-comparison-pca-vs.-t-sne",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "PCA: The PCA visualization shows a somewhat linear distribution of the data points, focusing on the major directions of data variance. It effectively captures the global structure of the data, emphasizing the spread along the directions of highest variance.\nt-SNE: In contrast, the t-SNE visualization exhibits a more scattered and differentiated structure with distinct clusters. This indicates t-SNE’s superior capability in capturing local structures within the data, potentially revealing intrinsic patterns that PCA might overlook.\n\n\n\n\nPCA: Provides a simplified overview, projecting the data into lower dimensions while trying to preserve the variance. It’s effective for quick explorations of the data to identify gross underlying patterns but might miss finer details.\nt-SNE: Produces more visually distinct clusters, making it easier to identify groups and patterns within the data. This makes t-SNE a better tool for tasks requiring detailed pattern recognition and cluster analysis.\n\n\n\n\n\nPCA:\n\nPros: Less computationally intensive, suitable for larger datasets, provides a quick overview.\nCons: Might miss non-linear relationships between features.\nBest for: Preliminary data analysis, reducing dimensionality for linear data, or when computational resources are limited.\n\nt-SNE:\n\nPros: Captures complex non-linear relationships, excellent for identifying clusters and local patterns.\nCons: Computationally expensive, sensitive to parameter settings (like perplexity), not suitable for very large datasets.\nBest for: Detailed exploratory data analysis, when clusters or local patterns within the data are of interest, and computational resources are adequate.\n\n\nBoth PCA and t-SNE offer valuable insights, but their applicability depends on the specific needs of the analysis. PCA can serve as a good starting point for linear dimensionality reduction, while t-SNE is more suited for in-depth analysis requiring detailed cluster identification."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#pca",
    "href": "technical-details/unsupervised-learning/instructions.html#pca",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the dataset\ndf = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')\n\n# Specify the feature columns\nfeatures = [\n    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',\n    'Subscriber Count', 'Definition', 'Mean Sentiment Score',\n    'Duration_seconds', 'genre_label', 'singer_followers', 'singer_popularity'\n]\n\n# Ensure the target column 'popularity' exists in DataFrame\nif 'Popularity' not in df.columns:\n    raise ValueError(\"The 'popularity' column is missing from the DataFrame.\")\n\n# Split into input (X) and target (y)\nX = df[features]  # Inputs\ny = df['Popularity']  # Target\n\n# Applying PCA\npca = PCA(n_components=3)  # Reduce to 3 dimensions for visualization\nX_pca = pca.fit_transform(X)\n# How much variance was retained?\nprint(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\nprint(\"Total Variance Explained:\", sum(pca.explained_variance_ratio_))\n\nExplained Variance Ratio: [0.35411601 0.1606999  0.12194563]\nTotal Variance Explained: 0.6367615347014879"
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#t-sne",
    "href": "technical-details/unsupervised-learning/instructions.html#t-sne",
    "title": "Dimensionality Reduction",
    "section": "t-SNE",
    "text": "t-SNE\n\nperplexities = [5, 30, 50, 100]\nfig, ax = plt.subplots(2, 2, figsize=(15, 10))\nfor i, perplexity in enumerate(perplexities):\n    # Applying t-SNE\n    tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=3000, random_state=42)\n    X_tsne = tsne.fit_transform(X)\n\n    # Plotting\n    ax[i//2, i%2].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', marker='o', alpha=0.5)\n    ax[i//2, i%2].set_title(f't-SNE with Perplexity={perplexity}')\n    ax[i//2, i%2].set_xlabel('t-SNE Feature 1')\n    ax[i//2, i%2].set_ylabel('t-SNE Feature 2')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFrom the plot, we can observe perplexities=50 has a better performance so we chose it as the parameter for dimension reduction."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#evaluation-and-comparison-pca-vs.-t-sne",
    "href": "technical-details/unsupervised-learning/instructions.html#evaluation-and-comparison-pca-vs.-t-sne",
    "title": "Dimensionality Reduction",
    "section": "Evaluation and Comparison: PCA vs. t-SNE",
    "text": "Evaluation and Comparison: PCA vs. t-SNE\n\nEffectiveness in Preserving Data Structure\n\nPCA: The PCA visualization shows a somewhat linear distribution of the data points, focusing on the major directions of data variance. It effectively captures the global structure of the data, emphasizing the spread along the directions of highest variance.\nt-SNE: In contrast, the t-SNE visualization exhibits a more scattered and differentiated structure with distinct clusters. This indicates t-SNE’s superior capability in capturing local structures within the data, potentially revealing intrinsic patterns that PCA might overlook.\n\n\nVisualization Capabilities\n\nPCA: Provides a simplified overview, projecting the data into lower dimensions while trying to preserve the variance. It’s effective for quick explorations of the data to identify gross underlying patterns but might miss finer details.\nt-SNE: Produces more visually distinct clusters, making it easier to identify groups and patterns within the data. This makes t-SNE a better tool for tasks requiring detailed pattern recognition and cluster analysis.\n\n\n\nTrade-offs and Scenarios\n\nPCA:\n\nPros: Less computationally intensive, suitable for larger datasets, provides a quick overview.\nCons: Might miss non-linear relationships between features.\nBest for: Preliminary data analysis, reducing dimensionality for linear data, or when computational resources are limited.\n\nt-SNE:\n\nPros: Captures complex non-linear relationships, excellent for identifying clusters and local patterns.\nCons: Computationally expensive, sensitive to parameter settings (like perplexity), not suitable for very large datasets.\nBest for: Detailed exploratory data analysis, when clusters or local patterns within the data are of interest, and computational resources are adequate.\n\n\nFor the data given, t-SNE has the better performance for dimentionality reduction and for future classification task."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#introduction-and-motivation-2",
    "href": "technical-details/supervised-learning/instructions.html#introduction-and-motivation-2",
    "title": "Instructions",
    "section": "Introduction and Motivation",
    "text": "Introduction and Motivation\nThe goal of this multi- class classification is to predict the interaction rate (like ratio= like count/ view count) of youtube mv. We want to predict the interaction rate of the content by features such as number of days since posting and number of comments.\nThis analysis firstly hopes to improve data analysis and model selection capabilities, and secondly tries to help content creators optimize their creation strategies."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#overview-of-methods-2",
    "href": "technical-details/supervised-learning/instructions.html#overview-of-methods-2",
    "title": "Instructions",
    "section": "Overview of Methods",
    "text": "Overview of Methods\nMultiple classification algorithms were used to predict the popularity (like ratio) of short video content. The like ratio was first categorized into three categories: low (≤0.7), medium (0.7-1.6), and high (&gt;1.6), and then predicted using three models: logistic regression, decision tree, and random forest. To address data imbalance, SMOTE oversampling technique was used. The input features of the model included nine variables such as days of posting/comments, number of subscribers, sentiment score, video duration, number of creator followers, and popularity. The decision tree model was hyper-parametrically optimized by grid search (GridSearchCV) and the model performance was evaluated using cross-validation. This combination of methods enables comprehensive assessment and prediction of video content popularity, providing data support for content creation and platform operation."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#code",
    "href": "technical-details/supervised-learning/instructions.html#code",
    "title": "Instructions",
    "section": "Code",
    "text": "Code\nlogistic regression\nX = df[features]\ny = df['Like Ratio Category']\n\n# Data set splitting\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Feature standardization\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n\n\n# Initialize the Logistic Regression model (One-vs-Rest for multi-class)\nmodel = LogisticRegression(multi_class='ovr', max_iter=1000, random_state=42)\n\n# Train the model\nmodel.fit(X_train_scaled, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test_scaled)\n\nX_scaled = scaler.fit_transform(X)  # Fit and transform the entire dataset\n\n# Cross-validation to get a better estimate of the model performance\ncross_val_scores = cross_val_score(model, X_scaled, y, cv=5)\nprint(\"\\nCross-validation scores:\", cross_val_scores)\nprint(\"\\nMean cross-validation score:\", cross_val_scores.mean())\n\n# Evaluate the model\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n#param_grid\nparam_grid_refined = {\n    'max_depth': [3,4,5],  \n    'min_samples_split': [8,10,15],\n    'min_samples_leaf': [3,4,5,6],   \n    'criterion': ['entropy'],        \n    'class_weight': ['balanced']  \n}\n\ngrid_search_refined = GridSearchCV(\n    DecisionTreeClassifier(random_state=42),\n    param_grid_refined,\n    cv=5,\n    scoring='f1_macro', \n    n_jobs=-1\n)\n\n# train mode\ngrid_search_refined.fit(X_train_scaled, y_train)\n\nprint(\"Refined Best parameters:\", grid_search_refined.best_params_)\nprint(\"Refined Best cross-validation score:\", grid_search_refined.best_score_)\n\nbest_model_refined = DecisionTreeClassifier(\n    **grid_search_refined.best_params_\n)\n\nbest_model_refined.fit(X_train_scaled, y_train)\n\ny_pred_refined = best_model_refined.predict(X_test_scaled)\nprint(\"\\nRefined Test set accuracy:\", accuracy_score(y_test, y_pred_refined))\nprint(\"\\nRefined Classification Report:\\n\", classification_report(y_test, y_pred_refined))\nrandom forest\nf_model = RandomForestClassifier(n_estimators=100)\n\n# Train the model\nrf_model.fit(X_train_scaled, y_train)\n\n# Predict on the test set\ny_pred_rf = rf_model.predict(X_test_scaled)\n\n# Evaluate the model\nprint(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\nprint(\"\\nRandom Forest Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n\n# Cross-validation to get a better estimate of the model performance\ncross_val_scores_rf = cross_val_score(rf_model, X_train_scaled, y_train, cv=5)\nprint(\"\\nRandom Forest Cross-validation scores:\", cross_val_scores_rf)\nprint(\"\\nMean cross-validation score (Random Forest):\", cross_val_scores_rf.mean())"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#introduction-and-motivation-2",
    "href": "technical-details/supervised-learning/main.html#introduction-and-motivation-2",
    "title": "Supervised Learning",
    "section": "Introduction and Motivation",
    "text": "Introduction and Motivation\nThe goal of this multi- class classification is to predict the interaction rate (like ratio= like count/ view count) of youtube mv. We want to predict the interaction rate of the content by features such as number of days since posting and number of comments.\nThis analysis firstly hopes to improve data analysis and model selection capabilities, and secondly tries to help content creators optimize their creation strategies."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#overview-of-methods-2",
    "href": "technical-details/supervised-learning/main.html#overview-of-methods-2",
    "title": "Supervised Learning",
    "section": "Overview of Methods",
    "text": "Overview of Methods\nMultiple classification algorithms were used to predict the popularity (like ratio) of short video content. The like ratio was first categorized into three categories: low (≤0.7), medium (0.7-1.6), and high (&gt;1.6), and then predicted using three models: logistic regression, decision tree, and random forest. To address data imbalance, SMOTE oversampling technique was used. The input features of the model included nine variables such as days of posting/comments, number of subscribers, sentiment score, video duration, number of creator followers, and popularity. The decision tree model was hyper-parametrically optimized by grid search (GridSearchCV) and the model performance was evaluated using cross-validation. This combination of methods enables comprehensive assessment and prediction of video content popularity, providing data support for content creation and platform operation."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#code",
    "href": "technical-details/supervised-learning/main.html#code",
    "title": "Supervised Learning",
    "section": "Code",
    "text": "Code\nlogistic regression\nX = df[features]\ny = df['Like Ratio Category']\n\n# Data set splitting\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Feature standardization\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n\n\n# Initialize the Logistic Regression model (One-vs-Rest for multi-class)\nmodel = LogisticRegression(multi_class='ovr', max_iter=1000, random_state=42)\n\n# Train the model\nmodel.fit(X_train_scaled, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test_scaled)\n\nX_scaled = scaler.fit_transform(X)  # Fit and transform the entire dataset\n\n# Cross-validation to get a better estimate of the model performance\ncross_val_scores = cross_val_score(model, X_scaled, y, cv=5)\nprint(\"\\nCross-validation scores:\", cross_val_scores)\nprint(\"\\nMean cross-validation score:\", cross_val_scores.mean())\n\n# Evaluate the model\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n#param_grid\nparam_grid_refined = {\n    'max_depth': [3,4,5],  \n    'min_samples_split': [8,10,15],\n    'min_samples_leaf': [3,4,5,6],   \n    'criterion': ['entropy'],        \n    'class_weight': ['balanced']  \n}\n\ngrid_search_refined = GridSearchCV(\n    DecisionTreeClassifier(random_state=42),\n    param_grid_refined,\n    cv=5,\n    scoring='f1_macro', \n    n_jobs=-1\n)\n\n# train mode\ngrid_search_refined.fit(X_train_scaled, y_train)\n\nprint(\"Refined Best parameters:\", grid_search_refined.best_params_)\nprint(\"Refined Best cross-validation score:\", grid_search_refined.best_score_)\n\nbest_model_refined = DecisionTreeClassifier(\n    **grid_search_refined.best_params_\n)\n\nbest_model_refined.fit(X_train_scaled, y_train)\n\ny_pred_refined = best_model_refined.predict(X_test_scaled)\nprint(\"\\nRefined Test set accuracy:\", accuracy_score(y_test, y_pred_refined))\nprint(\"\\nRefined Classification Report:\\n\", classification_report(y_test, y_pred_refined))\nrandom forest\nf_model = RandomForestClassifier(n_estimators=100)\n\n# Train the model\nrf_model.fit(X_train_scaled, y_train)\n\n# Predict on the test set\ny_pred_rf = rf_model.predict(X_test_scaled)\n\n# Evaluate the model\nprint(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\nprint(\"\\nRandom Forest Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n\n# Cross-validation to get a better estimate of the model performance\ncross_val_scores_rf = cross_val_score(rf_model, X_train_scaled, y_train, cv=5)\nprint(\"\\nRandom Forest Cross-validation scores:\", cross_val_scores_rf)\nprint(\"\\nMean cross-validation score (Random Forest):\", cross_val_scores_rf.mean())"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#result",
    "href": "technical-details/supervised-learning/instructions.html#result",
    "title": "Instructions",
    "section": "Result",
    "text": "Result\n\nModel performance Result\nComparison of Model Performance Metrics for Different Classification Models Used in Predicting like_ratio (Multi-Class Classification)\n\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nPrecision\nRecall\nF1-Score\n\n\n\n\nLogistic Regression\n77.33%\n0.64 (High), 0.86 (Low), 0.73 (Medium)\n0.82 (High), 0.86 (Low), 0.66 (Medium)\n0.72 (High), 0.86 (Low), 0.69 (Medium)\n\n\nDecision Tree\n74.67%\n0.56 (High), 0.85 (Low), 0.74 (Medium)\n0.91 (High), 0.83 (Low), 0.59 (Medium)\n0.69 (High), 0.84 (Low), 0.65 (Medium)\n\n\nRandom Forest\n78.67%\n0.69 (High), 0.88 (Low), 0.73 (Medium)\n0.82 (High), 0.80 (Low), 0.76 (Medium)\n0.75 (High), 0.84 (Low), 0.75 (Medium)\n\n\n\nCross-Validation Scores\n\n\n\n\n\n\n\n\nModel\nCross-Validation Scores\nMean Cross-Validation Score\n\n\n\n\nLogistic Regression\n[0.6533, 0.6133, 0.8133, 0.72, 0.4667]\n0.6533\n\n\nDecision Tree\nRefined Best CV: [0.6208 (balanced parameters)]\n0.6208\n\n\nRandom Forest\n[0.75, 0.6833, 0.6833, 0.7, 0.7833]\n0.72\n\n\n\n\n\nResult Interpretation\nWe can see from the table that the Random Forest model is the most effective model for predicting the like_ratio, achieving the highest accuracy (78.67%) and a balanced performance in terms of Precision, Recall, and F1-Score across all classes (High, Low, and Medium). Its average cross-validation score is 0.72, demonstrating good generalization.\nThe Logistic regression model is second effective one with an overall accuracy of 77.33%. While the low rank Precision (0.86) is strong, the medium rank Recall (0.66) performed relatively weakly. Nonetheless, the model is metrically consistent, showing its suitability for applications that require interpretability and stable performance.\nFor the Decision Tree model, it has the lowest overall accuracy (74.67%). While it performs well in recognizing high levels with a recall of 0.91, it performs poorly in recognizing intermediate levels where it has the lowest accuracy (0.74) and recall (0.59). But it can distinguish between high & low cases well."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#result-interpretation-2",
    "href": "technical-details/supervised-learning/instructions.html#result-interpretation-2",
    "title": "Instructions",
    "section": "Result Interpretation",
    "text": "Result Interpretation\nWe can see from the table that the Random Forest model is the most effective model for predicting the like_ratio, achieving the highest accuracy (78.67%) and a balanced performance in terms of Precision, Recall, and F1-Score across all classes (High, Low, and Medium). Its average cross-validation score is 0.72, demonstrating good generalization.\nThe Logistic regression model is second effective one with an overall accuracy of 77.33%. While the low rank Precision (0.86) is strong, the medium rank Recall (0.66) performed relatively weakly. Nonetheless, the model is metrically consistent, showing its suitability for applications that require interpretability and stable performance.\nFor the Decision Tree model, it has the lowest overall accuracy (74.67%). While it performs well in recognizing high levels with a recall of 0.91, it performs poorly in recognizing intermediate levels where it has the lowest accuracy (0.74) and recall (0.59). But it can distinguish between high & low cases well."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#insights-1",
    "href": "technical-details/supervised-learning/instructions.html#insights-1",
    "title": "Instructions",
    "section": "Insights",
    "text": "Insights\nFrom the results, we can find that the Random Forest model shows the best balance of accuracy, precision, recall, and F1 value in predicting the like_ratio of songs, especially in the medium class (Medium). This indicates that Random Forest has a strong ability in dealing with multi-categorization problems. The Logistic Regression model, on the other hand, has high precision and recall in identifying the low like_ratio category, suggesting that it is more effective in distinguishing between low popularity songs. while the Decision Tree model’s perform is not so good as we expected among all these three models, with low recall especially in the Medium category, suggesting that it may be unsuitable for dealing with such a complex multi-class classification task\nThis multi-class classification enables me to better understand the advantages and limitations of different models in multi-class classification tasks, and teach me lesson that we should choose the right model for future projects based on the specific needs of the task."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-performance-result",
    "href": "technical-details/supervised-learning/instructions.html#model-performance-result",
    "title": "Instructions",
    "section": "Model performance Result",
    "text": "Model performance Result\nComparison of Model Performance Metrics for Different Classification Models Used in Predicting like_ratio (Multi-Class Classification)\n\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nPrecision\nRecall\nF1-Score\n\n\n\n\nLogistic Regression\n77.33%\n0.64 (High), 0.86 (Low), 0.73 (Medium)\n0.82 (High), 0.86 (Low), 0.66 (Medium)\n0.72 (High), 0.86 (Low), 0.69 (Medium)\n\n\nDecision Tree\n74.67%\n0.56 (High), 0.85 (Low), 0.74 (Medium)\n0.91 (High), 0.83 (Low), 0.59 (Medium)\n0.69 (High), 0.84 (Low), 0.65 (Medium)\n\n\nRandom Forest\n78.67%\n0.69 (High), 0.88 (Low), 0.73 (Medium)\n0.82 (High), 0.80 (Low), 0.76 (Medium)\n0.75 (High), 0.84 (Low), 0.75 (Medium)\n\n\n\nCross-Validation Scores\n\n\n\n\n\n\n\n\nModel\nCross-Validation Scores\nMean Cross-Validation Score\n\n\n\n\nLogistic Regression\n[0.6533, 0.6133, 0.8133, 0.72, 0.4667]\n0.6533\n\n\nDecision Tree\nRefined Best CV: [0.6208 (balanced parameters)]\n0.6208\n\n\nRandom Forest\n[0.75, 0.6833, 0.6833, 0.7, 0.7833]\n0.72\n\n\n\n\nResult Interpretation\nWe can see from the table that the Random Forest model is the most effective model for predicting the like_ratio, achieving the highest accuracy (78.67%) and a balanced performance in terms of Precision, Recall, and F1-Score across all classes (High, Low, and Medium). Its average cross-validation score is 0.72, demonstrating good generalization.\nThe Logistic regression model is second effective one with an overall accuracy of 77.33%. While the low rank Precision (0.86) is strong, the medium rank Recall (0.66) performed relatively weakly. Nonetheless, the model is metrically consistent, showing its suitability for applications that require interpretability and stable performance.\nFor the Decision Tree model, it has the lowest overall accuracy (74.67%). While it performs well in recognizing high levels with a recall of 0.91, it performs poorly in recognizing intermediate levels where it has the lowest accuracy (0.74) and recall (0.59). But it can distinguish between high & low cases well."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#result",
    "href": "technical-details/supervised-learning/main.html#result",
    "title": "Supervised Learning",
    "section": "Result",
    "text": "Result\n\nModel performance Result\nComparison of Model Performance Metrics for Different Classification Models Used in Predicting like_ratio (Multi-Class Classification)\n\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nPrecision\nRecall\nF1-Score\n\n\n\n\nLogistic Regression\n77.33%\n0.64 (High), 0.86 (Low), 0.73 (Medium)\n0.82 (High), 0.86 (Low), 0.66 (Medium)\n0.72 (High), 0.86 (Low), 0.69 (Medium)\n\n\nDecision Tree\n74.67%\n0.56 (High), 0.85 (Low), 0.74 (Medium)\n0.91 (High), 0.83 (Low), 0.59 (Medium)\n0.69 (High), 0.84 (Low), 0.65 (Medium)\n\n\nRandom Forest\n78.67%\n0.69 (High), 0.88 (Low), 0.73 (Medium)\n0.82 (High), 0.80 (Low), 0.76 (Medium)\n0.75 (High), 0.84 (Low), 0.75 (Medium)\n\n\n\nCross-Validation Scores\n\n\n\n\n\n\n\n\nModel\nCross-Validation Scores\nMean Cross-Validation Score\n\n\n\n\nLogistic Regression\n[0.6533, 0.6133, 0.8133, 0.72, 0.4667]\n0.6533\n\n\nDecision Tree\nRefined Best CV: [0.6208 (balanced parameters)]\n0.6208\n\n\nRandom Forest\n[0.75, 0.6833, 0.6833, 0.7, 0.7833]\n0.72\n\n\n\n\n\nResult Interpretation\nWe can see from the table that the Random Forest model is the most effective model for predicting the like_ratio, achieving the highest accuracy (78.67%) and a balanced performance in terms of Precision, Recall, and F1-Score across all classes (High, Low, and Medium). Its average cross-validation score is 0.72, demonstrating good generalization.\nThe Logistic regression model is second effective one with an overall accuracy of 77.33%. While the low rank Precision (0.86) is strong, the medium rank Recall (0.66) performed relatively weakly. Nonetheless, the model is metrically consistent, showing its suitability for applications that require interpretability and stable performance.\nFor the Decision Tree model, it has the lowest overall accuracy (74.67%). While it performs well in recognizing high levels with a recall of 0.91, it performs poorly in recognizing intermediate levels where it has the lowest accuracy (0.74) and recall (0.59). But it can distinguish between high & low cases well."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#insights-1",
    "href": "technical-details/supervised-learning/main.html#insights-1",
    "title": "Supervised Learning",
    "section": "Insights",
    "text": "Insights\nFrom the results, we can find that the Random Forest model shows the best balance of accuracy, precision, recall, and F1 value in predicting the like_ratio of songs, especially in the medium class (Medium). This indicates that Random Forest has a strong ability in dealing with multi-categorization problems. The Logistic Regression model, on the other hand, has high precision and recall in identifying the low like_ratio category, suggesting that it is more effective in distinguishing between low popularity songs. while the Decision Tree model’s perform is not so good as we expected among all these three models, with low recall especially in the Medium category, suggesting that it may be unsuitable for dealing with such a complex multi-class classification task\nThis multi-class classification enables me to better understand the advantages and limitations of different models in multi-class classification tasks, and teach me lesson that we should choose the right model for future projects based on the specific needs of the task."
  },
  {
    "objectID": "technical-details/progress-log.html#member-1yiqin-zhou",
    "href": "technical-details/progress-log.html#member-1yiqin-zhou",
    "title": "Progress log",
    "section": "Member-1:Yiqin Zhou",
    "text": "Member-1:Yiqin Zhou\nClick here to open\nWeekly project contribution log: T: 12-15-2024\n\nCompleted final report section\n\nCompleted progess- log section\n\nCompleted unsupervised learning multi-class classification section\n\nF: 12-14-2024\n\nCompleted EDA section\n\nTh: 12-13-2024\n\nCompleted data collection\n\nW: 12-12-2024\n\nConfirmed data source in group meeting and began data collection\n\nT: 12-11-2024\n\nHeld group meeting to decide on project topic due to changes in Spotify API permissions\n\nW: 11-21-2024\n\nParticipated in group discussion to decide on project topic and data source"
  },
  {
    "objectID": "technical-details/progress-log.html#member-2xinzhou-li",
    "href": "technical-details/progress-log.html#member-2xinzhou-li",
    "title": "Progress log",
    "section": "Member-2:Xinzhou Li",
    "text": "Member-2:Xinzhou Li\nClick here to open\nProvide their name, a link to their “About Me” page.\nAlso, describe a log of their project roles.\nWeekly project contribution log:\nW: 10-16-2024 （星期，和日期）从近到远的顺序罗列\n\nAttend first group meeting"
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#k-means-clustering",
    "href": "technical-details/unsupervised-learning/instructions.html#k-means-clustering",
    "title": "Dimensionality Reduction",
    "section": "K-Means Clustering",
    "text": "K-Means Clustering\nK-Means clustering was applied to both PCA and t-SNE processed data. The algorithm partitions the data into K mutually exclusive clusters by assigning each data point to the cluster with the nearest mean. This method is effective in producing spherical clusters where the centroid represents the mean of the cluster’s points. The number of clusters, K, was set to 3 based on domain knowledge and preliminary analysis."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#dbscan",
    "href": "technical-details/unsupervised-learning/instructions.html#dbscan",
    "title": "Dimensionality Reduction",
    "section": "DBSCAN",
    "text": "DBSCAN\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) was utilized to identify arbitrarily shaped clusters based on density. It categorizes data points into clusters when they are closely packed together, while points in low-density areas are labeled as noise. This method is particularly effective for data with noise and outliers."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#hierarchical-clustering",
    "href": "technical-details/unsupervised-learning/instructions.html#hierarchical-clustering",
    "title": "Dimensionality Reduction",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nHierarchical clustering was performed using the Ward method, which minimizes the total within-cluster variance. At each step, the pair of clusters with the minimum between-cluster distance are merged. This method is well-suited for identifying hierarchical relationships between clusters."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#results-section",
    "href": "technical-details/unsupervised-learning/instructions.html#results-section",
    "title": "Dimensionality Reduction",
    "section": "Results Section",
    "text": "Results Section\n\nVisualizing Cluster Results\nThe clustering results from K-Means, DBSCAN, and Hierarchical clustering were visualized using scatter plots, clearly labeled with clusters identified from PCA and t-SNE processed data. Each visualization helps in understanding the cluster distribution and separation.\n\nimport numpy as np\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.cluster import AgglomerativeClustering\nimport matplotlib.pyplot as plt\n\n# K-Means Clustering\nkmeans = KMeans(n_clusters=3, random_state=42)\nkmeans_tsne = kmeans.fit(X_tsne)\nkmeans_tsne_labels = kmeans_tsne.labels_\n\n# DBSCAN\ndbscan = DBSCAN(eps=1, min_samples=5)\ndbscan_tsne = dbscan.fit(X_tsne)\ndbscan_tsne_labels = dbscan_tsne.labels_\n\n# Hierarchical Clustering\nhierarchical = AgglomerativeClustering(n_clusters=3)\nhierarchical_tsne = hierarchical.fit(X_tsne)\nhierarchical_tsne_labels = hierarchical_tsne.labels_\n\n# Plotting the results\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# K-Means Plot\naxes[0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=kmeans_tsne_labels, cmap='viridis', marker='o')\naxes[0].set_title('K-Means Clustering on t-SNE Data')\naxes[0].set_xlabel('t-SNE Axis 1')\naxes[0].set_ylabel('t-SNE Axis 2')\n\n# DBSCAN Plot\naxes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=dbscan_tsne_labels, cmap='viridis', marker='o')\naxes[1].set_title('DBSCAN Clustering on t-SNE Data')\naxes[1].set_xlabel('t-SNE Axis 1')\naxes[1].set_ylabel('t-SNE Axis 2')\n\n# Hierarchical Plot\naxes[2].scatter(X_tsne[:, 0], X_tsne[:, 1], c=hierarchical_tsne_labels, cmap='viridis', marker='o')\naxes[2].set_title('Hierarchical Clustering on t-SNE Data')\naxes[2].set_xlabel('t-SNE Axis 1')\naxes[2].set_ylabel('t-SNE Axis 2')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nResults Insights\n\nK-Means Clustering:\n\nK-Means with n_clusters=3 segmented the data into three distinct groups. The clusters are relatively well-separated with slight overlap, indicating a clear grouping pattern in the dataset.\n\nDBSCAN Clustering:\n\nDBSCAN with eps=1 and min_samples=5 identified varying densities within the dataset. This method has successfully differentiated the dense core groups from sparser outliers, which are not included in any cluster and are labeled as noise. But there is overlap between different groups.\n\nHierarchical Clustering:\n\nHierarchical clustering revealed three main clusters, with results similar to K-Means but differing slightly in the cluster boundaries.\n\n\n\n\nPerformance Comparison\n\nSilhouette Scores:\n\nK-Means achieved the highest Silhouette Score of 0.463882, suggesting a good level of separation and cohesion within clusters.\nHierarchical Clustering followed closely with a Silhouette Score of 0.456777.\nDBSCAN had a lower Silhouette Score of 0.3538208, indicating more overlap or less distinct clustering compared to the other methods.\n\n\nSo K-Means has the best performance on the t-sne processed dataset.\n\n\nConclusion\nFrom the visualization, we can see there is chance for the dataset to be classified into groups, indicating there may be differences beween different songs for their popularity according to our features given. And in the future this can be used for popularity forecast."
  }
]