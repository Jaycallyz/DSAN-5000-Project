# Instructions 

`Note`: You should remove these instructions once you have read and understood them. They should not be included in your final submission.

`Remember:` Exactly what do you on this page will be specific you your project and data. Some things might "make more sense" on other pages, depending on your workflow, for example, you might feel that normalization and scaling should be included in a later section, dealing with machine learning, rather than here, that is totally fine. Organize your project in the way that makes the most sense to you.  

## Suggested page structure

Hereâ€™s one suggested structure for organizing your technical pages. You can adjust this as needed:

`Audience`:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts. 

- **Introduction and Motivation:** Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.
- **Overview of Methods:** Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.
- **Code:** Include the code you used to implement your workflow.
- **Summary and Interpretation of Results:** Summarize your findings, interpret the results, and discuss their technical implications.

## General comments:

- **Iterative Process**: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.
- **Clarity and Reproducibility**: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.
- **Visualizations**: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.

By the end of this phase, your cleaned data should be well-documented and ready for further stages, such as **Exploratory Data Analysis (EDA)** and **Machine Learning**.

## What to address 

The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.

The **Data Cleaning** page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.

The following is a guide to help you get started with possible thing to address on this page .

- **Description of the Data Cleaning Process**: Explain the steps you took to clean and preprocess the data.
- **Code Documentation**: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).
- **Provide examples of data before and after cleaning**: e.g. with df.head() or df.describe()
- **Raw and Cleaned Data Links**: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in `data/processed-data`, or similar location which doesn't get synced to GitHub)

Possible things to include:

**Introduction to Data Cleaning**:

- Provide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.
- Mention that data cleaning may need to be revisited as the project evolves and analysis goals change.

**Managing Missing Data**:

- **Identify Missing Values**: Explain how you identified missing data and where it occurred.
- **Handling Missing Data**: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).
- **Visualize Missing Data**: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.

**Outlier Detection and Treatment**:

- **Identify Outliers**: Describe the methods you used to detect outliers in the dataset.
- **Addressing Outliers**: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).
- **Visualize Outliers**: Use visualizations (e.g., box plots) to show how outliers were managed.


**Data Type Correction and Formatting**:

- **Review Data Types**: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.
- **Transformation**: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.
- **Impact of Changes**: Briefly explain why these changes were necessary for accurate analysis.

**Normalization and Scaling**:

- **Data Distribution Analysis**: Check and discuss the distribution of numerical variables (e.g., skewness).
- **Normalization Techniques**: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).
- **Before-and-After Visualizations**: Provide visualizations comparing the data before and after scaling or normalization.

**Subsetting the Data**:

- **Data Filtering**: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).
- **Rationale**: Justify why you chose to work with a particular subset of the data.


```{python}
# Define your markdown content as a multi-line string
markdown_content = """
# Data Cleaning and Preprocessing Documentation

## Introduction and Motivation
This document outlines the steps taken to clean and preprocess the data from a dataset containing comments from Spotify and YouTube. The primary goal is to prepare the data for sentiment analysis and further statistical modeling, focusing on extracting meaningful insights about song popularity and engagement.

## Overview of Methods
The methods used include text cleaning, language detection, sentiment analysis, data transformation, and normalization. These techniques ensure that the data is in an appropriate format for analysis, removing any inconsistencies and ensuring quality and accuracy in the results.

## Code Implementation and Description

### Data Reading
The raw data is loaded from a CSV file using Pandas, which provides a convenient framework for data manipulation in Python.

\```python
df = pd.read_csv('../../data/raw-data/spotify_youtube.csv', encoding='iso-8859-1')
\```

### Comment Cleaning and Language Filtering
Comments are cleaned by removing HTML tags, punctuation, numbers, and ensuring they are in English. This is crucial for the accuracy of the sentiment analysis.

\```python
def clean_comments(comments):
    # Cleaning code here...
    return english_comments
\```

### Sentiment Analysis
Using NLTK's VADER, we perform sentiment analysis on the cleaned English comments. This provides a mean sentiment score for each record, indicating the overall sentiment of the comments associated with each song.

\```python
def average_sentiment_score(comments):
    # Sentiment analysis code here...
    return score
\```

### Data Transformations
Several transformations are applied to the dataset:
- Converting video duration from ISO 8601 format to seconds.
- Binary encoding of video definition (HD or SD).
- Factorizing the 'genre' column to prepare for modeling.

\```python
df['Duration_seconds'] = df['Duration'].apply(duration_to_seconds)
df['Definition'] = df['Definition'].apply(convert_definition)
df['genre_label'] = pd.factorize(df['genre'])[0]
\```

### Data Normalization
Using `StandardScaler`, numerical columns are scaled to have zero mean and unit variance. This step is important for models that are sensitive to the magnitude of input features.

\```python
scaler = StandardScaler()
df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])
\```

### Data Storage
The processed data is saved back to a CSV file, ensuring that all modifications are preserved for subsequent analysis.

\```python
df.to_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv', index=False)
\```

## Summary and Interpretation of Results
The preprocessing steps significantly cleaned and transformed the raw data, making it suitable for accurate and insightful analysis. The sentiment scores provide a quantitative measure of public perception, which, combined with other song metrics, can be used to gauge popularity and engagement.

By documenting each step in this markdown file, we ensure that the process is transparent and reproducible, facilitating peer review and future analyses.
"""

# Write the content to a markdown file
with open('Data_Processing_Documentation.md', 'w') as file:
    file.write(markdown_content)

print("Markdown file has been created successfully.")
```