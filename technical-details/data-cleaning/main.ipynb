{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Cleaning\"\n",
    "format:\n",
    "    html: \n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- After digesting the instructions, you can delete this cell, these are assignment instructions and do not need to be included in your final submission.  -->\n",
    "\n",
    "{{< include instructions.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code \n",
    "\n",
    "Provide the source code used for this section of the project here.\n",
    "\n",
    "If you're using a package for code organization, you can import it at this point. However, make sure that the **actual workflow steps**—including data processing, analysis, and other key tasks—are conducted and clearly demonstrated on this page. The goal is to show the technical flow of your project, highlighting how the code is executed to achieve your results.\n",
    "\n",
    "If relevant, link to additional documentation or external references that explain any complex components. This section should give readers a clear view of how the project is implemented from a technical perspective.\n",
    "\n",
    "Remember, this page is a technical narrative, NOT just a notebook with a collection of code cells, include in-line Prose, to describe what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:58: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:58: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/var/folders/1f/twjj6zg12057l62kw8fwf3f40000gn/T/ipykernel_83653/2869829918.py:58: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  pattern = re.compile('PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?')\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/CuiCuiLee/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Comments  Mean Sentiment Score\n",
      "0  ['Can we bring this back? This feeling? This m...              0.408180\n",
      "1  ['Stop asking who&#39;s still listening, we ne...              0.103640\n",
      "2  ['How can you not get chills listening to this...              0.626529\n",
      "3  ['My son and I was supposed to spend the summe...              0.681220\n",
      "4  ['What a talent Dave is. Drummer, singer, guit...              0.431156\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from langdetect import detect, DetectorFactory, LangDetectException\n",
    "\n",
    "# Set a consistent seed for langdetect\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Download VADER lexicon if not already downloaded\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize VADER\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Read CSV file\n",
    "df = pd.read_csv('../../data/raw-data/youTube_Video_Data_all.csv', encoding='iso-8859-1')\n",
    "\n",
    "def clean_comments(comments):\n",
    "    english_comments = []\n",
    "    try:\n",
    "        comments = eval(comments)  # Converts string representation of list to list\n",
    "        for comment in comments:\n",
    "            # Remove HTML tags and additional cleaning\n",
    "            comment_text = re.sub(r'<[^>]+>', '', comment)\n",
    "            comment_text = comment_text.replace('_', '')\n",
    "            comment_text = re.sub(r\"[^\\w\\s]\", \"\", comment_text)  # Remove punctuation\n",
    "            comment_text = re.sub(r\"\\d+\", \"\", comment_text)  # Remove numbers\n",
    "            comment_text = comment_text.lower().strip()\n",
    "            # Detect language after cleaning\n",
    "            if detect(comment_text) == 'en':\n",
    "                english_comments.append(comment_text)\n",
    "    except LangDetectException:\n",
    "        pass  # Ignore comments where language detection fails\n",
    "    except SyntaxError:\n",
    "        pass  # Handle syntax error if eval fails\n",
    "    return english_comments\n",
    "\n",
    "def average_sentiment_score(comments):\n",
    "    # Clean comments first\n",
    "    clean_english_comments = clean_comments(comments)\n",
    "    # Perform sentiment analysis on cleaned English comments\n",
    "    if clean_english_comments:\n",
    "        scores = [sia.polarity_scores(comment)['compound'] for comment in clean_english_comments]\n",
    "        return sum(scores) / len(scores) if scores else 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the function to compute the mean sentiment score\n",
    "df['Mean Sentiment Score'] = df['Comments'].apply(average_sentiment_score)\n",
    "\n",
    "# Add one more col filled with processed comments\n",
    "df['Processed_Comments'] = df['Comments'].apply(clean_comments)\n",
    "\n",
    "# Transfer the duration to seconds\n",
    "def duration_to_seconds(duration):\n",
    "    # Regular expression to extract the time parts from the ISO 8601 format\n",
    "    pattern = re.compile('PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?')\n",
    "    parts = pattern.match(duration)\n",
    "    \n",
    "    # Extract hours, minutes, and seconds from the match object\n",
    "    hours = int(parts.group(1)) if parts.group(1) else 0\n",
    "    minutes = int(parts.group(2)) if parts.group(2) else 0\n",
    "    seconds = int(parts.group(3)) if parts.group(3) else 0\n",
    "    \n",
    "    # Convert all to seconds\n",
    "    total_seconds = hours * 3600 + minutes * 60 + seconds\n",
    "    return total_seconds\n",
    "\n",
    "df['Duration_seconds'] = df['Duration'].apply(duration_to_seconds)\n",
    "\n",
    "# Replace Definition with hd=1, sd=0\n",
    "def convert_definition(definition):\n",
    "    if definition == 'hd':\n",
    "        return 1\n",
    "    elif definition == 'sd':\n",
    "        return 0\n",
    "    return None\n",
    "df['Definition'] = df['Definition'].apply(convert_definition)\n",
    "\n",
    "# Factorize the 'genre' column\n",
    "labels, unique = pd.factorize(df['genre'])\n",
    "\n",
    "# Add the labels as a new column in the DataFrame\n",
    "df['genre_label'] = labels\n",
    "\n",
    "# Display DataFrame head\n",
    "#print(df[['Comments', 'Mean Sentiment Score']].head())\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df.to_csv('../../data/processed-data/Updated_Data_with_Sentiments.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Days Since Published  View Count  Like Count  Comment Count  \\\n",
      "0              1.205386   -0.249444   -0.385191      -0.239650   \n",
      "1              1.205386   -0.051546   -0.146501      -0.175819   \n",
      "2              1.205386    0.319997    0.080880      -0.046905   \n",
      "3              0.937583   -0.385250   -0.442649      -0.265280   \n",
      "4              1.204883   -0.134397   -0.273697      -0.235877   \n",
      "\n",
      "   Subscriber Count  Definition  Mean Sentiment Score  Duration_seconds  \\\n",
      "0         -0.564071     0.44614              0.398865         -0.068896   \n",
      "1         -0.564071     0.44614             -0.959089         -0.035163   \n",
      "2         -0.564071     0.44614              1.372489         -0.082390   \n",
      "3         -0.564071     0.44614              1.616361         -0.145358   \n",
      "4         -0.564071     0.44614              0.501314         -0.116123   \n",
      "\n",
      "   genre_label  \n",
      "0    -1.414214  \n",
      "1    -1.414214  \n",
      "2    -1.414214  \n",
      "3    -1.414214  \n",
      "4    -1.414214  \n"
     ]
    }
   ],
   "source": [
    "# data standarlized\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "columns_to_scale = [\n",
    "    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',\n",
    "    'Subscriber Count', 'Definition', 'Mean Sentiment Score',\n",
    "    'Duration_seconds', 'genre_label'\n",
    "]\n",
    "\n",
    "# initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "\n",
    "print(df[columns_to_scale].head())\n",
    "\n",
    "df.to_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
