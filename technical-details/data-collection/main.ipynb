{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Collection\"\n",
    "format:\n",
    "    html: \n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include instructions.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "{{< include overview.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include methods.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code \n",
    "\n",
    "Provide the source code used for this section of the project here.\n",
    "\n",
    "If you're using a package for code organization, you can import it at this point. However, make sure that the **actual workflow steps**—including data processing, analysis, and other key tasks—are conducted and clearly demonstrated on this page. The goal is to show the technical flow of your project, highlighting how the code is executed to achieve your results.\n",
    "\n",
    "Ensure that the code is well-commented to enhance readability and understanding for others who may review or use it. If relevant, link to additional documentation or external references that explain any complex components. This section should give readers a clear view of how the project is implemented from a technical perspective.\n",
    "\n",
    "This page is a technical narrative, NOT just a notebook with a collection of code cells, include in-line Prose, to describe what is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "In the following code, we first utilized the requests library to retrieve the HTML content from the Wikipedia page. Afterward, we employed BeautifulSoup to parse the HTML and locate the specific table of interest by using the find function. Once the table was identified, we extracted the relevant data by iterating through its rows, gathering country names and their respective populations. Finally, we used Pandas to store the collected data in a DataFrame, allowing for easy analysis and visualization. The data could also be optionally saved as a CSV file for further use. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "This project collects music data through YouTube and Spotify APIs, covering information on the works of 20 representative artists in five genres: Electronic, Jazz, Hip-Hop, Pop and Rock. The data processing flow is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. YouTube Data Collection\n",
    "#### 1.1 Acquiring Official Music Video Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Video ID                                              Title  \\\n",
      "0  1VQ_3sBZEm0    Foo Fighters - Learn To Fly (Official HD Video)   \n",
      "1  eBG7P-K-r1Y        Foo Fighters - Everlong (Official HD Video)   \n",
      "2  SBjQ9tuuTJQ                       Foo Fighters - The Pretender   \n",
      "3  EqWRaAF6_WY         Foo Fighters - My Hero (Official HD Video)   \n",
      "4  h_L4Rixya64  Foo Fighters - Best Of You (Official Music Video)   \n",
      "\n",
      "                                         Description          Published At  \\\n",
      "0  Foo Fighters' official music video for 'Learn ...  2009-10-03T04:46:13Z   \n",
      "1  \"Everlong\" by Foo Fighters \\nListen to Foo Fig...  2009-10-03T04:49:58Z   \n",
      "2  Watch the official music video for \"The Preten...  2009-10-03T04:46:14Z   \n",
      "3  \"My Hero\" by Foo Fighters \\nListen to Foo Figh...  2011-03-18T19:35:42Z   \n",
      "4  Watch the official music video for \"Best Of Yo...  2009-10-03T20:49:33Z   \n",
      "\n",
      "   Days Since Published View Count Like Count Comment Count  \\\n",
      "0                  5550  183921366     808172         33856   \n",
      "1                  5550  324414087    1821270         53201   \n",
      "2                  5550  588092620    2785700         92245   \n",
      "3                  5018   87531478     564448         26099   \n",
      "4                  5549  265573360    1281212         34999   \n",
      "\n",
      "                                            Comments Subscriber Count  \\\n",
      "0  [I’m just realising how great Dave grohls acti...          1290000   \n",
      "1  [Dad died today. \\r<br>1:20 am.\\r<br>A five da...          1290000   \n",
      "2  [So thankful for this awesome song. I&#39;ll b...          1290000   \n",
      "3  [My son and I was supposed to spend the summer...          1290000   \n",
      "4  [The emotion in his face and his voice transce...          1290000   \n",
      "\n",
      "  Category ID Definition Duration  \n",
      "0          10         hd  PT4M37S  \n",
      "1          10         hd  PT4M52S  \n",
      "2          10         hd  PT4M31S  \n",
      "3          10         hd   PT4M3S  \n",
      "4          10         hd  PT4M16S  \n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import dateutil.parser\n",
    "\n",
    "# API Key\n",
    "api_key = \"AIzaSyDtKE-4QZj6EA-rwG7cj5gMJxdt4Fe14Nw\"\n",
    "\n",
    "# Initialize YouTube API client\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# List to store data\n",
    "all_data = []\n",
    "\n",
    "# Read song data and fetch YouTube statistics\n",
    "# We should have put 20*5 names of singers, but for the sake of presentation, we choose three singers as a demonstration here \n",
    "with open('find.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        artist = line.strip()\n",
    "        query = f\"{artist} official music video\"\n",
    "\n",
    "        # Search request for the query\n",
    "        search_request = youtube.search().list(\n",
    "            part=\"snippet\",\n",
    "            q=query,\n",
    "            maxResults=5,\n",
    "            type=\"video\",\n",
    "            order='relevance'\n",
    "        )\n",
    "        search_response = search_request.execute()\n",
    "\n",
    "        for item in search_response['items']:\n",
    "            video_id = item['id']['videoId']\n",
    "            video_request = youtube.videos().list(\n",
    "                part=\"snippet,contentDetails,statistics\",\n",
    "                id=video_id\n",
    "            )\n",
    "            video_response = video_request.execute()\n",
    "\n",
    "            for video in video_response['items']:\n",
    "                snippet = video['snippet']\n",
    "                content_details = video['contentDetails']\n",
    "                statistics = video['statistics']\n",
    "\n",
    "                # Calculate days since video was published\n",
    "                published_at = dateutil.parser.parse(snippet['publishedAt'])\n",
    "                days_since_published = (datetime.now(published_at.tzinfo) - published_at).days\n",
    "\n",
    "                # Fetch channel details for subscriber count\n",
    "                channel_response = youtube.channels().list(\n",
    "                    part='statistics',\n",
    "                    id=snippet['channelId']\n",
    "                ).execute()\n",
    "                subscriber_count = channel_response['items'][0]['statistics'].get('subscriberCount', '0')\n",
    "\n",
    "                # Fetch comments\n",
    "                comments_request = youtube.commentThreads().list(\n",
    "                    part='snippet',\n",
    "                    videoId=video_id,\n",
    "                    order='relevance',\n",
    "                    maxResults=10\n",
    "                )\n",
    "                comments_response = comments_request.execute()\n",
    "\n",
    "                comments = [comment['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                            for comment in comments_response.get('items', [])]\n",
    "\n",
    "                # Store all data in a dictionary\n",
    "                video_data = {\n",
    "                    'Video ID': video_id,\n",
    "                    'Title': snippet['title'],\n",
    "                    'Description': snippet['description'],\n",
    "                    'Published At': snippet['publishedAt'],\n",
    "                    'Days Since Published': days_since_published,\n",
    "                    'View Count': statistics.get('viewCount', '0'),\n",
    "                    'Like Count': statistics.get('likeCount', '0'),\n",
    "                    'Comment Count': statistics.get('commentCount', '0'),\n",
    "                    'Comments': comments,\n",
    "                    'Subscriber Count': subscriber_count,\n",
    "                    'Category ID': snippet['categoryId'],\n",
    "                    'Definition': content_details['definition'],\n",
    "                    'Duration': content_details['duration']\n",
    "                }\n",
    "\n",
    "                all_data.append(video_data)\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "final_df = pd.DataFrame(all_data)\n",
    "\n",
    "# Optionally save the DataFrame to a CSV file\n",
    "final_df.to_csv('Example_Detailed_YouTube_Video_Data.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Merge artist name and music genre into csv\n",
    "The row of the initial find.csv(include artist name and genre) is repeated five times per row to correspond to the five mv chosen by each artist (python)\n",
    "Then merge these two columns into the csv (copy manually) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed successfully and saved as ./Example_singer_info.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file = './find.csv'  \n",
    "output_file = './Example_singer_info.csv'  \n",
    "\n",
    "# Load the input CSV file\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Create an empty DataFrame to store repeated rows\n",
    "repeated_df = pd.DataFrame()\n",
    "\n",
    "# Repeat each row 5 times and append it to the new DataFrame\n",
    "for i in range(len(df)):\n",
    "    repeated_df = pd.concat([repeated_df, pd.DataFrame([df.iloc[i]] * 5)], ignore_index=True)\n",
    "\n",
    "# Save the processed DataFrame to a new CSV file\n",
    "repeated_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Data processed successfully and saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Data preprocessing\n",
    "Extract the song name from the csv's title and generate a new CSV file containing the singer's and song's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song titles extracted and saved to ./Example_extracted_song_names.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the CSV file with YouTube video data\n",
    "df = pd.read_csv('./Example_Detailed_YouTube_Video_Data.csv', encoding='MacRoman')\n",
    "\n",
    "# Function to extract the song name from the title\n",
    "def extract_song_name(title):\n",
    "    # Use regular expression to find text between \" - \" and \"(\"\n",
    "    match = re.search(r' - (.*?) \\(.*\\)', title)\n",
    "    if match:\n",
    "        return match.group(1) \n",
    "    else:\n",
    "        return title  \n",
    "\n",
    "# Create a new DataFrame with extracted song names\n",
    "new_df = pd.DataFrame({\n",
    "    'Extracted Song Name': df['Title'].apply(extract_song_name)\n",
    "})\n",
    "\n",
    "# Save the new DataFrame to a CSV file in the current directory\n",
    "output_file = './Example_extracted_song_names.csv'\n",
    "new_df.to_csv(output_file, index=False, encoding='MacRoman')\n",
    "\n",
    "print(f\"Song titles extracted and saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to ./Example_extracted_song_names_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_file = './Example_extracted_song_names.csv'\n",
    "df = pd.read_csv(input_file, encoding='MacRoman')\n",
    "\n",
    "# Function to remove content inside brackets (e.g., [example])\n",
    "def remove_brackets(text):\n",
    "    return re.sub(r'\\[.*?\\]', '', text)\n",
    "\n",
    "# Apply the function to the 'Extracted Song Name' column\n",
    "df['Extracted Song Name'] = df['Extracted Song Name'].apply(remove_brackets)\n",
    "\n",
    "\n",
    "output_file = './Example_extracted_song_names_cleaned.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Processed data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually merge example_extracted_song_name_cleaned.csv with example_artist_info.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Spotify Collection\n",
    "#### 2.1 Acquiring Track Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: Learn to Fly by Foo Fighters\n",
      "Found: Everlong by Foo Fighters\n",
      "Found: The Pretender by Foo Fighters\n",
      "Found: My Hero by Foo Fighters\n",
      "Found: Best of You by Foo Fighters\n",
      "Found: Mr. Brightside by The Killers\n",
      "Found: When You Were Young by The Killers\n",
      "Found: Mr. Brightside by The Killers\n",
      "Found: Somebody Told Me by The Killers\n",
      "Found: One Empty Grave by A Sound of Thunder\n",
      "Found: Basket Case by Green Day\n",
      "Found: When I Come Around by Green Day\n",
      "Found: American Idiot by Green Day\n",
      "Found: Boulevard of Broken Dreams by Green Day\n",
      "Found: Wake Me up When September Ends by Green Day\n",
      "Processed data saved to: ./Example_spotify_track_info.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "# Set up Spotify client credentials\n",
    "client_credentials_manager = SpotifyClientCredentials(\n",
    "    client_id='3e1596de002340b898f5d10c9aeae4ea',\n",
    "    client_secret='526fa44678974475b0f6ba5d8efd16c4'\n",
    ")\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = './Example_extracted_song_names_cleaned.csv'\n",
    "output_file = './Example_spotify_track_info.csv'\n",
    "\n",
    "# Load the input CSV file\n",
    "df = pd.read_csv(input_file, encoding='MacRoman')\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Process each row in the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    song = row['Extracted Song Name']\n",
    "    artist = row['singer']\n",
    "    query = f'{song} {artist}'  # Concatenate song and artist for the search query\n",
    "\n",
    "    try:\n",
    "        # Search for the track on Spotify\n",
    "        result = sp.search(q=query, limit=1, type='track')\n",
    "        tracks = result.get('tracks', {}).get('items', [])\n",
    "\n",
    "        if tracks:\n",
    "            # If a track is found, extract its details\n",
    "            track = tracks[0]\n",
    "            track_info = {\n",
    "                'Track Name': track['name'],\n",
    "                'Artist Name': track['artists'][0]['name'],\n",
    "                'Album Name': track['album']['name'],\n",
    "                'Popularity': track['popularity'],\n",
    "                'Duration (ms)': track['duration_ms'],\n",
    "                'Track ID': track['id'],\n",
    "                'Spotify URL': track['external_urls']['spotify']\n",
    "            }\n",
    "            print(f\"Found: {track_info['Track Name']} by {track_info['Artist Name']}\")\n",
    "        else:\n",
    "            # If no track is found, append placeholders\n",
    "            print(f\"Track not found: {query}\")\n",
    "            track_info = {\n",
    "                'Track Name': song,\n",
    "                'Artist Name': artist,\n",
    "                'Album Name': None,\n",
    "                'Popularity': None,\n",
    "                'Duration (ms)': None,\n",
    "                'Track ID': None,\n",
    "                'Spotify URL': None\n",
    "            }\n",
    "\n",
    "        # Append the result to the list\n",
    "        results.append(track_info)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle exceptions during the search\n",
    "        print(f\"Error processing query '{query}': {e}\")\n",
    "        results.append({\n",
    "            'Track Name': song,\n",
    "            'Artist Name': artist,\n",
    "            'Album Name': None,\n",
    "            'Popularity': None,\n",
    "            'Duration (ms)': None,\n",
    "            'Track ID': None,\n",
    "            'Spotify URL': None\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "output_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the output DataFrame to a CSV file\n",
    "output_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Processed data saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Obtaining Artist Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to: ./Example_artist_data_with_followers_and_popularity.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Function to obtain an access token for Spotify API\n",
    "def get_access_token(client_id, client_secret):\n",
    "    auth_url = 'https://accounts.spotify.com/api/token'\n",
    "    auth_data = {\n",
    "        'grant_type': 'client_credentials',\n",
    "        'client_id': client_id,\n",
    "        'client_secret': client_secret\n",
    "    }\n",
    "    response = requests.post(auth_url, data=auth_data)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['access_token']\n",
    "    else:\n",
    "        raise Exception('Failed to obtain access token')\n",
    "\n",
    "# Function to get the Spotify artist ID using the artist name\n",
    "def get_artist_id(artist_name, access_token):\n",
    "    search_url = f'https://api.spotify.com/v1/search?q={artist_name}&type=artist&limit=1'\n",
    "    headers = {'Authorization': f'Bearer {access_token}'}\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        search_results = response.json()\n",
    "        artists = search_results['artists']['items']\n",
    "        if artists:\n",
    "            return artists[0]['id']\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to retrieve the artist's followers and popularity\n",
    "def get_artist_followers(artist_id, access_token):\n",
    "    artist_url = f'https://api.spotify.com/v1/artists/{artist_id}'\n",
    "    headers = {'Authorization': f'Bearer {access_token}'}\n",
    "    response = requests.get(artist_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        artist_data = response.json()\n",
    "        return artist_data['followers']['total'], artist_data['popularity']\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Load the input CSV file \n",
    "input_file = './Example_singer_info.csv' \n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Spotify API credentials\n",
    "client_id = '31aba57b31344fdebf98f51375d07834' \n",
    "client_secret = '2c4f929786784910bc9a843518785cae'  \n",
    "\n",
    "# Obtain Spotify API access token\n",
    "access_token = get_access_token(client_id, client_secret)\n",
    "\n",
    "# Initialize lists to store followers and popularity data\n",
    "followers_list = []\n",
    "popularity_list = []\n",
    "\n",
    "# Process each artist in the DataFrame\n",
    "for artist_name in df['artist']:\n",
    "    artist_id = get_artist_id(artist_name, access_token)\n",
    "    if artist_id:\n",
    "        followers, popularity = get_artist_followers(artist_id, access_token)\n",
    "        followers_list.append(followers)\n",
    "        popularity_list.append(popularity)\n",
    "    else:\n",
    "        # If the artist is not found, append None\n",
    "        followers_list.append(None)\n",
    "        popularity_list.append(None)\n",
    "\n",
    "# Add followers and popularity data to the DataFrame\n",
    "df['Followers'] = followers_list\n",
    "df['Popularity'] = popularity_list\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file = './Example_artist_data_with_followers_and_popularity.csv'  \n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Processed data saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data integration and cleansing\n",
    "First manually merge Spotify and YouTube csv.\n",
    "Then using Spotify and YouTube artists as the matching key, match to verify artist match, if not, then delete the mismatched rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to: ./Example_spotify_youtube.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file = './Example_Detailed_YouTube_Video_Data.csv' \n",
    "df = pd.read_csv(input_file, encoding='MacRoman')\n",
    "\n",
    "# Filter rows where 'Artist Name' matches 'Artist'\n",
    "df_cleaned = df[df['Artist Name'] == df['artist']]\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "output_file = './Example_spotify_youtube.csv'  \n",
    "df_cleaned.to_csv(output_file, index=False)\n",
    "print(f\"Cleaned data saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we completed all the data collection steps!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
