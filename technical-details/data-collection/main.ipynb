{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Collection\"\n",
    "format:\n",
    "    html: \n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include instructions.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "{{< include overview.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include methods.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code \n",
    "\n",
    "Provide the source code used for this section of the project here.\n",
    "\n",
    "If you're using a package for code organization, you can import it at this point. However, make sure that the **actual workflow steps**—including data processing, analysis, and other key tasks—are conducted and clearly demonstrated on this page. The goal is to show the technical flow of your project, highlighting how the code is executed to achieve your results.\n",
    "\n",
    "Ensure that the code is well-commented to enhance readability and understanding for others who may review or use it. If relevant, link to additional documentation or external references that explain any complex components. This section should give readers a clear view of how the project is implemented from a technical perspective.\n",
    "\n",
    "This page is a technical narrative, NOT just a notebook with a collection of code cells, include in-line Prose, to describe what is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "In the following code, we first utilized the requests library to retrieve the HTML content from the Wikipedia page. Afterward, we employed BeautifulSoup to parse the HTML and locate the specific table of interest by using the find function. Once the table was identified, we extracted the relevant data by iterating through its rows, gathering country names and their respective populations. Finally, we used Pandas to store the collected data in a DataFrame, allowing for easy analysis and visualization. The data could also be optionally saved as a CSV file for further use. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Country     Population\n",
      "0                                  World  8,119,000,000\n",
      "1                                  China  1,409,670,000\n",
      "2                          1,404,910,000          17.3%\n",
      "3                          United States    335,893,238\n",
      "4                              Indonesia    281,603,800\n",
      "..                                   ...            ...\n",
      "235                   Niue (New Zealand)          1,681\n",
      "236                Tokelau (New Zealand)          1,647\n",
      "237                         Vatican City            764\n",
      "238  Cocos (Keeling) Islands (Australia)            593\n",
      "239                Pitcairn Islands (UK)             35\n",
      "\n",
      "[240 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Send a request to Wikipedia page\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Parse the page content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Step 3: Find the table containing the data (usually the first table for such lists)\n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "# Step 4: Extract data from the table rows\n",
    "countries = []\n",
    "populations = []\n",
    "\n",
    "# Iterate over the table rows\n",
    "for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) > 1:\n",
    "        country = cells[1].text.strip()  # The country name is in the second column\n",
    "        population = cells[2].text.strip()  # The population is in the third column\n",
    "        countries.append(country)\n",
    "        populations.append(population)\n",
    "\n",
    "# Step 5: Create a DataFrame to store the results\n",
    "data = pd.DataFrame({\n",
    "    'Country': countries,\n",
    "    'Population': populations\n",
    "})\n",
    "\n",
    "# Display the scraped data\n",
    "print(data)\n",
    "\n",
    "# Optionally save to CSV\n",
    "data.to_csv('../../data/raw-data/countries_population.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>view_counts</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Taylor Swift - Anti-Hero (Official Music Video)</td>\n",
       "      <td>212893513</td>\n",
       "      <td>Anti-Hero by Taylor Swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Taylor Swift - Anti-Hero (Official Lyric Video)</td>\n",
       "      <td>34875876</td>\n",
       "      <td>Anti-Hero by Taylor Swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Taylor Swift - Anti-Hero (Lyrics)</td>\n",
       "      <td>14266885</td>\n",
       "      <td>Anti-Hero by Taylor Swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Taylor Swift - Anti Hero (Lyrics) &amp;quot;It&amp;#39...</td>\n",
       "      <td>5444496</td>\n",
       "      <td>Anti-Hero by Taylor Swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Taylor Swift - Anti-Hero</td>\n",
       "      <td>1332074</td>\n",
       "      <td>Anti-Hero by Taylor Swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Lorde - Tennis Court</td>\n",
       "      <td>131679562</td>\n",
       "      <td>Tennis Court by Lorde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Lorde - Tennis Court (Flume Remix)</td>\n",
       "      <td>115590517</td>\n",
       "      <td>Tennis Court by Lorde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Lorde - Tennis Court (Audio)</td>\n",
       "      <td>2182597</td>\n",
       "      <td>Tennis Court by Lorde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Lorde - Tennis Court (Glastonbury 2017)</td>\n",
       "      <td>365081</td>\n",
       "      <td>Tennis Court by Lorde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Tennis Court - Lorde (Lyrics) 🎵</td>\n",
       "      <td>179298</td>\n",
       "      <td>Tennis Court by Lorde</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                titles  view_counts  \\\n",
       "0      Taylor Swift - Anti-Hero (Official Music Video)    212893513   \n",
       "1      Taylor Swift - Anti-Hero (Official Lyric Video)     34875876   \n",
       "2                    Taylor Swift - Anti-Hero (Lyrics)     14266885   \n",
       "3    Taylor Swift - Anti Hero (Lyrics) &quot;It&#39...      5444496   \n",
       "4                             Taylor Swift - Anti-Hero      1332074   \n",
       "..                                                 ...          ...   \n",
       "110                               Lorde - Tennis Court    131679562   \n",
       "111                 Lorde - Tennis Court (Flume Remix)    115590517   \n",
       "112                       Lorde - Tennis Court (Audio)      2182597   \n",
       "113            Lorde - Tennis Court (Glastonbury 2017)       365081   \n",
       "114                    Tennis Court - Lorde (Lyrics) 🎵       179298   \n",
       "\n",
       "                         query  \n",
       "0    Anti-Hero by Taylor Swift  \n",
       "1    Anti-Hero by Taylor Swift  \n",
       "2    Anti-Hero by Taylor Swift  \n",
       "3    Anti-Hero by Taylor Swift  \n",
       "4    Anti-Hero by Taylor Swift  \n",
       "..                         ...  \n",
       "110      Tennis Court by Lorde  \n",
       "111      Tennis Court by Lorde  \n",
       "112      Tennis Court by Lorde  \n",
       "113      Tennis Court by Lorde  \n",
       "114      Tennis Court by Lorde  \n",
       "\n",
       "[115 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "\n",
    "# API Key\n",
    "api_key = \"AIzaSyDtKE-4QZj6EA-rwG7cj5gMJxdt4Fe14Nw\"\n",
    "\n",
    "# Initialize YouTube API client\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# List to store data\n",
    "all_data = []\n",
    "\n",
    "# Read song data and fetch YouTube statistics\n",
    "with open('song_data.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Strip newline characters and spaces\n",
    "        query = line.strip()\n",
    "\n",
    "        # Search request for the query\n",
    "        search_request = youtube.search().list(\n",
    "            part=\"snippet\",\n",
    "            q=query,  # Use the query from the file\n",
    "            maxResults=5,\n",
    "            type=\"video\",\n",
    "            order='relevance'\n",
    "        )\n",
    "        search_response = search_request.execute()\n",
    "\n",
    "        # Get video IDs\n",
    "        video_ids = [item['id']['videoId'] for item in search_response['items']]\n",
    "        if not video_ids:\n",
    "            continue  # Skip if no results\n",
    "\n",
    "        # Fetch video details (statistics)\n",
    "        video_request = youtube.videos().list(\n",
    "            part=\"statistics\",\n",
    "            id=\",\".join(video_ids)\n",
    "        )\n",
    "        video_response = video_request.execute()\n",
    "\n",
    "        # Collect results for the current query\n",
    "        query_data = []\n",
    "        for item, stats in zip(search_response['items'], video_response['items']):\n",
    "            query_data.append({\n",
    "                \"titles\": item['snippet']['title'],\n",
    "                \"view_counts\": int(stats['statistics']['viewCount']),\n",
    "                \"query\": query\n",
    "            })\n",
    "\n",
    "        # Convert query-specific data to a DataFrame and sort by view_counts\n",
    "        query_df = pd.DataFrame(query_data)\n",
    "        query_df = query_df.sort_values(by=\"view_counts\", ascending=False)\n",
    "\n",
    "        # Append the sorted data to the final list\n",
    "        all_data.append(query_df)\n",
    "\n",
    "# Concatenate all sorted query-specific DataFrames into one\n",
    "final_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('view_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# 设置 YouTube Data API 密钥和服务\n",
    "API_KEY = \"AIzaSyC5VGKOdaG9IW3lauaZ03yk0nkP3oS4cTc\"\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# 视频ID列表\n",
    "video_ids = [\n",
    "    \"EqDlrimnMCE\", \"m6N6jOt7heY\"\n",
    "]\n",
    "\n",
    "# 要提取的信息\n",
    "fields = [\n",
    "    \"videoId\", \"title\", \"description\", \"publishedAt\", \"tags\",\n",
    "    \"viewCount\", \"likeCount\", \"commentCount\", \"categoryId\", \"duration\",\n",
    "    \"dimension\", \"definition\"\n",
    "]\n",
    "\n",
    "# 存储结果的列表\n",
    "results = []\n",
    "\n",
    "# 遍历视频ID，获取相关数据\n",
    "def get_video_data(video_id):\n",
    "    request = youtube.videos().list(\n",
    "        part=\"snippet,statistics,contentDetails\",\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    if \"items\" in response and response[\"items\"]:\n",
    "        item = response[\"items\"][0]\n",
    "        snippet = item.get(\"snippet\", {})\n",
    "        statistics = item.get(\"statistics\", {})\n",
    "        content_details = item.get(\"contentDetails\", {})\n",
    "        \n",
    "        data = {\n",
    "            \"videoId\": video_id,\n",
    "            \"title\": snippet.get(\"title\", \"\"),\n",
    "            \"description\": snippet.get(\"description\", \"\"),\n",
    "            \"publishedAt\": snippet.get(\"publishedAt\", \"\"),\n",
    "            \"tags\": snippet.get(\"tags\", []),\n",
    "            \"viewCount\": statistics.get(\"viewCount\", \"0\"),\n",
    "            \"likeCount\": statistics.get(\"likeCount\", \"0\"),\n",
    "            \"commentCount\": statistics.get(\"commentCount\", \"0\"),\n",
    "            \"categoryId\": snippet.get(\"categoryId\", \"\"),\n",
    "            \"duration\": content_details.get(\"duration\", \"\"),\n",
    "            \"dimension\": content_details.get(\"dimension\", \"\"),\n",
    "            \"definition\": content_details.get(\"definition\", \"\")\n",
    "        }\n",
    "        return data\n",
    "    return None\n",
    "\n",
    "for video_id in video_ids:\n",
    "    video_data = get_video_data(video_id)\n",
    "    if video_data:\n",
    "        results.append(video_data)\n",
    "\n",
    "# 输出到 CSV 文件\n",
    "output_path = \"../data/youtube_video_data.csv\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for row in results:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"数据已保存到 {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data: 400\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# 设置API密钥\n",
    "api_key = 'uGXLKQRdFV2dgKISJUa7YMSt2ex2UDxcdIAezx6I'\n",
    "\n",
    "# 构建请求URL\n",
    "base_url = 'https://api.usa.gov/crime/fbi/sapi/'\n",
    "endpoint = 'api/nibrs/violent-crime/offense/national/count'\n",
    "# 指定需要的年份\n",
    "year = '2019'\n",
    "\n",
    "# 完整URL\n",
    "url = f'https://api.usa.gov/crime/fbi/cde/hate-crime/state/VA?from=2020&type=race&to=2021&API_KEY=iiHnOKfno2Mgkt5AynpvPpUQTEyxE77jo1RU8PIv'\n",
    "\n",
    "# 发送GET请求\n",
    "response = requests.get(url)\n",
    "\n",
    "# 检查响应状态码\n",
    "if response.status_code == 200:\n",
    "    data = response.json()  # 解析返回的JSON数据\n",
    "    print(data)\n",
    "else:\n",
    "    print(\"Failed to retrieve data:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ENHYPEN (엔하이픈) 'No Doubt' Official MV\n",
      "Description: ENHYPEN (엔하이픈) 'No Doubt' Official MV\n",
      "\n",
      "Credits:\n",
      "Directed by Yunah Sheep\n",
      "\n",
      "ⓒ BELIFT LAB Inc. All Rights Reserved\n",
      "\n",
      "Connect with ENHYPEN\n",
      "OFFICIAL WEBSITE https://ENHYPEN.com\n",
      "ENHYPEN Weverse https://www.weverse.io/enhypen\n",
      "OFFICIAL YOUTUBE https://www.youtube.com/ENHYPENOFFICIAL\n",
      "OFFICIAL X (TWITTER) https://twitter.com/ENHYPEN\n",
      "ENHYPEN X (TWITTER) https://twitter.com/ENHYPEN_members\n",
      "OFFICIAL FACEBOOK https://www.facebook.com/officialENHYPEN\n",
      "OFFICIAL INSTAGRAM https://www.instagram.com/enhypen\n",
      "OFFICIAL TIKTOK  https://www.tiktok.com/@enhypen\n",
      "OFFICIAL WEIBO https://weibo.com/ENHYPEN\n",
      "OFFICIAL BILIBILI https://space.bilibili.com/3493119035181246\n",
      "OFFICIAL JAPAN X (TWITTER) https://twitter.com/ENHYPEN_JP\n",
      "\n",
      "#ENHYPEN #엔하이픈 #ROMANCE_UNTOLD_daydream #NoDoubt\n",
      "Duration: PT3M5S\n",
      "View count: 28195106\n",
      "Like count: 819592\n",
      "Comment count: 64086\n",
      "\n",
      "Top Comments:\n",
      "@attaetude: I LOVE THE CHOREOGRAPHY THE SHOULDER DANCE AND THE WHISTLE THING THE SONG THE OUTFITS THE CHORUS EVERITHJNG!\n",
      "@filzxwonie: can we talk about how enhypen still manages to include their lore into every single song , while having drastically different concepts ?? im soso proud this song is absolutely AMAZING, the visuals, the vocals, the outfits, every single thing has me jawdropped.\n",
      "@carachann5416: Jay&#39;s visual, Niki&#39;s visual, Sunghoon&#39;s visual, Jungwon&#39;s visual, Jake&#39;s visual, Heeseung&#39;s visual, and Sunoo&#39;s visual are drive me crazy<a href=\"UCkszU2WH9gy1mb0dV-11UJg/YvgfY-LIBpjChgHKyYCQBg\"></a>\n",
      "@mieoyuiki: This music video is a masterpiece! The visuals, the choreography, and their expressions are on point. Enhypen keeps raising the bar every time!\n",
      "@Tiramisu_Cake07: The song, the vocals, the choreography, the visuals, the beat, Jungwon, Heeseung, Jay, Jake, Sunghoon, Sunoo, NI-KI, everything is PERFECT I’m impressed\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "# 你的API密钥\n",
    "api_key = 'AIzaSyC5VGKOdaG9IW3lauaZ03yk0nkP3oS4cTc'\n",
    "\n",
    "# 创建 YouTube client 对象\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# 视频ID\n",
    "video_id = 'rDolt3jJRsM'\n",
    "\n",
    "# 调用API获取视频详情\n",
    "video_response = youtube.videos().list(\n",
    "    part='snippet,contentDetails,statistics',\n",
    "    id=video_id\n",
    ").execute()\n",
    "\n",
    "# 输出视频详细信息\n",
    "for video in video_response.get('items', []):\n",
    "    title = video['snippet']['title']\n",
    "    description = video['snippet']['description']\n",
    "    duration = video['contentDetails']['duration']\n",
    "    view_count = video['statistics']['viewCount']\n",
    "    like_count = video['statistics'].get('likeCount', 'Unavailable')\n",
    "    comment_count = video['statistics'].get('commentCount', 'Unavailable')\n",
    "\n",
    "    print(f'Title: {title}')\n",
    "    print(f'Description: {description}')\n",
    "    print(f'Duration: {duration}')\n",
    "    print(f'View count: {view_count}')\n",
    "    print(f'Like count: {like_count}')\n",
    "    print(f'Comment count: {comment_count}')\n",
    "\n",
    "# 获取热门评论\n",
    "comments_response = youtube.commentThreads().list(\n",
    "    part='snippet',\n",
    "    videoId=video_id,\n",
    "    order='relevance',  # 按相关性排序\n",
    "    maxResults=5  # 获取前5个热门评论\n",
    ").execute()\n",
    "\n",
    "print(\"\\nTop Comments:\")\n",
    "for comment in comments_response.get('items', []):\n",
    "    author = comment['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
    "    text = comment['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "    print(f'{author}: {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ENHYPEN 엔하이픈 No Doubt Official MV.csv\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import dateutil.parser\n",
    "\n",
    "# YouTube API Key\n",
    "api_key = 'AIzaSyC5VGKOdaG9IW3lauaZ03yk0nkP3oS4cTc'\n",
    "\n",
    "# Video ID\n",
    "video_id = 'rDolt3jJRsM'\n",
    "\n",
    "# Create a YouTube object\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Fetch video details\n",
    "video_response = youtube.videos().list(\n",
    "    part='snippet,contentDetails,statistics',\n",
    "    id=video_id\n",
    ").execute()\n",
    "\n",
    "# Extract video and channel details\n",
    "video = video_response['items'][0]\n",
    "snippet = video['snippet']\n",
    "statistics = video['statistics']\n",
    "content_details = video['contentDetails']\n",
    "\n",
    "# Calculate days since published\n",
    "published_at = dateutil.parser.parse(snippet['publishedAt'])\n",
    "days_since_published = (datetime.now(published_at.tzinfo) - published_at).days\n",
    "\n",
    "# Get channel details for subscriber count\n",
    "channel_id = snippet['channelId']\n",
    "channel_response = youtube.channels().list(\n",
    "    part='statistics',\n",
    "    id=channel_id\n",
    ").execute()\n",
    "subscriber_count = channel_response['items'][0]['statistics']['subscriberCount']\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    'Title': snippet['title'],\n",
    "    'Description': snippet['description'],\n",
    "    'Published At': snippet['publishedAt'],\n",
    "    'Days Since Published': days_since_published,\n",
    "    'View Count': statistics['viewCount'],\n",
    "    'Like Count': statistics.get('likeCount', 'Unavailable'),\n",
    "    'Comment Count': statistics.get('commentCount', 'Unavailable'),\n",
    "    'Subscriber Count': subscriber_count,\n",
    "    'Category ID': snippet['categoryId'],\n",
    "    'Definition': content_details['definition']\n",
    "}\n",
    "df = pd.DataFrame([data])\n",
    "\n",
    "# Get top 10 comments\n",
    "comments_response = youtube.commentThreads().list(\n",
    "    part='snippet',\n",
    "    videoId=video_id,\n",
    "    order='relevance',\n",
    "    maxResults=10\n",
    ").execute()\n",
    "\n",
    "top_comments = [comment['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                for comment in comments_response.get('items', [])]\n",
    "df['Top Comments'] = pd.Series([top_comments])\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "safe_title = \"\".join(x for x in snippet['title'] if x.isalnum() or x in \" _-\").rstrip()\n",
    "filename = f\"{safe_title}.csv\"\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f'Data saved to {filename}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
