{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Collection\"\n",
    "format:\n",
    "    html: \n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include instructions.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "{{< include overview.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include methods.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code \n",
    "\n",
    "Provide the source code used for this section of the project here.\n",
    "\n",
    "If you're using a package for code organization, you can import it at this point. However, make sure that the **actual workflow steps**â€”including data processing, analysis, and other key tasksâ€”are conducted and clearly demonstrated on this page. The goal is to show the technical flow of your project, highlighting how the code is executed to achieve your results.\n",
    "\n",
    "Ensure that the code is well-commented to enhance readability and understanding for others who may review or use it. If relevant, link to additional documentation or external references that explain any complex components. This section should give readers a clear view of how the project is implemented from a technical perspective.\n",
    "\n",
    "This page is a technical narrative, NOT just a notebook with a collection of code cells, include in-line Prose, to describe what is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "In the following code, we first utilized the requests library to retrieve the HTML content from the Wikipedia page. Afterward, we employed BeautifulSoup to parse the HTML and locate the specific table of interest by using the find function. Once the table was identified, we extracted the relevant data by iterating through its rows, gathering country names and their respective populations. Finally, we used Pandas to store the collected data in a DataFrame, allowing for easy analysis and visualization. The data could also be optionally saved as a CSV file for further use. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Country     Population\n",
      "0                                  World  8,119,000,000\n",
      "1                                  China  1,409,670,000\n",
      "2                          1,404,910,000          17.3%\n",
      "3                          United States    335,893,238\n",
      "4                              Indonesia    281,603,800\n",
      "..                                   ...            ...\n",
      "235                   Niue (New Zealand)          1,681\n",
      "236                Tokelau (New Zealand)          1,647\n",
      "237                         Vatican City            764\n",
      "238  Cocos (Keeling) Islands (Australia)            593\n",
      "239                Pitcairn Islands (UK)             35\n",
      "\n",
      "[240 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Send a request to Wikipedia page\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Parse the page content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Step 3: Find the table containing the data (usually the first table for such lists)\n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "# Step 4: Extract data from the table rows\n",
    "countries = []\n",
    "populations = []\n",
    "\n",
    "# Iterate over the table rows\n",
    "for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) > 1:\n",
    "        country = cells[1].text.strip()  # The country name is in the second column\n",
    "        population = cells[2].text.strip()  # The population is in the third column\n",
    "        countries.append(country)\n",
    "        populations.append(population)\n",
    "\n",
    "# Step 5: Create a DataFrame to store the results\n",
    "data = pd.DataFrame({\n",
    "    'Country': countries,\n",
    "    'Population': populations\n",
    "})\n",
    "\n",
    "# Display the scraped data\n",
    "print(data)\n",
    "\n",
    "# Optionally save to CSV\n",
    "data.to_csv('../../data/raw-data/countries_population.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>view_counts</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Taylor Swift - Anti-Hero (Official Music Video)</td>\n",
       "      <td>212893513</td>\n",
       "      <td>Anti-Hero by Taylor Swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Taylor Swift - Anti-Hero (Official Lyric Video)</td>\n",
       "      <td>34875876</td>\n",
       "      <td>Anti-Hero by Taylor Swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Taylor Swift - Anti-Hero (Lyrics)</td>\n",
       "      <td>14266885</td>\n",
       "      <td>Anti-Hero by Taylor Swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Taylor Swift - Anti Hero (Lyrics) &amp;quot;It&amp;#39...</td>\n",
       "      <td>5444496</td>\n",
       "      <td>Anti-Hero by Taylor Swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Taylor Swift - Anti-Hero</td>\n",
       "      <td>1332074</td>\n",
       "      <td>Anti-Hero by Taylor Swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Lorde - Tennis Court</td>\n",
       "      <td>131679562</td>\n",
       "      <td>Tennis Court by Lorde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Lorde - Tennis Court (Flume Remix)</td>\n",
       "      <td>115590517</td>\n",
       "      <td>Tennis Court by Lorde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Lorde - Tennis Court (Audio)</td>\n",
       "      <td>2182597</td>\n",
       "      <td>Tennis Court by Lorde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Lorde - Tennis Court (Glastonbury 2017)</td>\n",
       "      <td>365081</td>\n",
       "      <td>Tennis Court by Lorde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Tennis Court - Lorde (Lyrics) ğŸµ</td>\n",
       "      <td>179298</td>\n",
       "      <td>Tennis Court by Lorde</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                titles  view_counts  \\\n",
       "0      Taylor Swift - Anti-Hero (Official Music Video)    212893513   \n",
       "1      Taylor Swift - Anti-Hero (Official Lyric Video)     34875876   \n",
       "2                    Taylor Swift - Anti-Hero (Lyrics)     14266885   \n",
       "3    Taylor Swift - Anti Hero (Lyrics) &quot;It&#39...      5444496   \n",
       "4                             Taylor Swift - Anti-Hero      1332074   \n",
       "..                                                 ...          ...   \n",
       "110                               Lorde - Tennis Court    131679562   \n",
       "111                 Lorde - Tennis Court (Flume Remix)    115590517   \n",
       "112                       Lorde - Tennis Court (Audio)      2182597   \n",
       "113            Lorde - Tennis Court (Glastonbury 2017)       365081   \n",
       "114                    Tennis Court - Lorde (Lyrics) ğŸµ       179298   \n",
       "\n",
       "                         query  \n",
       "0    Anti-Hero by Taylor Swift  \n",
       "1    Anti-Hero by Taylor Swift  \n",
       "2    Anti-Hero by Taylor Swift  \n",
       "3    Anti-Hero by Taylor Swift  \n",
       "4    Anti-Hero by Taylor Swift  \n",
       "..                         ...  \n",
       "110      Tennis Court by Lorde  \n",
       "111      Tennis Court by Lorde  \n",
       "112      Tennis Court by Lorde  \n",
       "113      Tennis Court by Lorde  \n",
       "114      Tennis Court by Lorde  \n",
       "\n",
       "[115 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "\n",
    "# API Key\n",
    "api_key = \"AIzaSyDtKE-4QZj6EA-rwG7cj5gMJxdt4Fe14Nw\"\n",
    "\n",
    "# Initialize YouTube API client\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# List to store data\n",
    "all_data = []\n",
    "\n",
    "# Read song data and fetch YouTube statistics\n",
    "with open('song_data.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Strip newline characters and spaces\n",
    "        query = line.strip()\n",
    "\n",
    "        # Search request for the query\n",
    "        search_request = youtube.search().list(\n",
    "            part=\"snippet\",\n",
    "            q=query,  # Use the query from the file\n",
    "            maxResults=5,\n",
    "            type=\"video\",\n",
    "            order='relevance'\n",
    "        )\n",
    "        search_response = search_request.execute()\n",
    "\n",
    "        # Get video IDs\n",
    "        video_ids = [item['id']['videoId'] for item in search_response['items']]\n",
    "        if not video_ids:\n",
    "            continue  # Skip if no results\n",
    "\n",
    "        # Fetch video details (statistics)\n",
    "        video_request = youtube.videos().list(\n",
    "            part=\"statistics\",\n",
    "            id=\",\".join(video_ids)\n",
    "        )\n",
    "        video_response = video_request.execute()\n",
    "\n",
    "        # Collect results for the current query\n",
    "        query_data = []\n",
    "        for item, stats in zip(search_response['items'], video_response['items']):\n",
    "            query_data.append({\n",
    "                \"titles\": item['snippet']['title'],\n",
    "                \"view_counts\": int(stats['statistics']['viewCount']),\n",
    "                \"query\": query\n",
    "            })\n",
    "\n",
    "        # Convert query-specific data to a DataFrame and sort by view_counts\n",
    "        query_df = pd.DataFrame(query_data)\n",
    "        query_df = query_df.sort_values(by=\"view_counts\", ascending=False)\n",
    "\n",
    "        # Append the sorted data to the final list\n",
    "        all_data.append(query_df)\n",
    "\n",
    "# Concatenate all sorted query-specific DataFrames into one\n",
    "final_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('view_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# è®¾ç½® YouTube Data API å¯†é’¥å’ŒæœåŠ¡\n",
    "API_KEY = \"AIzaSyC5VGKOdaG9IW3lauaZ03yk0nkP3oS4cTc\"\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# è§†é¢‘IDåˆ—è¡¨\n",
    "video_ids = [\n",
    "    \"EqDlrimnMCE\", \"m6N6jOt7heY\"\n",
    "]\n",
    "\n",
    "# è¦æå–çš„ä¿¡æ¯\n",
    "fields = [\n",
    "    \"videoId\", \"title\", \"description\", \"publishedAt\", \"tags\",\n",
    "    \"viewCount\", \"likeCount\", \"commentCount\", \"categoryId\", \"duration\",\n",
    "    \"dimension\", \"definition\"\n",
    "]\n",
    "\n",
    "# å­˜å‚¨ç»“æœçš„åˆ—è¡¨\n",
    "results = []\n",
    "\n",
    "# éå†è§†é¢‘IDï¼Œè·å–ç›¸å…³æ•°æ®\n",
    "def get_video_data(video_id):\n",
    "    request = youtube.videos().list(\n",
    "        part=\"snippet,statistics,contentDetails\",\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    if \"items\" in response and response[\"items\"]:\n",
    "        item = response[\"items\"][0]\n",
    "        snippet = item.get(\"snippet\", {})\n",
    "        statistics = item.get(\"statistics\", {})\n",
    "        content_details = item.get(\"contentDetails\", {})\n",
    "        \n",
    "        data = {\n",
    "            \"videoId\": video_id,\n",
    "            \"title\": snippet.get(\"title\", \"\"),\n",
    "            \"description\": snippet.get(\"description\", \"\"),\n",
    "            \"publishedAt\": snippet.get(\"publishedAt\", \"\"),\n",
    "            \"tags\": snippet.get(\"tags\", []),\n",
    "            \"viewCount\": statistics.get(\"viewCount\", \"0\"),\n",
    "            \"likeCount\": statistics.get(\"likeCount\", \"0\"),\n",
    "            \"commentCount\": statistics.get(\"commentCount\", \"0\"),\n",
    "            \"categoryId\": snippet.get(\"categoryId\", \"\"),\n",
    "            \"duration\": content_details.get(\"duration\", \"\"),\n",
    "            \"dimension\": content_details.get(\"dimension\", \"\"),\n",
    "            \"definition\": content_details.get(\"definition\", \"\")\n",
    "        }\n",
    "        return data\n",
    "    return None\n",
    "\n",
    "for video_id in video_ids:\n",
    "    video_data = get_video_data(video_id)\n",
    "    if video_data:\n",
    "        results.append(video_data)\n",
    "\n",
    "# è¾“å‡ºåˆ° CSV æ–‡ä»¶\n",
    "output_path = \"../data/youtube_video_data.csv\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for row in results:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"æ•°æ®å·²ä¿å­˜åˆ° {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data: 400\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# è®¾ç½®APIå¯†é’¥\n",
    "api_key = 'uGXLKQRdFV2dgKISJUa7YMSt2ex2UDxcdIAezx6I'\n",
    "\n",
    "# æ„å»ºè¯·æ±‚URL\n",
    "base_url = 'https://api.usa.gov/crime/fbi/sapi/'\n",
    "endpoint = 'api/nibrs/violent-crime/offense/national/count'\n",
    "# æŒ‡å®šéœ€è¦çš„å¹´ä»½\n",
    "year = '2019'\n",
    "\n",
    "# å®Œæ•´URL\n",
    "url = f'https://api.usa.gov/crime/fbi/cde/hate-crime/state/VA?from=2020&type=race&to=2021&API_KEY=iiHnOKfno2Mgkt5AynpvPpUQTEyxE77jo1RU8PIv'\n",
    "\n",
    "# å‘é€GETè¯·æ±‚\n",
    "response = requests.get(url)\n",
    "\n",
    "# æ£€æŸ¥å“åº”çŠ¶æ€ç \n",
    "if response.status_code == 200:\n",
    "    data = response.json()  # è§£æè¿”å›çš„JSONæ•°æ®\n",
    "    print(data)\n",
    "else:\n",
    "    print(\"Failed to retrieve data:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ENHYPEN (ì—”í•˜ì´í”ˆ) 'No Doubt' Official MV\n",
      "Description: ENHYPEN (ì—”í•˜ì´í”ˆ) 'No Doubt' Official MV\n",
      "\n",
      "Credits:\n",
      "Directed by Yunah Sheep\n",
      "\n",
      "â“’ BELIFT LAB Inc. All Rights Reserved\n",
      "\n",
      "Connect with ENHYPEN\n",
      "OFFICIAL WEBSITE https://ENHYPEN.com\n",
      "ENHYPEN Weverse https://www.weverse.io/enhypen\n",
      "OFFICIAL YOUTUBE https://www.youtube.com/ENHYPENOFFICIAL\n",
      "OFFICIAL X (TWITTER) https://twitter.com/ENHYPEN\n",
      "ENHYPEN X (TWITTER) https://twitter.com/ENHYPEN_members\n",
      "OFFICIAL FACEBOOK https://www.facebook.com/officialENHYPEN\n",
      "OFFICIAL INSTAGRAM https://www.instagram.com/enhypen\n",
      "OFFICIAL TIKTOK  https://www.tiktok.com/@enhypen\n",
      "OFFICIAL WEIBO https://weibo.com/ENHYPEN\n",
      "OFFICIAL BILIBILI https://space.bilibili.com/3493119035181246\n",
      "OFFICIAL JAPAN X (TWITTER) https://twitter.com/ENHYPEN_JP\n",
      "\n",
      "#ENHYPEN #ì—”í•˜ì´í”ˆ #ROMANCE_UNTOLD_daydream #NoDoubt\n",
      "Duration: PT3M5S\n",
      "View count: 28195106\n",
      "Like count: 819592\n",
      "Comment count: 64086\n",
      "\n",
      "Top Comments:\n",
      "@attaetude: I LOVE THE CHOREOGRAPHY THE SHOULDER DANCE AND THE WHISTLE THING THE SONG THE OUTFITS THE CHORUS EVERITHJNG!\n",
      "@filzxwonie: can we talk about how enhypen still manages to include their lore into every single song , while having drastically different concepts ?? im soso proud this song is absolutely AMAZING, the visuals, the vocals, the outfits, every single thing has me jawdropped.\n",
      "@carachann5416: Jay&#39;s visual, Niki&#39;s visual, Sunghoon&#39;s visual, Jungwon&#39;s visual, Jake&#39;s visual, Heeseung&#39;s visual, and Sunoo&#39;s visual are drive me crazy<a href=\"UCkszU2WH9gy1mb0dV-11UJg/YvgfY-LIBpjChgHKyYCQBg\"></a>\n",
      "@mieoyuiki: This music video is a masterpiece! The visuals, the choreography, and their expressions are on point. Enhypen keeps raising the bar every time!\n",
      "@Tiramisu_Cake07: The song, the vocals, the choreography, the visuals, the beat, Jungwon, Heeseung, Jay, Jake, Sunghoon, Sunoo, NI-KI, everything is PERFECT Iâ€™m impressed\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "# ä½ çš„APIå¯†é’¥\n",
    "api_key = 'AIzaSyC5VGKOdaG9IW3lauaZ03yk0nkP3oS4cTc'\n",
    "\n",
    "# åˆ›å»º YouTube client å¯¹è±¡\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# è§†é¢‘ID\n",
    "video_id = 'rDolt3jJRsM'\n",
    "\n",
    "# è°ƒç”¨APIè·å–è§†é¢‘è¯¦æƒ…\n",
    "video_response = youtube.videos().list(\n",
    "    part='snippet,contentDetails,statistics',\n",
    "    id=video_id\n",
    ").execute()\n",
    "\n",
    "# è¾“å‡ºè§†é¢‘è¯¦ç»†ä¿¡æ¯\n",
    "for video in video_response.get('items', []):\n",
    "    title = video['snippet']['title']\n",
    "    description = video['snippet']['description']\n",
    "    duration = video['contentDetails']['duration']\n",
    "    view_count = video['statistics']['viewCount']\n",
    "    like_count = video['statistics'].get('likeCount', 'Unavailable')\n",
    "    comment_count = video['statistics'].get('commentCount', 'Unavailable')\n",
    "\n",
    "    print(f'Title: {title}')\n",
    "    print(f'Description: {description}')\n",
    "    print(f'Duration: {duration}')\n",
    "    print(f'View count: {view_count}')\n",
    "    print(f'Like count: {like_count}')\n",
    "    print(f'Comment count: {comment_count}')\n",
    "\n",
    "# è·å–çƒ­é—¨è¯„è®º\n",
    "comments_response = youtube.commentThreads().list(\n",
    "    part='snippet',\n",
    "    videoId=video_id,\n",
    "    order='relevance',  # æŒ‰ç›¸å…³æ€§æ’åº\n",
    "    maxResults=5  # è·å–å‰5ä¸ªçƒ­é—¨è¯„è®º\n",
    ").execute()\n",
    "\n",
    "print(\"\\nTop Comments:\")\n",
    "for comment in comments_response.get('items', []):\n",
    "    author = comment['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
    "    text = comment['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "    print(f'{author}: {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ENHYPEN ì—”í•˜ì´í”ˆ No Doubt Official MV.csv\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import dateutil.parser\n",
    "\n",
    "# YouTube API Key\n",
    "api_key = 'AIzaSyC5VGKOdaG9IW3lauaZ03yk0nkP3oS4cTc'\n",
    "\n",
    "# Video ID\n",
    "video_id = 'rDolt3jJRsM'\n",
    "\n",
    "# Create a YouTube object\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Fetch video details\n",
    "video_response = youtube.videos().list(\n",
    "    part='snippet,contentDetails,statistics',\n",
    "    id=video_id\n",
    ").execute()\n",
    "\n",
    "# Extract video and channel details\n",
    "video = video_response['items'][0]\n",
    "snippet = video['snippet']\n",
    "statistics = video['statistics']\n",
    "content_details = video['contentDetails']\n",
    "\n",
    "# Calculate days since published\n",
    "published_at = dateutil.parser.parse(snippet['publishedAt'])\n",
    "days_since_published = (datetime.now(published_at.tzinfo) - published_at).days\n",
    "\n",
    "# Get channel details for subscriber count\n",
    "channel_id = snippet['channelId']\n",
    "channel_response = youtube.channels().list(\n",
    "    part='statistics',\n",
    "    id=channel_id\n",
    ").execute()\n",
    "subscriber_count = channel_response['items'][0]['statistics']['subscriberCount']\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    'Title': snippet['title'],\n",
    "    'Description': snippet['description'],\n",
    "    'Published At': snippet['publishedAt'],\n",
    "    'Days Since Published': days_since_published,\n",
    "    'View Count': statistics['viewCount'],\n",
    "    'Like Count': statistics.get('likeCount', 'Unavailable'),\n",
    "    'Comment Count': statistics.get('commentCount', 'Unavailable'),\n",
    "    'Subscriber Count': subscriber_count,\n",
    "    'Category ID': snippet['categoryId'],\n",
    "    'Definition': content_details['definition']\n",
    "}\n",
    "df = pd.DataFrame([data])\n",
    "\n",
    "# Get top 10 comments\n",
    "comments_response = youtube.commentThreads().list(\n",
    "    part='snippet',\n",
    "    videoId=video_id,\n",
    "    order='relevance',\n",
    "    maxResults=10\n",
    ").execute()\n",
    "\n",
    "top_comments = [comment['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                for comment in comments_response.get('items', [])]\n",
    "df['Top Comments'] = pd.Series([top_comments])\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "safe_title = \"\".join(x for x in snippet['title'] if x.isalnum() or x in \" _-\").rstrip()\n",
    "filename = f\"{safe_title}.csv\"\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f'Data saved to {filename}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
