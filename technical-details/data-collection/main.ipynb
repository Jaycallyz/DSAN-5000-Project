{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Collection\"\n",
    "format:\n",
    "    html: \n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include instructions.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "{{< include overview.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include methods.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code \n",
    "\n",
    "Provide the source code used for this section of the project here.\n",
    "\n",
    "If you're using a package for code organization, you can import it at this point. However, make sure that the **actual workflow steps**‚Äîincluding data processing, analysis, and other key tasks‚Äîare conducted and clearly demonstrated on this page. The goal is to show the technical flow of your project, highlighting how the code is executed to achieve your results.\n",
    "\n",
    "Ensure that the code is well-commented to enhance readability and understanding for others who may review or use it. If relevant, link to additional documentation or external references that explain any complex components. This section should give readers a clear view of how the project is implemented from a technical perspective.\n",
    "\n",
    "This page is a technical narrative, NOT just a notebook with a collection of code cells, include in-line Prose, to describe what is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "In the following code, we first utilized the requests library to retrieve the HTML content from the Wikipedia page. Afterward, we employed BeautifulSoup to parse the HTML and locate the specific table of interest by using the find function. Once the table was identified, we extracted the relevant data by iterating through its rows, gathering country names and their respective populations. Finally, we used Pandas to store the collected data in a DataFrame, allowing for easy analysis and visualization. The data could also be optionally saved as a CSV file for further use. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Country     Population\n",
      "0                                  World  8,119,000,000\n",
      "1                                  China  1,409,670,000\n",
      "2                          1,404,910,000          17.3%\n",
      "3                          United States    335,893,238\n",
      "4                              Indonesia    281,603,800\n",
      "..                                   ...            ...\n",
      "235                   Niue (New Zealand)          1,681\n",
      "236                Tokelau (New Zealand)          1,647\n",
      "237                         Vatican City            764\n",
      "238  Cocos (Keeling) Islands (Australia)            593\n",
      "239                Pitcairn Islands (UK)             35\n",
      "\n",
      "[240 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Send a request to Wikipedia page\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Parse the page content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Step 3: Find the table containing the data (usually the first table for such lists)\n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "# Step 4: Extract data from the table rows\n",
    "countries = []\n",
    "populations = []\n",
    "\n",
    "# Iterate over the table rows\n",
    "for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) > 1:\n",
    "        country = cells[1].text.strip()  # The country name is in the second column\n",
    "        population = cells[2].text.strip()  # The population is in the third column\n",
    "        countries.append(country)\n",
    "        populations.append(population)\n",
    "\n",
    "# Step 5: Create a DataFrame to store the results\n",
    "data = pd.DataFrame({\n",
    "    'Country': countries,\n",
    "    'Population': populations\n",
    "})\n",
    "\n",
    "# Display the scraped data\n",
    "print(data)\n",
    "\n",
    "# Optionally save to CSV\n",
    "data.to_csv('../../data/raw-data/countries_population.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>view_counts</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Taylor Swift - Anti-Hero (Official Music Video)</td>\n",
       "      <td>212893513</td>\n",
       "      <td>Anti-Hero by Taylor Swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Taylor Swift - Anti-Hero (Official Lyric Video)</td>\n",
       "      <td>34875876</td>\n",
       "      <td>Anti-Hero by Taylor Swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Taylor Swift - Anti-Hero (Lyrics)</td>\n",
       "      <td>14266885</td>\n",
       "      <td>Anti-Hero by Taylor Swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Taylor Swift - Anti Hero (Lyrics) &amp;quot;It&amp;#39...</td>\n",
       "      <td>5444496</td>\n",
       "      <td>Anti-Hero by Taylor Swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Taylor Swift - Anti-Hero</td>\n",
       "      <td>1332074</td>\n",
       "      <td>Anti-Hero by Taylor Swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Lorde - Tennis Court</td>\n",
       "      <td>131679562</td>\n",
       "      <td>Tennis Court by Lorde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Lorde - Tennis Court (Flume Remix)</td>\n",
       "      <td>115590517</td>\n",
       "      <td>Tennis Court by Lorde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Lorde - Tennis Court (Audio)</td>\n",
       "      <td>2182597</td>\n",
       "      <td>Tennis Court by Lorde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Lorde - Tennis Court (Glastonbury 2017)</td>\n",
       "      <td>365081</td>\n",
       "      <td>Tennis Court by Lorde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Tennis Court - Lorde (Lyrics) üéµ</td>\n",
       "      <td>179298</td>\n",
       "      <td>Tennis Court by Lorde</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                titles  view_counts  \\\n",
       "0      Taylor Swift - Anti-Hero (Official Music Video)    212893513   \n",
       "1      Taylor Swift - Anti-Hero (Official Lyric Video)     34875876   \n",
       "2                    Taylor Swift - Anti-Hero (Lyrics)     14266885   \n",
       "3    Taylor Swift - Anti Hero (Lyrics) &quot;It&#39...      5444496   \n",
       "4                             Taylor Swift - Anti-Hero      1332074   \n",
       "..                                                 ...          ...   \n",
       "110                               Lorde - Tennis Court    131679562   \n",
       "111                 Lorde - Tennis Court (Flume Remix)    115590517   \n",
       "112                       Lorde - Tennis Court (Audio)      2182597   \n",
       "113            Lorde - Tennis Court (Glastonbury 2017)       365081   \n",
       "114                    Tennis Court - Lorde (Lyrics) üéµ       179298   \n",
       "\n",
       "                         query  \n",
       "0    Anti-Hero by Taylor Swift  \n",
       "1    Anti-Hero by Taylor Swift  \n",
       "2    Anti-Hero by Taylor Swift  \n",
       "3    Anti-Hero by Taylor Swift  \n",
       "4    Anti-Hero by Taylor Swift  \n",
       "..                         ...  \n",
       "110      Tennis Court by Lorde  \n",
       "111      Tennis Court by Lorde  \n",
       "112      Tennis Court by Lorde  \n",
       "113      Tennis Court by Lorde  \n",
       "114      Tennis Court by Lorde  \n",
       "\n",
       "[115 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "\n",
    "# API Key\n",
    "api_key = \"AIzaSyDtKE-4QZj6EA-rwG7cj5gMJxdt4Fe14Nw\"\n",
    "\n",
    "# Initialize YouTube API client\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# List to store data\n",
    "all_data = []\n",
    "\n",
    "# Read song data and fetch YouTube statistics\n",
    "with open('song_data.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Strip newline characters and spaces\n",
    "        query = line.strip()\n",
    "\n",
    "        # Search request for the query\n",
    "        search_request = youtube.search().list(\n",
    "            part=\"snippet\",\n",
    "            q=query,  # Use the query from the file\n",
    "            maxResults=5,\n",
    "            type=\"video\",\n",
    "            order='relevance'\n",
    "        )\n",
    "        search_response = search_request.execute()\n",
    "\n",
    "        # Get video IDs\n",
    "        video_ids = [item['id']['videoId'] for item in search_response['items']]\n",
    "        if not video_ids:\n",
    "            continue  # Skip if no results\n",
    "\n",
    "        # Fetch video details (statistics)\n",
    "        video_request = youtube.videos().list(\n",
    "            part=\"statistics\",\n",
    "            id=\",\".join(video_ids)\n",
    "        )\n",
    "        video_response = video_request.execute()\n",
    "\n",
    "        # Collect results for the current query\n",
    "        query_data = []\n",
    "        for item, stats in zip(search_response['items'], video_response['items']):\n",
    "            query_data.append({\n",
    "                \"titles\": item['snippet']['title'],\n",
    "                \"view_counts\": int(stats['statistics']['viewCount']),\n",
    "                \"query\": query\n",
    "            })\n",
    "\n",
    "        # Convert query-specific data to a DataFrame and sort by view_counts\n",
    "        query_df = pd.DataFrame(query_data)\n",
    "        query_df = query_df.sort_values(by=\"view_counts\", ascending=False)\n",
    "\n",
    "        # Append the sorted data to the final list\n",
    "        all_data.append(query_df)\n",
    "\n",
    "# Concatenate all sorted query-specific DataFrames into one\n",
    "final_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('view_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# ËÆæÁΩÆ YouTube Data API ÂØÜÈí•ÂíåÊúçÂä°\n",
    "API_KEY = \"AIzaSyC5VGKOdaG9IW3lauaZ03yk0nkP3oS4cTc\"\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# ËßÜÈ¢ëIDÂàóË°®\n",
    "video_ids = [\n",
    "    \"EqDlrimnMCE\", \"m6N6jOt7heY\"\n",
    "]\n",
    "\n",
    "# Ë¶ÅÊèêÂèñÁöÑ‰ø°ÊÅØ\n",
    "fields = [\n",
    "    \"videoId\", \"title\", \"description\", \"publishedAt\", \"tags\",\n",
    "    \"viewCount\", \"likeCount\", \"commentCount\", \"categoryId\", \"duration\",\n",
    "    \"dimension\", \"definition\"\n",
    "]\n",
    "\n",
    "# Â≠òÂÇ®ÁªìÊûúÁöÑÂàóË°®\n",
    "results = []\n",
    "\n",
    "# ÈÅçÂéÜËßÜÈ¢ëIDÔºåËé∑ÂèñÁõ∏ÂÖ≥Êï∞ÊçÆ\n",
    "def get_video_data(video_id):\n",
    "    request = youtube.videos().list(\n",
    "        part=\"snippet,statistics,contentDetails\",\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    if \"items\" in response and response[\"items\"]:\n",
    "        item = response[\"items\"][0]\n",
    "        snippet = item.get(\"snippet\", {})\n",
    "        statistics = item.get(\"statistics\", {})\n",
    "        content_details = item.get(\"contentDetails\", {})\n",
    "        \n",
    "        data = {\n",
    "            \"videoId\": video_id,\n",
    "            \"title\": snippet.get(\"title\", \"\"),\n",
    "            \"description\": snippet.get(\"description\", \"\"),\n",
    "            \"publishedAt\": snippet.get(\"publishedAt\", \"\"),\n",
    "            \"tags\": snippet.get(\"tags\", []),\n",
    "            \"viewCount\": statistics.get(\"viewCount\", \"0\"),\n",
    "            \"likeCount\": statistics.get(\"likeCount\", \"0\"),\n",
    "            \"commentCount\": statistics.get(\"commentCount\", \"0\"),\n",
    "            \"categoryId\": snippet.get(\"categoryId\", \"\"),\n",
    "            \"duration\": content_details.get(\"duration\", \"\"),\n",
    "            \"dimension\": content_details.get(\"dimension\", \"\"),\n",
    "            \"definition\": content_details.get(\"definition\", \"\")\n",
    "        }\n",
    "        return data\n",
    "    return None\n",
    "\n",
    "for video_id in video_ids:\n",
    "    video_data = get_video_data(video_id)\n",
    "    if video_data:\n",
    "        results.append(video_data)\n",
    "\n",
    "# ËæìÂá∫Âà∞ CSV Êñá‰ª∂\n",
    "output_path = \"../data/youtube_video_data.csv\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for row in results:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Êï∞ÊçÆÂ∑≤‰øùÂ≠òÂà∞ {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data: 400\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# ËÆæÁΩÆAPIÂØÜÈí•\n",
    "api_key = 'uGXLKQRdFV2dgKISJUa7YMSt2ex2UDxcdIAezx6I'\n",
    "\n",
    "# ÊûÑÂª∫ËØ∑Ê±ÇURL\n",
    "base_url = 'https://api.usa.gov/crime/fbi/sapi/'\n",
    "endpoint = 'api/nibrs/violent-crime/offense/national/count'\n",
    "# ÊåáÂÆöÈúÄË¶ÅÁöÑÂπ¥‰ªΩ\n",
    "year = '2019'\n",
    "\n",
    "# ÂÆåÊï¥URL\n",
    "url = f'https://api.usa.gov/crime/fbi/cde/hate-crime/state/VA?from=2020&type=race&to=2021&API_KEY=iiHnOKfno2Mgkt5AynpvPpUQTEyxE77jo1RU8PIv'\n",
    "\n",
    "# ÂèëÈÄÅGETËØ∑Ê±Ç\n",
    "response = requests.get(url)\n",
    "\n",
    "# Ê£ÄÊü•ÂìçÂ∫îÁä∂ÊÄÅÁ†Å\n",
    "if response.status_code == 200:\n",
    "    data = response.json()  # Ëß£ÊûêËøîÂõûÁöÑJSONÊï∞ÊçÆ\n",
    "    print(data)\n",
    "else:\n",
    "    print(\"Failed to retrieve data:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ENHYPEN (ÏóîÌïòÏù¥Ìîà) 'No Doubt' Official MV\n",
      "Description: ENHYPEN (ÏóîÌïòÏù¥Ìîà) 'No Doubt' Official MV\n",
      "\n",
      "Credits:\n",
      "Directed by Yunah Sheep\n",
      "\n",
      "‚ìí BELIFT LAB Inc. All Rights Reserved\n",
      "\n",
      "Connect with ENHYPEN\n",
      "OFFICIAL WEBSITE https://ENHYPEN.com\n",
      "ENHYPEN Weverse https://www.weverse.io/enhypen\n",
      "OFFICIAL YOUTUBE https://www.youtube.com/ENHYPENOFFICIAL\n",
      "OFFICIAL X (TWITTER) https://twitter.com/ENHYPEN\n",
      "ENHYPEN X (TWITTER) https://twitter.com/ENHYPEN_members\n",
      "OFFICIAL FACEBOOK https://www.facebook.com/officialENHYPEN\n",
      "OFFICIAL INSTAGRAM https://www.instagram.com/enhypen\n",
      "OFFICIAL TIKTOK  https://www.tiktok.com/@enhypen\n",
      "OFFICIAL WEIBO https://weibo.com/ENHYPEN\n",
      "OFFICIAL BILIBILI https://space.bilibili.com/3493119035181246\n",
      "OFFICIAL JAPAN X (TWITTER) https://twitter.com/ENHYPEN_JP\n",
      "\n",
      "#ENHYPEN #ÏóîÌïòÏù¥Ìîà #ROMANCE_UNTOLD_daydream #NoDoubt\n",
      "Duration: PT3M5S\n",
      "View count: 28195106\n",
      "Like count: 819592\n",
      "Comment count: 64086\n",
      "\n",
      "Top Comments:\n",
      "@attaetude: I LOVE THE CHOREOGRAPHY THE SHOULDER DANCE AND THE WHISTLE THING THE SONG THE OUTFITS THE CHORUS EVERITHJNG!\n",
      "@filzxwonie: can we talk about how enhypen still manages to include their lore into every single song , while having drastically different concepts ?? im soso proud this song is absolutely AMAZING, the visuals, the vocals, the outfits, every single thing has me jawdropped.\n",
      "@carachann5416: Jay&#39;s visual, Niki&#39;s visual, Sunghoon&#39;s visual, Jungwon&#39;s visual, Jake&#39;s visual, Heeseung&#39;s visual, and Sunoo&#39;s visual are drive me crazy<a href=\"UCkszU2WH9gy1mb0dV-11UJg/YvgfY-LIBpjChgHKyYCQBg\"></a>\n",
      "@mieoyuiki: This music video is a masterpiece! The visuals, the choreography, and their expressions are on point. Enhypen keeps raising the bar every time!\n",
      "@Tiramisu_Cake07: The song, the vocals, the choreography, the visuals, the beat, Jungwon, Heeseung, Jay, Jake, Sunghoon, Sunoo, NI-KI, everything is PERFECT I‚Äôm impressed\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "# ‰Ω†ÁöÑAPIÂØÜÈí•\n",
    "api_key = 'AIzaSyC5VGKOdaG9IW3lauaZ03yk0nkP3oS4cTc'\n",
    "\n",
    "# ÂàõÂª∫ YouTube client ÂØπË±°\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# ËßÜÈ¢ëID\n",
    "video_id = 'rDolt3jJRsM'\n",
    "\n",
    "# Ë∞ÉÁî®APIËé∑ÂèñËßÜÈ¢ëËØ¶ÊÉÖ\n",
    "video_response = youtube.videos().list(\n",
    "    part='snippet,contentDetails,statistics',\n",
    "    id=video_id\n",
    ").execute()\n",
    "\n",
    "# ËæìÂá∫ËßÜÈ¢ëËØ¶ÁªÜ‰ø°ÊÅØ\n",
    "for video in video_response.get('items', []):\n",
    "    title = video['snippet']['title']\n",
    "    description = video['snippet']['description']\n",
    "    duration = video['contentDetails']['duration']\n",
    "    view_count = video['statistics']['viewCount']\n",
    "    like_count = video['statistics'].get('likeCount', 'Unavailable')\n",
    "    comment_count = video['statistics'].get('commentCount', 'Unavailable')\n",
    "\n",
    "    print(f'Title: {title}')\n",
    "    print(f'Description: {description}')\n",
    "    print(f'Duration: {duration}')\n",
    "    print(f'View count: {view_count}')\n",
    "    print(f'Like count: {like_count}')\n",
    "    print(f'Comment count: {comment_count}')\n",
    "\n",
    "# Ëé∑ÂèñÁÉ≠Èó®ËØÑËÆ∫\n",
    "comments_response = youtube.commentThreads().list(\n",
    "    part='snippet',\n",
    "    videoId=video_id,\n",
    "    order='relevance',  # ÊåâÁõ∏ÂÖ≥ÊÄßÊéíÂ∫è\n",
    "    maxResults=5  # Ëé∑ÂèñÂâç5‰∏™ÁÉ≠Èó®ËØÑËÆ∫\n",
    ").execute()\n",
    "\n",
    "print(\"\\nTop Comments:\")\n",
    "for comment in comments_response.get('items', []):\n",
    "    author = comment['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
    "    text = comment['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "    print(f'{author}: {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ENHYPEN ÏóîÌïòÏù¥Ìîà No Doubt Official MV.csv\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import dateutil.parser\n",
    "\n",
    "# YouTube API Key\n",
    "api_key = 'AIzaSyC5VGKOdaG9IW3lauaZ03yk0nkP3oS4cTc'\n",
    "\n",
    "# Video ID\n",
    "video_id = 'rDolt3jJRsM'\n",
    "\n",
    "# Create a YouTube object\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Fetch video details\n",
    "video_response = youtube.videos().list(\n",
    "    part='snippet,contentDetails,statistics',\n",
    "    id=video_id\n",
    ").execute()\n",
    "\n",
    "# Extract video and channel details\n",
    "video = video_response['items'][0]\n",
    "snippet = video['snippet']\n",
    "statistics = video['statistics']\n",
    "content_details = video['contentDetails']\n",
    "\n",
    "# Calculate days since published\n",
    "published_at = dateutil.parser.parse(snippet['publishedAt'])\n",
    "days_since_published = (datetime.now(published_at.tzinfo) - published_at).days\n",
    "\n",
    "# Get channel details for subscriber count\n",
    "channel_id = snippet['channelId']\n",
    "channel_response = youtube.channels().list(\n",
    "    part='statistics',\n",
    "    id=channel_id\n",
    ").execute()\n",
    "subscriber_count = channel_response['items'][0]['statistics']['subscriberCount']\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    'Title': snippet['title'],\n",
    "    'Description': snippet['description'],\n",
    "    'Published At': snippet['publishedAt'],\n",
    "    'Days Since Published': days_since_published,\n",
    "    'View Count': statistics['viewCount'],\n",
    "    'Like Count': statistics.get('likeCount', 'Unavailable'),\n",
    "    'Comment Count': statistics.get('commentCount', 'Unavailable'),\n",
    "    'Subscriber Count': subscriber_count,\n",
    "    'Category ID': snippet['categoryId'],\n",
    "    'Definition': content_details['definition']\n",
    "}\n",
    "df = pd.DataFrame([data])\n",
    "\n",
    "# Get top 10 comments\n",
    "comments_response = youtube.commentThreads().list(\n",
    "    part='snippet',\n",
    "    videoId=video_id,\n",
    "    order='relevance',\n",
    "    maxResults=10\n",
    ").execute()\n",
    "\n",
    "top_comments = [comment['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                for comment in comments_response.get('items', [])]\n",
    "df['Top Comments'] = pd.Series([top_comments])\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "safe_title = \"\".join(x for x in snippet['title'] if x.isalnum() or x in \" _-\").rstrip()\n",
    "filename = f\"{safe_title}.csv\"\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f'Data saved to {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ENHYPEN (ÏóîÌïòÏù¥Ìîà) 'No Doubt' Official MV.csv\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import dateutil.parser\n",
    "\n",
    "# Replace 'YOUR_API_KEY' with your actual YouTube API key\n",
    "#api_key = 'YOUR_API_KEY'\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# YouTube Video ID\n",
    "video_id = 'rDolt3jJRsM'\n",
    "\n",
    "# Fetching video details\n",
    "video_response = youtube.videos().list(\n",
    "    part='snippet,contentDetails,statistics',\n",
    "    id=video_id\n",
    ").execute()\n",
    "\n",
    "video = video_response['items'][0]\n",
    "snippet = video['snippet']\n",
    "statistics = video['statistics']\n",
    "content_details = video['contentDetails']\n",
    "\n",
    "# Calculating days since the video was published\n",
    "published_at = dateutil.parser.parse(snippet['publishedAt'])\n",
    "days_since_published = (datetime.now(published_at.tzinfo) - published_at).days\n",
    "\n",
    "# Fetching channel details for subscriber count\n",
    "channel_response = youtube.channels().list(\n",
    "    part='statistics',\n",
    "    id=snippet['channelId']\n",
    ").execute()\n",
    "subscriber_count = channel_response['items'][0]['statistics']['subscriberCount']\n",
    "\n",
    "# Fetching top 10 comments\n",
    "comments_response = youtube.commentThreads().list(\n",
    "    part='snippet',\n",
    "    videoId=video_id,\n",
    "    order='relevance',\n",
    "    maxResults=10\n",
    ").execute()\n",
    "\n",
    "top_comments = [comment['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                for comment in comments_response.get('items', [])]\n",
    "\n",
    "# Preparing data for CSV\n",
    "data = {\n",
    "    'Title': snippet['title'],\n",
    "    'Description': snippet['description'],\n",
    "    'Published At': snippet['publishedAt'],\n",
    "    'Days Since Published': days_since_published,\n",
    "    'View Count': statistics['viewCount'],\n",
    "    'Like Count': statistics.get('likeCount', 'Unavailable'),\n",
    "    'Comment Count': statistics.get('commentCount', 'Unavailable'),\n",
    "    'Subscriber Count': subscriber_count,\n",
    "    'Category ID': snippet['categoryId'],\n",
    "    'Definition': content_details['definition'],\n",
    "    'Tags': snippet.get('tags', []),\n",
    "    'Top Comments': top_comments\n",
    "}\n",
    "df = pd.DataFrame([data])\n",
    "\n",
    "# Saving to CSV\n",
    "filename = f\"{snippet['title']}.csv\".replace('/', '_').replace('\\\\', '_')  # Cleaning filename\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f'Data saved to {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Genre         Track Name        Artist  \\\n",
      "0        Pop     Shiny Beat TGD  Pop Star RXC   \n",
      "1        Pop   Shiny Rhythm GPO    Rhythm FPV   \n",
      "2        Pop    Happy Pulse IEY    Melody CJJ   \n",
      "3        Pop    Shiny Pulse NZJ    Melody VQW   \n",
      "4        Pop     Shiny Beat RUZ     Music WJL   \n",
      "..       ...                ...           ...   \n",
      "395  Hip Hop  Street Rhythm LSE    Street NUU   \n",
      "396  Hip Hop  Street Rhythm KQU    Street MPC   \n",
      "397  Hip Hop    Street Vibe AJZ     Urban SPY   \n",
      "398  Hip Hop    Smooth Beat FRU     Urban FUN   \n",
      "399  Hip Hop   Street Sound GAE     Urban UJK   \n",
      "\n",
      "                       YouTube MV Link  Duration (min)  \n",
      "0    https://youtube.com/watch?v=65302            2.60  \n",
      "1    https://youtube.com/watch?v=10851            4.78  \n",
      "2    https://youtube.com/watch?v=55082            4.31  \n",
      "3    https://youtube.com/watch?v=85674            3.08  \n",
      "4    https://youtube.com/watch?v=57819            2.99  \n",
      "..                                 ...             ...  \n",
      "395  https://youtube.com/watch?v=90481            4.59  \n",
      "396  https://youtube.com/watch?v=44483            3.17  \n",
      "397  https://youtube.com/watch?v=32973            4.42  \n",
      "398  https://youtube.com/watch?v=40365            4.35  \n",
      "399  https://youtube.com/watch?v=86818            3.10  \n",
      "\n",
      "[400 rows x 5 columns]\n",
      "\n",
      "Êï∞ÊçÆÈõÜÂ∑≤‰øùÂ≠ò‰∏∫ 'music_tracks_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "\n",
    "def generate_track_name(genre):\n",
    "    \"\"\"ÁîüÊàêÂ∏¶ÊúâÊµÅÊ¥æÈ£éÊ†ºÁöÑÈü≥‰πêTrackÂêçÁß∞\"\"\"\n",
    "    adjectives = {\n",
    "        'Pop': ['Shiny', 'Happy', 'Bright', 'Lovely', 'Sweet'],\n",
    "        'Rock': ['Loud', 'Wild', 'Intense', 'Dark', 'Heavy'],\n",
    "        'Electronic': ['Cyber', 'Digital', 'Synthetic', 'Techno', 'Future'],\n",
    "        'Hip Hop': ['Street', 'Urban', 'Raw', 'Smooth', 'Groove']\n",
    "    }\n",
    "    \n",
    "    def random_string(length=3):\n",
    "        return ''.join(random.choices(string.ascii_uppercase, k=length))\n",
    "    \n",
    "    return f\"{random.choice(adjectives[genre])} {random.choice(['Beat', 'Rhythm', 'Sound', 'Pulse', 'Vibe'])} {random_string()}\"\n",
    "\n",
    "def generate_artists(genre):\n",
    "    \"\"\"ÁîüÊàêÁ¨¶ÂêàÊµÅÊ¥æÁöÑËâ∫ÊúØÂÆ∂ÂêçÁß∞\"\"\"\n",
    "    artist_prefixes = {\n",
    "        'Pop': ['Pop Star', 'Music', 'Melody', 'Rhythm'],\n",
    "        'Rock': ['Rock Band', 'Guitar', 'Sonic', 'Metal'],\n",
    "        'Electronic': ['Digital', 'Synth', 'Tech', 'Beat'],\n",
    "        'Hip Hop': ['Flow', 'Street', 'Urban', 'Rap']\n",
    "    }\n",
    "    \n",
    "    def random_string(length=3):\n",
    "        return ''.join(random.choices(string.ascii_uppercase, k=length))\n",
    "    \n",
    "    return f\"{random.choice(artist_prefixes[genre])} {random_string()}\"\n",
    "\n",
    "def generate_dataset():\n",
    "    random.seed(42)  # ËÆæÁΩÆÈöèÊú∫Êï∞ÁßçÂ≠ê‰ª•‰øùËØÅÊØèÊ¨°ÁîüÊàêÁöÑÊï∞ÊçÆ‰∏ÄËá¥\n",
    "    genres = ['Pop', 'Rock', 'Electronic', 'Hip Hop']\n",
    "    all_tracks = []\n",
    "\n",
    "    for genre in genres:\n",
    "        for _ in range(100):\n",
    "            track = {\n",
    "                'Genre': genre,\n",
    "                'Track Name': generate_track_name(genre),\n",
    "                'Artist': generate_artists(genre),\n",
    "                'YouTube MV Link': f\"https://youtube.com/watch?v={random.randint(10000, 99999)}\",\n",
    "                'Duration (min)': round(random.uniform(2.5, 5.5), 2)\n",
    "            }\n",
    "            all_tracks.append(track)\n",
    "\n",
    "    df = pd.DataFrame(all_tracks)\n",
    "    return df\n",
    "\n",
    "# ÁîüÊàêÊï∞ÊçÆÈõÜ\n",
    "dataset = generate_dataset()\n",
    "\n",
    "# Â±ïÁ§∫ÊâÄÊúâÊï∞ÊçÆ\n",
    "print(dataset)\n",
    "\n",
    "# ‰øùÂ≠òCSV\n",
    "dataset.to_csv('music_tracks_dataset.csv', index=False)\n",
    "print(\"\\nÊï∞ÊçÆÈõÜÂ∑≤‰øùÂ≠ò‰∏∫ 'music_tracks_dataset.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Genre         Artist         Track\n",
      "0  Rock  Artist_1_Rock  Track_1_Rock\n",
      "1  Rock  Artist_2_Rock  Track_2_Rock\n",
      "2  Rock  Artist_3_Rock  Track_3_Rock\n",
      "3  Rock  Artist_4_Rock  Track_4_Rock\n",
      "4  Rock  Artist_5_Rock  Track_5_Rock\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '/mnt/data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Save the DataFrame to CSV\u001b[39;00m\n\u001b[1;32m     28\u001b[0m csv_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/data/Music_Videos_Dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 29\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(csv_file_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     31\u001b[0m csv_file_path\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3965\u001b[0m )\n\u001b[0;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[1;32m   3968\u001b[0m     path_or_buf,\n\u001b[1;32m   3969\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   3970\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   3971\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m   3972\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m   3973\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   3974\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[1;32m   3975\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   3976\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[1;32m   3977\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m   3978\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[1;32m   3979\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[1;32m   3980\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[1;32m   3981\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[1;32m   3982\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[1;32m   3983\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   3984\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/formats/csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    254\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    255\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[1;32m    256\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[1;32m    257\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[1;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 749\u001b[0m     check_parent_directory(\u001b[38;5;28mstr\u001b[39m(handle))\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '/mnt/data'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Let's create a hypothetical dataset representing 4 music genres with 100 music videos (tracks) each on YouTube.\n",
    "\n",
    "# Define the genres\n",
    "genres = ['Rock', 'Hip-Hop', 'Pop', 'Electronic/Dance']\n",
    "\n",
    "# Simulating the data generation by randomly creating 'artist - track' names\n",
    "# For simplicity, the artist names and tracks are made up and should be replaced with real data.\n",
    "\n",
    "# Dictionary to hold genre, artist, and track\n",
    "data = {'Genre': [], 'Artist': [], 'Track': []}\n",
    "\n",
    "# Generating dummy data for each genre\n",
    "for genre in genres:\n",
    "    for i in range(1, 101):  # Assuming 100 unique tracks per genre\n",
    "        data['Genre'].append(genre)\n",
    "        data['Artist'].append(f\"Artist_{i}_{genre}\")\n",
    "        data['Track'].append(f\"Track_{i}_{genre}\")\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display a portion of the DataFrame to verify\n",
    "print(df.head())\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "csv_file_path = \"/mnt/data/Music_Videos_Dataset.csv\"\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "csv_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Êàë‰ªéËøôÈáåÂºÄÂßãÂÜôÁöÑ*\n",
    "# Data Collection\n",
    "This project collects music data through YouTube and Spotify APIs, covering information on the works of 20 representative artists in five genres: Electronic, Jazz, Hip-Hop, Pop and Rock. The data processing flow is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. YouTube Data Collection\n",
    "## 1.1 Acquiring Official Music Video Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'song_data.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m all_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Read song data and fetch YouTube statistics\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msong_data.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[1;32m     16\u001b[0m         artist \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'song_data.txt'"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "\n",
    "# API Key\n",
    "api_key = \"AIzaSyDtKE-4QZj6EA-rwG7cj5gMJxdt4Fe14Nw\"\n",
    "\n",
    "# Initialize YouTube API client\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# List to store data\n",
    "all_data = []\n",
    "\n",
    "# Read song data and fetch YouTube statistics\n",
    "with open('song_data.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        artist = line.strip()\n",
    "        query = f\"{artist} official music video\"\n",
    "\n",
    "        # Search request for the query\n",
    "        search_request = youtube.search().list(\n",
    "            part=\"snippet\",\n",
    "            q=query,\n",
    "            maxResults=5,\n",
    "            type=\"video\",\n",
    "            order='relevance'\n",
    "        )\n",
    "        search_response = search_request.execute()\n",
    "\n",
    "        for item in search_response['items']:\n",
    "            video_id = item['id']['videoId']\n",
    "            video_request = youtube.videos().list(\n",
    "                part=\"snippet,contentDetails,statistics\",\n",
    "                id=video_id\n",
    "            )\n",
    "            video_response = video_request.execute()\n",
    "\n",
    "            for video in video_response['items']:\n",
    "                snippet = video['snippet']\n",
    "                content_details = video['contentDetails']\n",
    "                statistics = video['statistics']\n",
    "\n",
    "                # Calculate days since video was published\n",
    "                published_at = dateutil.parser.parse(snippet['publishedAt'])\n",
    "                days_since_published = (datetime.now(published_at.tzinfo) - published_at).days\n",
    "\n",
    "                # Fetch channel details for subscriber count\n",
    "                channel_response = youtube.channels().list(\n",
    "                    part='statistics',\n",
    "                    id=snippet['channelId']\n",
    "                ).execute()\n",
    "                subscriber_count = channel_response['items'][0]['statistics'].get('subscriberCount', '0')\n",
    "\n",
    "                # Fetch comments\n",
    "                comments_request = youtube.commentThreads().list(\n",
    "                    part='snippet',\n",
    "                    videoId=video_id,\n",
    "                    order='relevance',\n",
    "                    maxResults=10\n",
    "                )\n",
    "                comments_response = comments_request.execute()\n",
    "\n",
    "                comments = [comment['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                            for comment in comments_response.get('items', [])]\n",
    "\n",
    "                # Store all data in a dictionary\n",
    "                video_data = {\n",
    "                    'Video ID': video_id,\n",
    "                    'Title': snippet['title'],\n",
    "                    'Description': snippet['description'],\n",
    "                    'Published At': snippet['publishedAt'],\n",
    "                    'Days Since Published': days_since_published,\n",
    "                    'View Count': statistics.get('viewCount', '0'),\n",
    "                    'Like Count': statistics.get('likeCount', '0'),\n",
    "                    'Comment Count': statistics.get('commentCount', '0'),\n",
    "                    'Comments': comments,\n",
    "                    'Subscriber Count': subscriber_count,\n",
    "                    'Category ID': snippet['categoryId'],\n",
    "                    'Definition': content_details['definition'],\n",
    "                    'Duration': content_details['duration']\n",
    "                }\n",
    "\n",
    "                all_data.append(video_data)\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "final_df = pd.DataFrame(all_data)\n",
    "\n",
    "# Optionally save the DataFrame to a CSV file\n",
    "final_df.to_csv('Detailed_YouTube_Video_Data.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Video ID                                              Title  \\\n",
      "0  1VQ_3sBZEm0    Foo Fighters - Learn To Fly (Official HD Video)   \n",
      "1  eBG7P-K-r1Y        Foo Fighters - Everlong (Official HD Video)   \n",
      "2  SBjQ9tuuTJQ                       Foo Fighters - The Pretender   \n",
      "3  EqWRaAF6_WY         Foo Fighters - My Hero (Official HD Video)   \n",
      "4  h_L4Rixya64  Foo Fighters - Best Of You (Official Music Video)   \n",
      "\n",
      "                                         Description          Published At  \\\n",
      "0  Foo Fighters' official music video for 'Learn ...  2009-10-03T04:46:13Z   \n",
      "1  \"Everlong\" by Foo Fighters \\nListen to Foo Fig...  2009-10-03T04:49:58Z   \n",
      "2  Watch the official music video for \"The Preten...  2009-10-03T04:46:14Z   \n",
      "3  \"My Hero\" by Foo Fighters \\nListen to Foo Figh...  2011-03-18T19:35:42Z   \n",
      "4  Watch the official music video for \"Best Of Yo...  2009-10-03T20:49:33Z   \n",
      "\n",
      "   Days Since Published View Count Like Count Comment Count  \\\n",
      "0                  5550  183921366     808172         33856   \n",
      "1                  5550  324414087    1821270         53201   \n",
      "2                  5550  588092620    2785700         92245   \n",
      "3                  5018   87531478     564448         26099   \n",
      "4                  5549  265573360    1281212         34999   \n",
      "\n",
      "                                            Comments Subscriber Count  \\\n",
      "0  [I‚Äôm just realising how great Dave grohls acti...          1290000   \n",
      "1  [Dad died today. \\r<br>1:20 am.\\r<br>A five da...          1290000   \n",
      "2  [So thankful for this awesome song. I&#39;ll b...          1290000   \n",
      "3  [My son and I was supposed to spend the summer...          1290000   \n",
      "4  [The emotion in his face and his voice transce...          1290000   \n",
      "\n",
      "  Category ID Definition Duration  \n",
      "0          10         hd  PT4M37S  \n",
      "1          10         hd  PT4M52S  \n",
      "2          10         hd  PT4M31S  \n",
      "3          10         hd   PT4M3S  \n",
      "4          10         hd  PT4M16S  \n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import dateutil.parser\n",
    "\n",
    "# API Key\n",
    "api_key = \"AIzaSyDtKE-4QZj6EA-rwG7cj5gMJxdt4Fe14Nw\"\n",
    "\n",
    "# Initialize YouTube API client\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# List to store data\n",
    "all_data = []\n",
    "\n",
    "# Read song data and fetch YouTube statistics\n",
    "# We should have put 20*5 names of singers, but for the sake of presentation, we choose three singers as a demonstration here \n",
    "with open('find.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        artist = line.strip()\n",
    "        query = f\"{artist} official music video\"\n",
    "\n",
    "        # Search request for the query\n",
    "        search_request = youtube.search().list(\n",
    "            part=\"snippet\",\n",
    "            q=query,\n",
    "            maxResults=5,\n",
    "            type=\"video\",\n",
    "            order='relevance'\n",
    "        )\n",
    "        search_response = search_request.execute()\n",
    "\n",
    "        for item in search_response['items']:\n",
    "            video_id = item['id']['videoId']\n",
    "            video_request = youtube.videos().list(\n",
    "                part=\"snippet,contentDetails,statistics\",\n",
    "                id=video_id\n",
    "            )\n",
    "            video_response = video_request.execute()\n",
    "\n",
    "            for video in video_response['items']:\n",
    "                snippet = video['snippet']\n",
    "                content_details = video['contentDetails']\n",
    "                statistics = video['statistics']\n",
    "\n",
    "                # Calculate days since video was published\n",
    "                published_at = dateutil.parser.parse(snippet['publishedAt'])\n",
    "                days_since_published = (datetime.now(published_at.tzinfo) - published_at).days\n",
    "\n",
    "                # Fetch channel details for subscriber count\n",
    "                channel_response = youtube.channels().list(\n",
    "                    part='statistics',\n",
    "                    id=snippet['channelId']\n",
    "                ).execute()\n",
    "                subscriber_count = channel_response['items'][0]['statistics'].get('subscriberCount', '0')\n",
    "\n",
    "                # Fetch comments\n",
    "                comments_request = youtube.commentThreads().list(\n",
    "                    part='snippet',\n",
    "                    videoId=video_id,\n",
    "                    order='relevance',\n",
    "                    maxResults=10\n",
    "                )\n",
    "                comments_response = comments_request.execute()\n",
    "\n",
    "                comments = [comment['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                            for comment in comments_response.get('items', [])]\n",
    "\n",
    "                # Store all data in a dictionary\n",
    "                video_data = {\n",
    "                    'Video ID': video_id,\n",
    "                    'Title': snippet['title'],\n",
    "                    'Description': snippet['description'],\n",
    "                    'Published At': snippet['publishedAt'],\n",
    "                    'Days Since Published': days_since_published,\n",
    "                    'View Count': statistics.get('viewCount', '0'),\n",
    "                    'Like Count': statistics.get('likeCount', '0'),\n",
    "                    'Comment Count': statistics.get('commentCount', '0'),\n",
    "                    'Comments': comments,\n",
    "                    'Subscriber Count': subscriber_count,\n",
    "                    'Category ID': snippet['categoryId'],\n",
    "                    'Definition': content_details['definition'],\n",
    "                    'Duration': content_details['duration']\n",
    "                }\n",
    "\n",
    "                all_data.append(video_data)\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "final_df = pd.DataFrame(all_data)\n",
    "\n",
    "# Optionally save the DataFrame to a CSV file\n",
    "final_df.to_csv('Example_Detailed_YouTube_Video_Data.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Merge artist name and music genre into csv\n",
    "The row of the initial find.csv(include artist name and genre) is repeated five times per row to correspond to the five mv chosen by each artist (python)\n",
    "Then merge these two columns into the csv (copy manually) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed successfully and saved as ./Example_singer_info.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file = './find.csv'  \n",
    "output_file = './Example_singer_info.csv'  \n",
    "\n",
    "# Load the input CSV file\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Create an empty DataFrame to store repeated rows\n",
    "repeated_df = pd.DataFrame()\n",
    "\n",
    "# Repeat each row 5 times and append it to the new DataFrame\n",
    "for i in range(len(df)):\n",
    "    repeated_df = pd.concat([repeated_df, pd.DataFrame([df.iloc[i]] * 5)], ignore_index=True)\n",
    "\n",
    "# Save the processed DataFrame to a new CSV file\n",
    "repeated_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Data processed successfully and saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data preprocessing\n",
    "Extract the song name from the csv's title and generate a new CSV file containing the singer's and song's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song titles extracted and saved to ./Example_extracted_song_names.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the CSV file with YouTube video data\n",
    "df = pd.read_csv('./Example_Detailed_YouTube_Video_Data.csv', encoding='MacRoman')\n",
    "\n",
    "# Function to extract the song name from the title\n",
    "def extract_song_name(title):\n",
    "    # Use regular expression to find text between \" - \" and \"(\"\n",
    "    match = re.search(r' - (.*?) \\(.*\\)', title)\n",
    "    if match:\n",
    "        return match.group(1) \n",
    "    else:\n",
    "        return title  \n",
    "\n",
    "# Create a new DataFrame with extracted song names\n",
    "new_df = pd.DataFrame({\n",
    "    'Extracted Song Name': df['Title'].apply(extract_song_name)\n",
    "})\n",
    "\n",
    "# Save the new DataFrame to a CSV file in the current directory\n",
    "output_file = './Example_extracted_song_names.csv'\n",
    "new_df.to_csv(output_file, index=False, encoding='MacRoman')\n",
    "\n",
    "print(f\"Song titles extracted and saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to ./Example_extracted_song_names_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_file = './Example_extracted_song_names.csv'\n",
    "df = pd.read_csv(input_file, encoding='MacRoman')\n",
    "\n",
    "# Function to remove content inside brackets (e.g., [example])\n",
    "def remove_brackets(text):\n",
    "    return re.sub(r'\\[.*?\\]', '', text)\n",
    "\n",
    "# Apply the function to the 'Extracted Song Name' column\n",
    "df['Extracted Song Name'] = df['Extracted Song Name'].apply(remove_brackets)\n",
    "\n",
    "\n",
    "output_file = './Example_extracted_song_names_cleaned.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Processed data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually merge example_extracted_song_name_cleaned.csv with example_artist_info.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spotify Collection\n",
    "## 2.1 Acquiring Track Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: Learn to Fly by Foo Fighters\n",
      "Found: Everlong by Foo Fighters\n",
      "Found: The Pretender by Foo Fighters\n",
      "Found: My Hero by Foo Fighters\n",
      "Found: Best of You by Foo Fighters\n",
      "Found: Mr. Brightside by The Killers\n",
      "Found: When You Were Young by The Killers\n",
      "Found: Mr. Brightside by The Killers\n",
      "Found: Somebody Told Me by The Killers\n",
      "Found: One Empty Grave by A Sound of Thunder\n",
      "Found: Basket Case by Green Day\n",
      "Found: When I Come Around by Green Day\n",
      "Found: American Idiot by Green Day\n",
      "Found: Boulevard of Broken Dreams by Green Day\n",
      "Found: Wake Me up When September Ends by Green Day\n",
      "Processed data saved to: ./Example_spotify_track_info.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "# Set up Spotify client credentials\n",
    "client_credentials_manager = SpotifyClientCredentials(\n",
    "    client_id='3e1596de002340b898f5d10c9aeae4ea',\n",
    "    client_secret='526fa44678974475b0f6ba5d8efd16c4'\n",
    ")\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = './Example_extracted_song_names_cleaned.csv'\n",
    "output_file = './Example_spotify_track_info.csv'\n",
    "\n",
    "# Load the input CSV file\n",
    "df = pd.read_csv(input_file, encoding='MacRoman')\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Process each row in the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    song = row['Extracted Song Name']\n",
    "    artist = row['singer']\n",
    "    query = f'{song} {artist}'  # Concatenate song and artist for the search query\n",
    "\n",
    "    try:\n",
    "        # Search for the track on Spotify\n",
    "        result = sp.search(q=query, limit=1, type='track')\n",
    "        tracks = result.get('tracks', {}).get('items', [])\n",
    "\n",
    "        if tracks:\n",
    "            # If a track is found, extract its details\n",
    "            track = tracks[0]\n",
    "            track_info = {\n",
    "                'Track Name': track['name'],\n",
    "                'Artist Name': track['artists'][0]['name'],\n",
    "                'Album Name': track['album']['name'],\n",
    "                'Popularity': track['popularity'],\n",
    "                'Duration (ms)': track['duration_ms'],\n",
    "                'Track ID': track['id'],\n",
    "                'Spotify URL': track['external_urls']['spotify']\n",
    "            }\n",
    "            print(f\"Found: {track_info['Track Name']} by {track_info['Artist Name']}\")\n",
    "        else:\n",
    "            # If no track is found, append placeholders\n",
    "            print(f\"Track not found: {query}\")\n",
    "            track_info = {\n",
    "                'Track Name': song,\n",
    "                'Artist Name': artist,\n",
    "                'Album Name': None,\n",
    "                'Popularity': None,\n",
    "                'Duration (ms)': None,\n",
    "                'Track ID': None,\n",
    "                'Spotify URL': None\n",
    "            }\n",
    "\n",
    "        # Append the result to the list\n",
    "        results.append(track_info)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle exceptions during the search\n",
    "        print(f\"Error processing query '{query}': {e}\")\n",
    "        results.append({\n",
    "            'Track Name': song,\n",
    "            'Artist Name': artist,\n",
    "            'Album Name': None,\n",
    "            'Popularity': None,\n",
    "            'Duration (ms)': None,\n",
    "            'Track ID': None,\n",
    "            'Spotify URL': None\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "output_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the output DataFrame to a CSV file\n",
    "output_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Processed data saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Obtaining Artist Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to: ./Example_artist_data_with_followers_and_popularity.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Function to obtain an access token for Spotify API\n",
    "def get_access_token(client_id, client_secret):\n",
    "    auth_url = 'https://accounts.spotify.com/api/token'\n",
    "    auth_data = {\n",
    "        'grant_type': 'client_credentials',\n",
    "        'client_id': client_id,\n",
    "        'client_secret': client_secret\n",
    "    }\n",
    "    response = requests.post(auth_url, data=auth_data)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['access_token']\n",
    "    else:\n",
    "        raise Exception('Failed to obtain access token')\n",
    "\n",
    "# Function to get the Spotify artist ID using the artist name\n",
    "def get_artist_id(artist_name, access_token):\n",
    "    search_url = f'https://api.spotify.com/v1/search?q={artist_name}&type=artist&limit=1'\n",
    "    headers = {'Authorization': f'Bearer {access_token}'}\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        search_results = response.json()\n",
    "        artists = search_results['artists']['items']\n",
    "        if artists:\n",
    "            return artists[0]['id']\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to retrieve the artist's followers and popularity\n",
    "def get_artist_followers(artist_id, access_token):\n",
    "    artist_url = f'https://api.spotify.com/v1/artists/{artist_id}'\n",
    "    headers = {'Authorization': f'Bearer {access_token}'}\n",
    "    response = requests.get(artist_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        artist_data = response.json()\n",
    "        return artist_data['followers']['total'], artist_data['popularity']\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Load the input CSV file \n",
    "input_file = './Example_singer_info.csv' \n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Spotify API credentials\n",
    "client_id = '31aba57b31344fdebf98f51375d07834' \n",
    "client_secret = '2c4f929786784910bc9a843518785cae'  \n",
    "\n",
    "# Obtain Spotify API access token\n",
    "access_token = get_access_token(client_id, client_secret)\n",
    "\n",
    "# Initialize lists to store followers and popularity data\n",
    "followers_list = []\n",
    "popularity_list = []\n",
    "\n",
    "# Process each artist in the DataFrame\n",
    "for artist_name in df['artist']:\n",
    "    artist_id = get_artist_id(artist_name, access_token)\n",
    "    if artist_id:\n",
    "        followers, popularity = get_artist_followers(artist_id, access_token)\n",
    "        followers_list.append(followers)\n",
    "        popularity_list.append(popularity)\n",
    "    else:\n",
    "        # If the artist is not found, append None\n",
    "        followers_list.append(None)\n",
    "        popularity_list.append(None)\n",
    "\n",
    "# Add followers and popularity data to the DataFrame\n",
    "df['Followers'] = followers_list\n",
    "df['Popularity'] = popularity_list\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file = './Example_artist_data_with_followers_and_popularity.csv'  \n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Processed data saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data integration and cleansing\n",
    "First manually merge Spotify and YouTube csv.\n",
    "Then using Spotify and YouTube artists as the matching key, match to verify artist match, if not, then delete the mismatched rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to: ./Example_spotify_youtube.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file = './Example_Detailed_YouTube_Video_Data.csv' \n",
    "df = pd.read_csv(input_file, encoding='MacRoman')\n",
    "\n",
    "# Filter rows where 'Artist Name' matches 'Artist'\n",
    "df_cleaned = df[df['Artist Name'] == df['artist']]\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "output_file = './Example_spotify_youtube.csv'  \n",
    "df_cleaned.to_csv(output_file, index=False)\n",
    "print(f\"Cleaned data saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we completed all the data collection steps!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
