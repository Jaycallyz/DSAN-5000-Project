# Instructions 

`Note`: You should remove these instruction once you have read and understood them. They should not be included in your final submission.

`Remember:` Exactly what do you put on this page will be specific you your project and data. Some things might "make more sense" on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.  

## Suggested page structure

Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:

`Audience`:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts. 

- **Introduction and Motivation:** Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.
- **Overview of Methods:** Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.
- **Code:** Include the code you used to implement your workflow.
- **Summary and Interpretation of Results:** Summarize your findings, interpret the results, and discuss their technical implications.

## What to address 

The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.

Please do some form of "Feature selection" in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.

Please break this page into a "regression" section, "binary classification" section, and a "Multi-class classification" section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results. 

## Data Preprocessing

1. **Normalization or Standardization**: Apply techniques to scale the data appropriately.
2. **Feature Selection or Extraction**: Identify and select the most relevant features for your analysis.
3. **Encoding Categorical Variables**: Convert categorical variables into a suitable format for modeling.

## Model Selection

1. **Model Rationale**: Explain the reasons for selecting specific models or algorithms.
2. **Overview of Algorithms**: Provide a brief overview of the algorithms used 

## Training and Testing Strategy

1. **Split Methods**: Detail the splitting methods used (e.g., train-test split, cross-validation).
2. **Dataset Proportions**: Specify the proportions used for splitting the dataset.

## Model Evaluation Metrics

1. **Binary Classification Metrics**: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.
2. **Multiclass Classification Metrics**: Include metrics such as confusion matrix and macro/micro F1 score.
3. **Regression Metrics**: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc.

## Results

1. **Model Performance Summary**: Provide a summary of the model's performance.
2. **Visualizations**: Include visualizations of results (e.g., ROC curves, feature importance plots).

## Discussion

1. **Result Interpretation**: Interpret the results obtained from the analysis.
2. **Model Performance Comparison**: Compare the performance of different models.
3. **Insights Gained**: Share insights learned from the analysis.

# Regression
## Introduction and Motivation

This analysis aims to explore the factors influencing the popularity of content on a digital platform. We will use various regression models to predict popularity based on multiple features extracted from our dataset.

## Overview of Methods

We will use Support Vector Regression (SVR) and linear models like Ridge and Lasso regression. 

```python
import pandas as pd
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.inspection import permutation_importance
from sklearn.model_selection import train_test_split

# Load the dataset
df = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')

# Specify the feature columns
features = [
    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',
    'Subscriber Count', 'Definition', 'Mean Sentiment Score',
    'Duration_seconds', 'genre_label', 'singer_followers', 'singer_popularity'
]

# Ensure the target column 'popularity' exists in DataFrame
if 'Popularity' not in df.columns:
    raise ValueError("The 'popularity' column is missing from the DataFrame.")

# Split into input (X) and target (y)
X = df[features]  # Inputs
y = df['Popularity']  # Target

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Assuming the SVR model is already trained as svr_model
svr_model = SVR(kernel='rbf', C=100, gamma='auto')
svr_model.fit(X_train, y_train)

# Predict on the testing data
y_pred = svr_model.predict(X_test)
# Calculate mean squared error and R^2 score
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}, R^2 Score: {r2}")

# Perform permutation importance
perm_importance = permutation_importance(svr_model, X_test, y_test, n_repeats=30, random_state=42)

# Get importance scores
importance_scores = perm_importance.importances_mean

# Print feature importance
print("Feature importances:")
for i, feature in enumerate(features):
    print(f"{feature}: {importance_scores[i]}")
```

## Conclusion 

| Model            | MSE                      | R² Score                 |
|------------------|--------------------------|--------------------------|
| SVR              | 99.74699515216331        | 0.568546463925952        |
| Linear Regression| 123.52107678768438       | 0.46571217229730644      |
| Ridge Regression | 125.61840469866303       | 0.4566402243943334       |
| Lasso Regression | 124.26131518891665       | 0.46251028661381044      |

Feature importances from SVR: 
- Days Since Published: 0.14590051030422282
- View Count: 0.0689113148502238
- Like Count: 0.06300639647560934
- Comment Count: 0.004829835437846231
- Subscriber Count: 0.15936065653760564
- Definition: 0.05992188839233011
- Mean Sentiment Score: 0.037774091955604584
- Duration_seconds: 0.00110286673615743
- genre_label: 0.1581978297916415
- singer_followers: 0.0360773423716881
- singer_popularity: 0.6712934520249385

```python
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.barh(features, importance_scores, color='skyblue')
plt.xlabel('Importance Score')
plt.title('Feature Importance Scores')
plt.gca().invert_yaxis()  # Invert y-axis to have the highest score on top
plt.show()
```

### Result Interpretation
The Support Vector Regression (SVR) model outperformed the linear models in terms of Mean Squared Error (MSE) and R² score. The lower MSE and higher R² of the SVR indicate better performance in fitting the data compared to the Linear, Ridge, and Lasso regressions. The R² scores suggest that the SVR model was able to explain approximately 56.85% of the variance in the dataset, which is more than the approximately 46.57% by the Linear Regression, 45.66% by Ridge, and 46.25% by Lasso Regression.

### Insights
From the SVR model's permutation importance, 'singer_popularity' emerged as the most influential feature, significantly impacting the prediction of a song's popularity. This suggests that more popular singers tend to have more popular songs, highlighting the influence of an artist's existing reputation on new releases.


# Binary Classification

## Introduction and Motivation

The goal of this section is to predict whether a song is considered "popular" using binary classification methods. We aim to understand the features that significantly influence song popularity on digital platforms.


## Overview of Methods

In this section, we focus on Logistic Regression for binary classification. Logistic Regression is chosen for its ability to provide probabilities for outcomes and its interpretability.

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import train_test_split

# Assume df is your DataFrame and the preprocessing has been done to define 'is_popular'
features = [
    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',
    'Subscriber Count', 'Definition', 'Mean Sentiment Score',
    'Duration_seconds', 'genre_label', 'singer_followers', 'singer_popularity'
]
X = df[features]
y = df['is_popular']

# Splitting the dataset and under-sampling
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
rus = RandomUnderSampler(random_state=42)
X_resampled, y_resampled = rus.fit_resample(X_train, y_train)

# Logistic Regression model
lr_model = LogisticRegression(random_state=42, max_iter=1000)
lr_model.fit(X_resampled, y_resampled)

# Making predictions and evaluating the model
y_pred = lr_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print("Logistic Regression Model Evaluation")
print("Accuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", class_report)
```

## Results
The Logistic Regression model showed an accuracy of 77.33%, with a detailed classification report indicating precision, recall, and F1-score for both classes.

### Model Performance Summary

Comparison of model performance metrics for different classification models used in predicting song popularity:

| Model                  | Accuracy | Precision | Recall | F1-Score |
|------------------------|----------|-----------|--------|----------|
| **Logistic Regression**| 77.33%   | 0.96 (False), 0.35 (True) | 0.77 (False), 0.80 (True) | 0.85 (False), 0.48 (True) |
| **SVM (Best Kernel)**  | 75.00%   | 0.91 (False), 0.26 (True) | 0.78 (False), 0.50 (True) | 0.84 (False), 0.34 (True) |
| **Random Forest**      | 71.00%   | 0.92 (False), 0.25 (True) | 0.72 (False), 0.60 (True) | 0.81 (False), 0.35 (True) |


## Conclusion
The Logistic Regression model, adjusted for class imbalance via under-sampling, provided satisfactory classification results, proving effective for identifying popular songs.

## Result Interpretation
The model was particularly strong in identifying non-popular songs (class 'False') with high precision and recall. And it has a higher recall score on popular songs compared to other models.

## Model Performance Comparison
This model was compared to other binary classification models such as SVM and Random Forest. Logistic Regression was chosen for its balance between performance and interpretability in this specific context. And it has a higher recall score on popular songs compared to other models.


# Multi-class Classification
## Introduction and Motivation
The goal of this multi- class classification is to predict the interaction rate (like ratio= like count/ view count) of youtube mv. We want to predict the interaction rate of the content by features such as number of days since posting and number of comments.

This analysis firstly hopes to improve data analysis and model selection capabilities, and secondly tries to help content creators optimize their creation strategies.


## Overview of Methods
Multiple classification algorithms were used to predict the popularity (like ratio) of short video content. The like ratio was first categorized into three categories: low (≤0.7), medium (0.7-1.6), and high (>1.6), and then predicted using three models: logistic regression, decision tree, and random forest. To address data imbalance, SMOTE oversampling technique was used. The input features of the model included nine variables such as days of posting/comments, number of subscribers, sentiment score, video duration, number of creator followers, and popularity. The decision tree model was hyper-parametrically optimized by grid search (GridSearchCV) and the model performance was evaluated using cross-validation. This combination of methods enables comprehensive assessment and prediction of video content popularity, providing data support for content creation and platform operation.


## Code
logistic regression
```python
X = df[features]
y = df['Like Ratio Category']

# Data set splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature standardization
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)


# Initialize the Logistic Regression model (One-vs-Rest for multi-class)
model = LogisticRegression(multi_class='ovr', max_iter=1000, random_state=42)

# Train the model
model.fit(X_train_scaled, y_train)

# Predict on the test set
y_pred = model.predict(X_test_scaled)

X_scaled = scaler.fit_transform(X)  # Fit and transform the entire dataset

# Cross-validation to get a better estimate of the model performance
cross_val_scores = cross_val_score(model, X_scaled, y, cv=5)
print("\nCross-validation scores:", cross_val_scores)
print("\nMean cross-validation score:", cross_val_scores.mean())

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
```


```python
#param_grid
param_grid_refined = {
    'max_depth': [3,4,5],  
    'min_samples_split': [8,10,15],
    'min_samples_leaf': [3,4,5,6],   
    'criterion': ['entropy'],        
    'class_weight': ['balanced']  
}

grid_search_refined = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid_refined,
    cv=5,
    scoring='f1_macro', 
    n_jobs=-1
)

# train mode
grid_search_refined.fit(X_train_scaled, y_train)

print("Refined Best parameters:", grid_search_refined.best_params_)
print("Refined Best cross-validation score:", grid_search_refined.best_score_)

best_model_refined = DecisionTreeClassifier(
    **grid_search_refined.best_params_
)

best_model_refined.fit(X_train_scaled, y_train)

y_pred_refined = best_model_refined.predict(X_test_scaled)
print("\nRefined Test set accuracy:", accuracy_score(y_test, y_pred_refined))
print("\nRefined Classification Report:\n", classification_report(y_test, y_pred_refined))
```
random forest
```python
f_model = RandomForestClassifier(n_estimators=100)

# Train the model
rf_model.fit(X_train_scaled, y_train)

# Predict on the test set
y_pred_rf = rf_model.predict(X_test_scaled)

# Evaluate the model
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print("\nRandom Forest Classification Report:\n", classification_report(y_test, y_pred_rf))

# Cross-validation to get a better estimate of the model performance
cross_val_scores_rf = cross_val_score(rf_model, X_train_scaled, y_train, cv=5)
print("\nRandom Forest Cross-validation scores:", cross_val_scores_rf)
print("\nMean cross-validation score (Random Forest):", cross_val_scores_rf.mean())
```
## Result
### Model performance Result

Comparison of Model Performance Metrics for Different Classification Models Used in Predicting like_ratio (Multi-Class Classification)

| Model                  | Accuracy | Precision                          | Recall                            | F1-Score                         |
|------------------------|----------|-----------------------------------|-----------------------------------|-----------------------------------|
| **Logistic Regression**| 77.33%   | 0.64 (High), 0.86 (Low), 0.73 (Medium) | 0.82 (High), 0.86 (Low), 0.66 (Medium) | 0.72 (High), 0.86 (Low), 0.69 (Medium) |
| **Decision Tree**      | 74.67%   | 0.56 (High), 0.85 (Low), 0.74 (Medium) | 0.91 (High), 0.83 (Low), 0.59 (Medium) | 0.69 (High), 0.84 (Low), 0.65 (Medium) |
| **Random Forest**      | 78.67%   | 0.69 (High), 0.88 (Low), 0.73 (Medium) | 0.82 (High), 0.80 (Low), 0.76 (Medium) | 0.75 (High), 0.84 (Low), 0.75 (Medium) |



Cross-Validation Scores


| Model                  | Cross-Validation Scores                             | Mean Cross-Validation Score |
|------------------------|----------------------------------------------------|-----------------------------|
| **Logistic Regression**| [0.6533, 0.6133, 0.8133, 0.72, 0.4667]             | 0.6533                      |
| **Decision Tree**      | Refined Best CV: [0.6208 (balanced parameters)]     | 0.6208                      |
| **Random Forest**      | [0.75, 0.6833, 0.6833, 0.7, 0.7833]                | 0.72                        |


### Result Interpretation
We can see from the table that the Random Forest model is the most effective model for predicting the like_ratio, achieving the highest accuracy (78.67%) and a balanced performance in terms of Precision, Recall, and F1-Score across all classes (High, Low, and Medium). Its average cross-validation score is 0.72, demonstrating good generalization.

The Logistic regression model is second effective one with an overall accuracy of 77.33%. While the low rank Precision (0.86) is strong, the medium rank Recall (0.66) performed relatively weakly. Nonetheless, the model is metrically consistent, showing its suitability for applications that require interpretability and stable performance.

For the Decision Tree model, it has the lowest overall accuracy (74.67%). While it performs well in recognizing high levels with a recall of 0.91, it performs poorly in recognizing intermediate levels where it has the lowest accuracy (0.74) and recall (0.59). But it can distinguish between high & low cases well.

## Insights
From the results, we can find that the Random Forest model shows the best balance of accuracy, precision, recall, and F1 value in predicting the like_ratio of songs, especially in the medium class (Medium). This indicates that Random Forest has a strong ability in dealing with multi-categorization problems. The Logistic Regression model, on the other hand, has high precision and recall in identifying the low like_ratio category, suggesting that it is more effective in distinguishing between low popularity songs. while the Decision Tree model's perform is not so good as we expected among all these three models, with low recall especially in the Medium category, suggesting that it may be unsuitable for dealing with such a complex multi-class classification task

This multi-class classification enables me to better understand the advantages and limitations of different models in multi-class classification tasks, and teach me lesson that we should choose the right model for future projects based on the specific needs of the task.
