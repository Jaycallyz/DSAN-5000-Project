# Instructions 

`Note`: You should remove these instructions once you have read and understood them. They should not be included in your final submission.

`Remember:` Exactly what do you put on this page will be specific you your project and data. Some things might "make more sense" on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.  

## Suggested page structure

Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:

`Audience`:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts. 

- **Introduction and Motivation:** Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.
- **Overview of Methods:** Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.
- **Code:** Include the code you used to implement your workflow.
- **Summary and Interpretation of Results:** Summarize your findings, interpret the results, and discuss their technical implications.

## What to address 

The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.

This page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.

### Part 1: Dimensionality Reduction

The objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.

1. **PCA (Principal Component Analysis):**
   - Apply PCA to your dataset.
   - Determine the optimal number of principal components.
   - Visualize the reduced-dimensional data.
   - Analyze and interpret the results.

2. **t-SNE (t-distributed Stochastic Neighbor Embedding):**
   - Implement t-SNE on the same dataset.
   - Experiment with different perplexity values.
   - Visualize the t-SNE output to reveal patterns and clusters.
   - Compare the results of t-SNE with those from PCA.

3. **Evaluation and Comparison:**
   - Evaluate the effectiveness of PCA and t-SNE in preserving data structure.
   - Compare the visualization capabilities of both techniques.
   - Discuss the trade-offs and scenarios where one technique may perform better than the other.

### Part 2: Clustering Methods

Apply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.

1. **Clustering Methods:**
   - Apply **K-Means**, **DBSCAN**, and **Hierarchical clustering** to your dataset.
   - Write a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).

2. **Results Section:**
   - Discuss and visualize the results of each clustering analysis.
   - Compare the performance of different clustering methods, noting any insights gained from the analysis.
   - Visualize cluster patterns and how they relate (if at all) to existing labels in the dataset.
   - Use professional, labeled, and clear visualizations that support your discussion.

3. **Conclusion:**
   - Summarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations.

## PCA
```{python}
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# Load the dataset
df = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')

# Specify the feature columns
features = [
    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',
    'Subscriber Count', 'Definition', 'Mean Sentiment Score',
    'Duration_seconds', 'genre_label', 'singer_followers', 'singer_popularity'
]

# Ensure the target column 'popularity' exists in DataFrame
if 'Popularity' not in df.columns:
    raise ValueError("The 'popularity' column is missing from the DataFrame.")

# Split into input (X) and target (y)
X = df[features]  # Inputs
y = df['Popularity']  # Target

# Applying PCA
pca = PCA(n_components=3)  # Reduce to 4 dimensions for visualization
X_pca = pca.fit_transform(X)
# How much variance was retained?
print("Explained Variance Ratio:", pca.explained_variance_ratio_)
print("Total Variance Explained:", sum(pca.explained_variance_ratio_))
```
## t-SNE
```{python}
perplexities = [5, 30, 50, 100]
fig, ax = plt.subplots(2, 2, figsize=(15, 10))
for i, perplexity in enumerate(perplexities):
    # Applying t-SNE
    tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=3000, random_state=42)
    X_tsne = tsne.fit_transform(X)

    # Plotting
    ax[i//2, i%2].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', marker='o', alpha=0.5)
    ax[i//2, i%2].set_title(f't-SNE with Perplexity={perplexity}')
    ax[i//2, i%2].set_xlabel('t-SNE Feature 1')
    ax[i//2, i%2].set_ylabel('t-SNE Feature 2')

plt.tight_layout()
plt.show()
```

## Evaluation and Comparison: PCA vs. t-SNE

### Effectiveness in Preserving Data Structure

- **PCA:** The PCA visualization shows a somewhat linear distribution of the data points, focusing on the major directions of data variance. It effectively captures the global structure of the data, emphasizing the spread along the directions of highest variance.

- **t-SNE:** In contrast, the t-SNE visualization exhibits a more scattered and differentiated structure with distinct clusters. This indicates t-SNE's superior capability in capturing local structures within the data, potentially revealing intrinsic patterns that PCA might overlook.

#### Visualization Capabilities

- **PCA:** Provides a simplified overview, projecting the data into lower dimensions while trying to preserve the variance. It's effective for quick explorations of the data to identify gross underlying patterns but might miss finer details.

- **t-SNE:** Produces more visually distinct clusters, making it easier to identify groups and patterns within the data. This makes t-SNE a better tool for tasks requiring detailed pattern recognition and cluster analysis.

#### Trade-offs and Scenarios

- **PCA:**
  - **Pros:** Less computationally intensive, suitable for larger datasets, provides a quick overview.
  - **Cons:** Might miss non-linear relationships between features.
  - **Best for:** Preliminary data analysis, reducing dimensionality for linear data, or when computational resources are limited.

- **t-SNE:**
  - **Pros:** Captures complex non-linear relationships, excellent for identifying clusters and local patterns.
  - **Cons:** Computationally expensive, sensitive to parameter settings (like perplexity), not suitable for very large datasets.
  - **Best for:** Detailed exploratory data analysis, when clusters or local patterns within the data are of interest, and computational resources are adequate.

Both PCA and t-SNE offer valuable insights, but their applicability depends on the specific needs of the analysis. PCA can serve as a good starting point for linear dimensionality reduction, while t-SNE is more suited for in-depth analysis requiring detailed cluster identification.
