# Instructions 

`Note`: You should remove these instructions once you have read and understood them. They should not be included in your final submission.

`Remember:` Exactly what do you put on this page will be specific you your project and data. Some things might "make more sense" on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.  

## Suggested page structure

Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:

`Audience`:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts. 

- **Introduction and Motivation:** Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.
- **Overview of Methods:** Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.
- **Code:** Include the code you used to implement your workflow.
- **Summary and Interpretation of Results:** Summarize your findings, interpret the results, and discuss their technical implications.

## What to address 

The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.

This page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.

### Part 1: Dimensionality Reduction

The objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.

1. **PCA (Principal Component Analysis):**
   - Apply PCA to your dataset.
   - Determine the optimal number of principal components.
   - Visualize the reduced-dimensional data.
   - Analyze and interpret the results.

2. **t-SNE (t-distributed Stochastic Neighbor Embedding):**
   - Implement t-SNE on the same dataset.
   - Experiment with different perplexity values.
   - Visualize the t-SNE output to reveal patterns and clusters.
   - Compare the results of t-SNE with those from PCA.

3. **Evaluation and Comparison:**
   - Evaluate the effectiveness of PCA and t-SNE in preserving data structure.
   - Compare the visualization capabilities of both techniques.
   - Discuss the trade-offs and scenarios where one technique may perform better than the other.

### Part 2: Clustering Methods

Apply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.

1. **Clustering Methods:**
   - Apply **K-Means**, **DBSCAN**, and **Hierarchical clustering** to your dataset.
   - Write a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).

2. **Results Section:**
   - Discuss and visualize the results of each clustering analysis.
   - Compare the performance of different clustering methods, noting any insights gained from the analysis.
   - Visualize cluster patterns and how they relate (if at all) to existing labels in the dataset.
   - Use professional, labeled, and clear visualizations that support your discussion.

3. **Conclusion:**
   - Summarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations.

# Dimensionality Reduction
## PCA
```{python}
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# Load the dataset
df = pd.read_csv('../../data/processed-data/Normalized_Data_with_Sentiments.csv')

# Specify the feature columns
features = [
    'Days Since Published', 'View Count', 'Like Count', 'Comment Count',
    'Subscriber Count', 'Definition', 'Mean Sentiment Score',
    'Duration_seconds', 'genre_label', 'singer_followers', 'singer_popularity'
]

# Ensure the target column 'popularity' exists in DataFrame
if 'Popularity' not in df.columns:
    raise ValueError("The 'popularity' column is missing from the DataFrame.")

# Split into input (X) and target (y)
X = df[features]  # Inputs
y = df['Popularity']  # Target

# Applying PCA
pca = PCA(n_components=3)  # Reduce to 3 dimensions for visualization
X_pca = pca.fit_transform(X)
# How much variance was retained?
print("Explained Variance Ratio:", pca.explained_variance_ratio_)
print("Total Variance Explained:", sum(pca.explained_variance_ratio_))
```

## t-SNE
```{python}
perplexities = [5, 30, 50, 100]
fig, ax = plt.subplots(2, 2, figsize=(15, 10))
for i, perplexity in enumerate(perplexities):
    # Applying t-SNE
    tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=3000, random_state=42)
    X_tsne = tsne.fit_transform(X)

    # Plotting
    ax[i//2, i%2].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', marker='o', alpha=0.5)
    ax[i//2, i%2].set_title(f't-SNE with Perplexity={perplexity}')
    ax[i//2, i%2].set_xlabel('t-SNE Feature 1')
    ax[i//2, i%2].set_ylabel('t-SNE Feature 2')

plt.tight_layout()
plt.show()
```

From the plot, we can observe perplexities=50 has a better performance so we chose it as the parameter for dimension reduction.

## Evaluation and Comparison: PCA vs. t-SNE

### Effectiveness in Preserving Data Structure

- **PCA:** The PCA visualization shows a somewhat linear distribution of the data points, focusing on the major directions of data variance. It effectively captures the global structure of the data, emphasizing the spread along the directions of highest variance.

- **t-SNE:** In contrast, the t-SNE visualization exhibits a more scattered and differentiated structure with distinct clusters. This indicates t-SNE's superior capability in capturing local structures within the data, potentially revealing intrinsic patterns that PCA might overlook.

#### Visualization Capabilities

- **PCA:** Provides a simplified overview, projecting the data into lower dimensions while trying to preserve the variance. It's effective for quick explorations of the data to identify gross underlying patterns but might miss finer details.

- **t-SNE:** Produces more visually distinct clusters, making it easier to identify groups and patterns within the data. This makes t-SNE a better tool for tasks requiring detailed pattern recognition and cluster analysis.

#### Trade-offs and Scenarios

- **PCA:**
  - **Pros:** Less computationally intensive, suitable for larger datasets, provides a quick overview.
  - **Cons:** Might miss non-linear relationships between features.
  - **Best for:** Preliminary data analysis, reducing dimensionality for linear data, or when computational resources are limited.

- **t-SNE:**
  - **Pros:** Captures complex non-linear relationships, excellent for identifying clusters and local patterns.
  - **Cons:** Computationally expensive, sensitive to parameter settings (like perplexity), not suitable for very large datasets.
  - **Best for:** Detailed exploratory data analysis, when clusters or local patterns within the data are of interest, and computational resources are adequate.

For the data given, t-SNE has the better performance for dimentionality reduction and for future classification task.

# Clustering Methods

In this part of the analysis, three clustering techniques—K-Means, DBSCAN, and Hierarchical Clustering—were applied to the dataset that was preprocessed using PCA and t-SNE for dimensionality reduction. This approach allows us to compare how each clustering method performs on data transformed by these techniques, offering insights into their different use cases and effectiveness.

## K-Means Clustering
K-Means clustering was applied to both PCA and t-SNE processed data. The algorithm partitions the data into K mutually exclusive clusters by assigning each data point to the cluster with the nearest mean. This method is effective in producing spherical clusters where the centroid represents the mean of the cluster's points. The number of clusters, K, was set to 3 based on domain knowledge and preliminary analysis.

## DBSCAN
DBSCAN (Density-Based Spatial Clustering of Applications with Noise) was utilized to identify arbitrarily shaped clusters based on density. It categorizes data points into clusters when they are closely packed together, while points in low-density areas are labeled as noise. This method is particularly effective for data with noise and outliers.

## Hierarchical Clustering
Hierarchical clustering was performed using the Ward method, which minimizes the total within-cluster variance. At each step, the pair of clusters with the minimum between-cluster distance are merged. This method is well-suited for identifying hierarchical relationships between clusters.

## Results Section

### Visualizing Cluster Results
The clustering results from K-Means, DBSCAN, and Hierarchical clustering were visualized using scatter plots, clearly labeled with clusters identified from PCA and t-SNE processed data. Each visualization helps in understanding the cluster distribution and separation.
```{python}
import numpy as np
from sklearn.cluster import KMeans, DBSCAN
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt

# K-Means Clustering
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans_tsne = kmeans.fit(X_tsne)
kmeans_tsne_labels = kmeans_tsne.labels_

# DBSCAN
dbscan = DBSCAN(eps=1, min_samples=5)
dbscan_tsne = dbscan.fit(X_tsne)
dbscan_tsne_labels = dbscan_tsne.labels_

# Hierarchical Clustering
hierarchical = AgglomerativeClustering(n_clusters=3)
hierarchical_tsne = hierarchical.fit(X_tsne)
hierarchical_tsne_labels = hierarchical_tsne.labels_

# Plotting the results
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# K-Means Plot
axes[0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=kmeans_tsne_labels, cmap='viridis', marker='o')
axes[0].set_title('K-Means Clustering on t-SNE Data')
axes[0].set_xlabel('t-SNE Axis 1')
axes[0].set_ylabel('t-SNE Axis 2')

# DBSCAN Plot
axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=dbscan_tsne_labels, cmap='viridis', marker='o')
axes[1].set_title('DBSCAN Clustering on t-SNE Data')
axes[1].set_xlabel('t-SNE Axis 1')
axes[1].set_ylabel('t-SNE Axis 2')

# Hierarchical Plot
axes[2].scatter(X_tsne[:, 0], X_tsne[:, 1], c=hierarchical_tsne_labels, cmap='viridis', marker='o')
axes[2].set_title('Hierarchical Clustering on t-SNE Data')
axes[2].set_xlabel('t-SNE Axis 1')
axes[2].set_ylabel('t-SNE Axis 2')

plt.tight_layout()
plt.show()
```

### Results Insights

1. **K-Means Clustering**:
   - K-Means with `n_clusters=3` segmented the data into three distinct groups. The clusters are relatively well-separated with slight overlap, indicating a clear grouping pattern in the dataset.

2. **DBSCAN Clustering**:
   - DBSCAN with `eps=1` and `min_samples=5` identified varying densities within the dataset. This method has successfully differentiated the dense core groups from sparser outliers, which are not included in any cluster and are labeled as noise. But there is overlap between different groups.

3. **Hierarchical Clustering**:
   - Hierarchical clustering revealed three main clusters, with results similar to K-Means but differing slightly in the cluster boundaries. 

### Performance Comparison

- **Silhouette Scores**:
  - K-Means achieved the highest Silhouette Score of 0.463882, suggesting a good level of separation and cohesion within clusters.
  - Hierarchical Clustering followed closely with a Silhouette Score of 0.456777.
  - DBSCAN had a lower Silhouette Score of 0.3538208, indicating more overlap or less distinct clustering compared to the other methods.

So K-Means has the best performance on the t-sne processed dataset.

### Conclusion
From the visualization, we can see there is chance for the dataset to be classified into groups, indicating there may be differences beween different songs for their popularity according to our features given. And in the future this can be used for popularity forecast.

